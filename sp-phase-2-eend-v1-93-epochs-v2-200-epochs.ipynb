{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10531093,"sourceType":"datasetVersion","datasetId":6517104},{"sourceId":601180,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":450500,"modelId":466854}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:29:14.440068Z","iopub.execute_input":"2025-10-24T03:29:14.440310Z","iopub.status.idle":"2025-10-24T03:29:15.244766Z","shell.execute_reply.started":"2025-10-24T03:29:14.440291Z","shell.execute_reply":"2025-10-24T03:29:15.243973Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"/kaggle/input/contraeend/pytorch/default/1/contraeend_best.pth\n/kaggle/input/callhome/labels/labels_14.rttm\n/kaggle/input/callhome/labels/labels_135.rttm\n/kaggle/input/callhome/labels/labels_25.rttm\n/kaggle/input/callhome/labels/labels_28.rttm\n/kaggle/input/callhome/labels/labels_48.rttm\n/kaggle/input/callhome/labels/labels_91.rttm\n/kaggle/input/callhome/labels/labels_32.rttm\n/kaggle/input/callhome/labels/labels_136.rttm\n/kaggle/input/callhome/labels/labels_18.rttm\n/kaggle/input/callhome/labels/labels_80.rttm\n/kaggle/input/callhome/labels/labels_54.rttm\n/kaggle/input/callhome/labels/labels_9.rttm\n/kaggle/input/callhome/labels/labels_38.rttm\n/kaggle/input/callhome/labels/labels_98.rttm\n/kaggle/input/callhome/labels/labels_127.rttm\n/kaggle/input/callhome/labels/labels_51.rttm\n/kaggle/input/callhome/labels/labels_29.rttm\n/kaggle/input/callhome/labels/labels_131.rttm\n/kaggle/input/callhome/labels/labels_24.rttm\n/kaggle/input/callhome/labels/labels_58.rttm\n/kaggle/input/callhome/labels/labels_87.rttm\n/kaggle/input/callhome/labels/labels_77.rttm\n/kaggle/input/callhome/labels/labels_105.rttm\n/kaggle/input/callhome/labels/labels_12.rttm\n/kaggle/input/callhome/labels/labels_16.rttm\n/kaggle/input/callhome/labels/labels_74.rttm\n/kaggle/input/callhome/labels/labels_114.rttm\n/kaggle/input/callhome/labels/labels_121.rttm\n/kaggle/input/callhome/labels/labels_108.rttm\n/kaggle/input/callhome/labels/labels_3.rttm\n/kaggle/input/callhome/labels/labels_84.rttm\n/kaggle/input/callhome/labels/labels_36.rttm\n/kaggle/input/callhome/labels/labels_57.rttm\n/kaggle/input/callhome/labels/labels_69.rttm\n/kaggle/input/callhome/labels/labels_8.rttm\n/kaggle/input/callhome/labels/labels_63.rttm\n/kaggle/input/callhome/labels/labels_106.rttm\n/kaggle/input/callhome/labels/labels_0.rttm\n/kaggle/input/callhome/labels/labels_94.rttm\n/kaggle/input/callhome/labels/labels_75.rttm\n/kaggle/input/callhome/labels/labels_133.rttm\n/kaggle/input/callhome/labels/labels_85.rttm\n/kaggle/input/callhome/labels/labels_115.rttm\n/kaggle/input/callhome/labels/labels_118.rttm\n/kaggle/input/callhome/labels/labels_67.rttm\n/kaggle/input/callhome/labels/labels_21.rttm\n/kaggle/input/callhome/labels/labels_123.rttm\n/kaggle/input/callhome/labels/labels_138.rttm\n/kaggle/input/callhome/labels/labels_23.rttm\n/kaggle/input/callhome/labels/labels_49.rttm\n/kaggle/input/callhome/labels/labels_93.rttm\n/kaggle/input/callhome/labels/labels_124.rttm\n/kaggle/input/callhome/labels/labels_31.rttm\n/kaggle/input/callhome/labels/labels_102.rttm\n/kaggle/input/callhome/labels/labels_10.rttm\n/kaggle/input/callhome/labels/labels_137.rttm\n/kaggle/input/callhome/labels/labels_59.rttm\n/kaggle/input/callhome/labels/labels_33.rttm\n/kaggle/input/callhome/labels/labels_117.rttm\n/kaggle/input/callhome/labels/labels_92.rttm\n/kaggle/input/callhome/labels/labels_89.rttm\n/kaggle/input/callhome/labels/labels_83.rttm\n/kaggle/input/callhome/labels/labels_97.rttm\n/kaggle/input/callhome/labels/labels_5.rttm\n/kaggle/input/callhome/labels/labels_50.rttm\n/kaggle/input/callhome/labels/labels_78.rttm\n/kaggle/input/callhome/labels/labels_39.rttm\n/kaggle/input/callhome/labels/labels_35.rttm\n/kaggle/input/callhome/labels/labels_27.rttm\n/kaggle/input/callhome/labels/labels_68.rttm\n/kaggle/input/callhome/labels/labels_104.rttm\n/kaggle/input/callhome/labels/labels_43.rttm\n/kaggle/input/callhome/labels/labels_22.rttm\n/kaggle/input/callhome/labels/labels_116.rttm\n/kaggle/input/callhome/labels/labels_110.rttm\n/kaggle/input/callhome/labels/labels_30.rttm\n/kaggle/input/callhome/labels/labels_17.rttm\n/kaggle/input/callhome/labels/labels_71.rttm\n/kaggle/input/callhome/labels/labels_61.rttm\n/kaggle/input/callhome/labels/labels_81.rttm\n/kaggle/input/callhome/labels/labels_64.rttm\n/kaggle/input/callhome/labels/labels_130.rttm\n/kaggle/input/callhome/labels/labels_65.rttm\n/kaggle/input/callhome/labels/labels_112.rttm\n/kaggle/input/callhome/labels/labels_52.rttm\n/kaggle/input/callhome/labels/labels_7.rttm\n/kaggle/input/callhome/labels/labels_82.rttm\n/kaggle/input/callhome/labels/labels_95.rttm\n/kaggle/input/callhome/labels/labels_26.rttm\n/kaggle/input/callhome/labels/labels_53.rttm\n/kaggle/input/callhome/labels/labels_66.rttm\n/kaggle/input/callhome/labels/labels_86.rttm\n/kaggle/input/callhome/labels/labels_129.rttm\n/kaggle/input/callhome/labels/labels_44.rttm\n/kaggle/input/callhome/labels/labels_20.rttm\n/kaggle/input/callhome/labels/labels_79.rttm\n/kaggle/input/callhome/labels/labels_11.rttm\n/kaggle/input/callhome/labels/labels_19.rttm\n/kaggle/input/callhome/labels/labels_4.rttm\n/kaggle/input/callhome/labels/labels_101.rttm\n/kaggle/input/callhome/labels/labels_56.rttm\n/kaggle/input/callhome/labels/labels_119.rttm\n/kaggle/input/callhome/labels/labels_113.rttm\n/kaggle/input/callhome/labels/labels_109.rttm\n/kaggle/input/callhome/labels/labels_139.rttm\n/kaggle/input/callhome/labels/labels_42.rttm\n/kaggle/input/callhome/labels/labels_34.rttm\n/kaggle/input/callhome/labels/labels_41.rttm\n/kaggle/input/callhome/labels/labels_96.rttm\n/kaggle/input/callhome/labels/labels_122.rttm\n/kaggle/input/callhome/labels/labels_2.rttm\n/kaggle/input/callhome/labels/labels_47.rttm\n/kaggle/input/callhome/labels/labels_132.rttm\n/kaggle/input/callhome/labels/labels_70.rttm\n/kaggle/input/callhome/labels/labels_45.rttm\n/kaggle/input/callhome/labels/labels_55.rttm\n/kaggle/input/callhome/labels/labels_15.rttm\n/kaggle/input/callhome/labels/labels_90.rttm\n/kaggle/input/callhome/labels/labels_128.rttm\n/kaggle/input/callhome/labels/labels_13.rttm\n/kaggle/input/callhome/labels/labels_126.rttm\n/kaggle/input/callhome/labels/labels_73.rttm\n/kaggle/input/callhome/labels/labels_111.rttm\n/kaggle/input/callhome/labels/labels_107.rttm\n/kaggle/input/callhome/labels/labels_99.rttm\n/kaggle/input/callhome/labels/labels_1.rttm\n/kaggle/input/callhome/labels/labels_62.rttm\n/kaggle/input/callhome/labels/labels_46.rttm\n/kaggle/input/callhome/labels/labels_100.rttm\n/kaggle/input/callhome/labels/labels_134.rttm\n/kaggle/input/callhome/labels/labels_88.rttm\n/kaggle/input/callhome/labels/labels_6.rttm\n/kaggle/input/callhome/labels/labels_125.rttm\n/kaggle/input/callhome/labels/labels_72.rttm\n/kaggle/input/callhome/labels/labels_103.rttm\n/kaggle/input/callhome/labels/labels_60.rttm\n/kaggle/input/callhome/labels/labels_37.rttm\n/kaggle/input/callhome/labels/labels_76.rttm\n/kaggle/input/callhome/labels/labels_120.rttm\n/kaggle/input/callhome/labels/labels_40.rttm\n/kaggle/input/callhome/audio/audio_49.wav\n/kaggle/input/callhome/audio/audio_90.wav\n/kaggle/input/callhome/audio/audio_77.wav\n/kaggle/input/callhome/audio/audio_66.wav\n/kaggle/input/callhome/audio/audio_54.wav\n/kaggle/input/callhome/audio/audio_42.wav\n/kaggle/input/callhome/audio/audio_81.wav\n/kaggle/input/callhome/audio/audio_72.wav\n/kaggle/input/callhome/audio/audio_107.wav\n/kaggle/input/callhome/audio/audio_79.wav\n/kaggle/input/callhome/audio/audio_83.wav\n/kaggle/input/callhome/audio/audio_88.wav\n/kaggle/input/callhome/audio/audio_60.wav\n/kaggle/input/callhome/audio/audio_55.wav\n/kaggle/input/callhome/audio/audio_25.wav\n/kaggle/input/callhome/audio/audio_112.wav\n/kaggle/input/callhome/audio/audio_132.wav\n/kaggle/input/callhome/audio/audio_44.wav\n/kaggle/input/callhome/audio/audio_7.wav\n/kaggle/input/callhome/audio/audio_119.wav\n/kaggle/input/callhome/audio/audio_106.wav\n/kaggle/input/callhome/audio/audio_20.wav\n/kaggle/input/callhome/audio/audio_59.wav\n/kaggle/input/callhome/audio/audio_129.wav\n/kaggle/input/callhome/audio/audio_12.wav\n/kaggle/input/callhome/audio/audio_31.wav\n/kaggle/input/callhome/audio/audio_97.wav\n/kaggle/input/callhome/audio/audio_40.wav\n/kaggle/input/callhome/audio/audio_29.wav\n/kaggle/input/callhome/audio/audio_37.wav\n/kaggle/input/callhome/audio/audio_67.wav\n/kaggle/input/callhome/audio/audio_94.wav\n/kaggle/input/callhome/audio/audio_82.wav\n/kaggle/input/callhome/audio/audio_61.wav\n/kaggle/input/callhome/audio/audio_27.wav\n/kaggle/input/callhome/audio/audio_100.wav\n/kaggle/input/callhome/audio/audio_32.wav\n/kaggle/input/callhome/audio/audio_110.wav\n/kaggle/input/callhome/audio/audio_50.wav\n/kaggle/input/callhome/audio/audio_120.wav\n/kaggle/input/callhome/audio/audio_113.wav\n/kaggle/input/callhome/audio/audio_56.wav\n/kaggle/input/callhome/audio/audio_114.wav\n/kaggle/input/callhome/audio/audio_104.wav\n/kaggle/input/callhome/audio/audio_71.wav\n/kaggle/input/callhome/audio/audio_28.wav\n/kaggle/input/callhome/audio/audio_76.wav\n/kaggle/input/callhome/audio/audio_57.wav\n/kaggle/input/callhome/audio/audio_102.wav\n/kaggle/input/callhome/audio/audio_45.wav\n/kaggle/input/callhome/audio/audio_41.wav\n/kaggle/input/callhome/audio/audio_86.wav\n/kaggle/input/callhome/audio/audio_2.wav\n/kaggle/input/callhome/audio/audio_4.wav\n/kaggle/input/callhome/audio/audio_118.wav\n/kaggle/input/callhome/audio/audio_19.wav\n/kaggle/input/callhome/audio/audio_89.wav\n/kaggle/input/callhome/audio/audio_70.wav\n/kaggle/input/callhome/audio/audio_87.wav\n/kaggle/input/callhome/audio/audio_75.wav\n/kaggle/input/callhome/audio/audio_84.wav\n/kaggle/input/callhome/audio/audio_95.wav\n/kaggle/input/callhome/audio/audio_23.wav\n/kaggle/input/callhome/audio/audio_137.wav\n/kaggle/input/callhome/audio/audio_93.wav\n/kaggle/input/callhome/audio/audio_14.wav\n/kaggle/input/callhome/audio/audio_5.wav\n/kaggle/input/callhome/audio/audio_80.wav\n/kaggle/input/callhome/audio/audio_13.wav\n/kaggle/input/callhome/audio/audio_8.wav\n/kaggle/input/callhome/audio/audio_22.wav\n/kaggle/input/callhome/audio/audio_85.wav\n/kaggle/input/callhome/audio/audio_126.wav\n/kaggle/input/callhome/audio/audio_38.wav\n/kaggle/input/callhome/audio/audio_47.wav\n/kaggle/input/callhome/audio/audio_111.wav\n/kaggle/input/callhome/audio/audio_127.wav\n/kaggle/input/callhome/audio/audio_130.wav\n/kaggle/input/callhome/audio/audio_96.wav\n/kaggle/input/callhome/audio/audio_98.wav\n/kaggle/input/callhome/audio/audio_52.wav\n/kaggle/input/callhome/audio/audio_16.wav\n/kaggle/input/callhome/audio/audio_78.wav\n/kaggle/input/callhome/audio/audio_101.wav\n/kaggle/input/callhome/audio/audio_26.wav\n/kaggle/input/callhome/audio/audio_30.wav\n/kaggle/input/callhome/audio/audio_17.wav\n/kaggle/input/callhome/audio/audio_24.wav\n/kaggle/input/callhome/audio/audio_0.wav\n/kaggle/input/callhome/audio/audio_6.wav\n/kaggle/input/callhome/audio/audio_92.wav\n/kaggle/input/callhome/audio/audio_69.wav\n/kaggle/input/callhome/audio/audio_64.wav\n/kaggle/input/callhome/audio/audio_131.wav\n/kaggle/input/callhome/audio/audio_105.wav\n/kaggle/input/callhome/audio/audio_21.wav\n/kaggle/input/callhome/audio/audio_35.wav\n/kaggle/input/callhome/audio/audio_15.wav\n/kaggle/input/callhome/audio/audio_63.wav\n/kaggle/input/callhome/audio/audio_122.wav\n/kaggle/input/callhome/audio/audio_125.wav\n/kaggle/input/callhome/audio/audio_115.wav\n/kaggle/input/callhome/audio/audio_68.wav\n/kaggle/input/callhome/audio/audio_138.wav\n/kaggle/input/callhome/audio/audio_1.wav\n/kaggle/input/callhome/audio/audio_10.wav\n/kaggle/input/callhome/audio/audio_121.wav\n/kaggle/input/callhome/audio/audio_48.wav\n/kaggle/input/callhome/audio/audio_117.wav\n/kaggle/input/callhome/audio/audio_11.wav\n/kaggle/input/callhome/audio/audio_39.wav\n/kaggle/input/callhome/audio/audio_103.wav\n/kaggle/input/callhome/audio/audio_136.wav\n/kaggle/input/callhome/audio/audio_46.wav\n/kaggle/input/callhome/audio/audio_139.wav\n/kaggle/input/callhome/audio/audio_134.wav\n/kaggle/input/callhome/audio/audio_62.wav\n/kaggle/input/callhome/audio/audio_18.wav\n/kaggle/input/callhome/audio/audio_116.wav\n/kaggle/input/callhome/audio/audio_123.wav\n/kaggle/input/callhome/audio/audio_133.wav\n/kaggle/input/callhome/audio/audio_135.wav\n/kaggle/input/callhome/audio/audio_108.wav\n/kaggle/input/callhome/audio/audio_109.wav\n/kaggle/input/callhome/audio/audio_34.wav\n/kaggle/input/callhome/audio/audio_53.wav\n/kaggle/input/callhome/audio/audio_73.wav\n/kaggle/input/callhome/audio/audio_43.wav\n/kaggle/input/callhome/audio/audio_99.wav\n/kaggle/input/callhome/audio/audio_124.wav\n/kaggle/input/callhome/audio/audio_9.wav\n/kaggle/input/callhome/audio/audio_3.wav\n/kaggle/input/callhome/audio/audio_65.wav\n/kaggle/input/callhome/audio/audio_33.wav\n/kaggle/input/callhome/audio/audio_58.wav\n/kaggle/input/callhome/audio/audio_128.wav\n/kaggle/input/callhome/audio/audio_36.wav\n/kaggle/input/callhome/audio/audio_74.wav\n/kaggle/input/callhome/audio/audio_51.wav\n/kaggle/input/callhome/audio/audio_91.wav\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 0.Full Block","metadata":{}},{"cell_type":"code","source":"\"\"\"\nContraEEND - phase 2 v2: Contrastive Pretraining (FIXED VERSION)\nPretrain encoder on callhome for speaker discrimination\n\"\"\"\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom typing import Tuple, Dict, List\nimport random\nfrom tqdm import tqdm\nimport math\nimport time\ndef set_seed(seed=42):\n    \"\"\"Set random seed for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef setup_device():\n    \"\"\"Setup device with comprehensive CUDA checking for Kaggle\"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        print(f\"‚úÖ CUDA is available!\")\n        print(f\"üöÄ Using GPU: {torch.cuda.get_device_name(0)}\")\n        print(\n            f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n        )\n        print(f\"üî• CUDA Version: {torch.version.cuda}\")\n\n        # Set memory allocation strategy for Kaggle\n        torch.cuda.empty_cache()\n        if hasattr(torch.cuda, \"set_memory_fraction\"):\n            torch.cuda.set_memory_fraction(0.8)  # Use 80% of GPU memory\n\n    else:\n        device = torch.device(\"cpu\")\n        print(\"‚ö†Ô∏è  CUDA not available, using CPU\")\n        print(\"üí° Consider enabling GPU in Kaggle: Settings -> Accelerator -> GPU\")\n\n    return device\n\nsetup_device()\n\nclass AudioProcessor:\n    \"\"\"Unified audio processing pipeline\"\"\"\n    def __init__(self, \n                 sample_rate: int = 16000,\n                 n_fft: int = 400,  # 25ms at 16kHz\n                 hop_length: int = 160,  # 10ms at 16kHz\n                 n_mels: int = 83,\n                 win_length: int = 400):\n        self.sample_rate = sample_rate\n        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n            sample_rate=sample_rate,\n            n_fft=n_fft,\n            hop_length=hop_length,\n            win_length=win_length,\n            n_mels=n_mels,\n            f_min=20,\n            f_max=sample_rate // 2\n        )\n    \n    def __call__(self, waveform: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            waveform: (channels, time) or (time,)\n        Returns:\n            log_mel: (n_mels, frames)\n        \"\"\"\n        if waveform.dim() == 1:\n            waveform = waveform.unsqueeze(0)\n        \n        # Ensure mono\n        if waveform.shape[0] > 1:\n            waveform = waveform.mean(dim=0, keepdim=True)\n        \n        # Compute mel spectrogram\n        mel = self.mel_transform(waveform)\n        \n        # Log scaling with small epsilon for stability\n        log_mel = torch.log(mel + 1e-6)\n        \n        return log_mel.squeeze(0)  # (n_mels, frames)\n\n\nclass SpecAugment(nn.Module):\n    \"\"\"SpecAugment for contrastive learning\"\"\"\n    def __init__(self, freq_mask_param=27, time_mask_param=100, n_freq_masks=2, n_time_masks=2):\n        super().__init__()\n        self.freq_mask_param = freq_mask_param\n        self.time_mask_param = time_mask_param\n        self.n_freq_masks = n_freq_masks\n        self.n_time_masks = n_time_masks\n    \n    def forward(self, mel: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            mel: (n_mels, time)\n        Returns:\n            augmented: (n_mels, time)\n        \"\"\"\n        mel = mel.clone()\n        n_mels, n_frames = mel.shape\n        \n        # Frequency masking\n        for _ in range(self.n_freq_masks):\n            f = random.randint(0, self.freq_mask_param)\n            f0 = random.randint(0, max(0, n_mels - f))\n            mel[f0:f0+f, :] = 0\n        \n        # Time masking\n        for _ in range(self.n_time_masks):\n            t = random.randint(0, min(self.time_mask_param, max(1, n_frames - 1)))\n            t0 = random.randint(0, max(0, n_frames - t))\n            mel[:, t0:t0+t] = 0\n        \n        return mel\n\nclass AudioAugmentation:\n    \"\"\"Advanced audio augmentation for speaker discrimination\"\"\"\n    def __init__(self, sample_rate=16000):\n        self.sample_rate = sample_rate\n    \n    def time_stretch(self, waveform: torch.Tensor, rate: float = None) -> torch.Tensor:\n        \"\"\"Time stretching (speed perturbation)\"\"\"\n        if rate is None:\n            rate = random.choice([0.9, 1.0, 1.1])\n        \n        if rate == 1.0:\n            return waveform\n        \n        # Simple resampling-based time stretch\n        original_length = waveform.shape[0]\n        stretched_length = int(original_length / rate)\n        \n        if stretched_length > 0:\n            stretched = F.interpolate(\n                waveform.unsqueeze(0).unsqueeze(0),\n                size=stretched_length,\n                mode='linear',\n                align_corners=False\n            ).squeeze()\n            \n            # Crop or pad to original length\n            if stretched.shape[0] > original_length:\n                return stretched[:original_length]\n            else:\n                padding = original_length - stretched.shape[0]\n                return F.pad(stretched, (0, padding))\n        \n        return waveform\n    \n    def pitch_shift(self, waveform: torch.Tensor, n_steps: int = None) -> torch.Tensor:\n        \"\"\"Pitch shifting\"\"\"\n        if n_steps is None:\n            n_steps = random.choice([-2, -1, 0, 1, 2])\n        \n        if n_steps == 0:\n            return waveform\n        \n        # Approximate pitch shift via time stretch + resampling\n        rate = 2 ** (n_steps / 12)\n        shifted = self.time_stretch(waveform, rate)\n        \n        return shifted\n    \n    def add_noise(self, waveform: torch.Tensor, snr_db: float = None) -> torch.Tensor:\n        \"\"\"Add Gaussian noise\"\"\"\n        if snr_db is None:\n            snr_db = random.uniform(15, 30)  # SNR between 15-30 dB\n        \n        signal_power = waveform.pow(2).mean()\n        snr_linear = 10 ** (snr_db / 10)\n        noise_power = signal_power / snr_linear\n        \n        noise = torch.randn_like(waveform) * torch.sqrt(noise_power)\n        \n        return waveform + noise\n    \n    def __call__(self, waveform: torch.Tensor, prob: float = 0.5) -> torch.Tensor:\n        \"\"\"Apply random augmentation\"\"\"\n        if random.random() < prob:\n            aug_type = random.choice(['time_stretch', 'pitch_shift', 'noise'])\n            \n            if aug_type == 'time_stretch':\n                return self.time_stretch(waveform)\n            elif aug_type == 'pitch_shift':\n                return self.pitch_shift(waveform)\n            elif aug_type == 'noise':\n                return self.add_noise(waveform)\n        \n        return waveform\n\ndef build_audio_rttm_mapping(audio_dir: str, rttm_dir: str) -> List[Tuple[Path, Path]]:\n    \"\"\"\n    Build mapping between audio files and RTTM files by index\n    audio_0.wav <-> labels_0.rttm\n    \"\"\"\n    audio_dir = Path(audio_dir)\n    rttm_dir = Path(rttm_dir)\n    \n    # Get all audio and rttm files\n    audio_files = sorted(audio_dir.glob('audio_*.wav'))\n    rttm_files = sorted(rttm_dir.glob('labels_*.rttm'))\n    \n    # Also try other extensions\n    if len(audio_files) == 0:\n        for ext in ['.flac', '.sph', '.mp3']:\n            audio_files = sorted(audio_dir.glob(f'audio_*{ext}'))\n            if len(audio_files) > 0:\n                break\n    \n    print(f\"\\nFound {len(audio_files)} audio files\")\n    print(f\"Found {len(rttm_files)} RTTM files\")\n    \n    if len(audio_files) == 0:\n        raise ValueError(f\"No audio files found in {audio_dir}\")\n    if len(rttm_files) == 0:\n        raise ValueError(f\"No RTTM files found in {rttm_dir}\")\n    \n    # Extract indices and match\n    audio_map = {}\n    for audio_file in audio_files:\n        # Extract index from \"audio_123.wav\"\n        try:\n            idx = int(audio_file.stem.split('_')[1])\n            audio_map[idx] = audio_file\n        except (IndexError, ValueError):\n            print(f\"Warning: Cannot parse index from {audio_file.name}\")\n    \n    rttm_map = {}\n    for rttm_file in rttm_files:\n        # Extract index from \"labels_123.rttm\"\n        try:\n            idx = int(rttm_file.stem.split('_')[1])\n            rttm_map[idx] = rttm_file\n        except (IndexError, ValueError):\n            print(f\"Warning: Cannot parse index from {rttm_file.name}\")\n    \n    # Match by index\n    pairs = []\n    for idx in sorted(audio_map.keys()):\n        if idx in rttm_map:\n            pairs.append((audio_map[idx], rttm_map[idx]))\n        else:\n            print(f\"Warning: No RTTM file for audio index {idx}\")\n    \n    print(f\"Matched {len(pairs)} audio-RTTM pairs\")\n    \n    if len(pairs) == 0:\n        raise ValueError(\"No matching audio-RTTM pairs found\")\n    \n    # Show examples\n    print(f\"\\nExample pairs:\")\n    for audio_file, rttm_file in pairs[:3]:\n        print(f\"  {audio_file.name} <-> {rttm_file.name}\")\n    \n    return pairs\n\n\nclass callhomeContrastiveDataset(Dataset):\n    \"\"\"\n    Callhome dataset for contrastive learning\n    Parses RTTM files to extract speaker segments\n    Handles indexed naming: audio_0.wav <-> labels_0.rttm\n    \n    OPTIMIZATION: Caches audio files in memory for faster loading\n    \"\"\"\n    def __init__(self, \n             audio_dir: str,\n             rttm_dir: str,\n             audio_processor: AudioProcessor,\n             segment_length: float = 3.0,\n             sample_rate: int = 16000,\n             min_segment_length: float = 2.0,\n             apply_augment: bool = True,\n             cache_audio: bool = True,\n             use_audio_augment: bool = True):  # NEW\n    \n        self.audio_dir = Path(audio_dir)\n        self.rttm_dir = Path(rttm_dir)\n        self.audio_processor = audio_processor\n        self.segment_length = segment_length\n        self.segment_samples = int(segment_length * sample_rate)\n        self.min_samples = int(min_segment_length * sample_rate)\n        self.sample_rate = sample_rate\n        self.apply_augment = apply_augment\n        self.cache_audio = cache_audio\n        self.audio_cache = {}\n        \n        # Spec augmentation\n        self.spec_augment = SpecAugment(\n            freq_mask_param=15,\n            time_mask_param=50,\n            n_freq_masks=1,\n            n_time_masks=1\n        ) if apply_augment else None\n        \n        # Audio augmentation (NEW)\n        self.use_audio_augment = use_audio_augment\n        self.audio_augment = AudioAugmentation(sample_rate) if use_audio_augment else None\n        \n        # Build audio-RTTM pairs by index\n        print(\"\\n\" + \"=\"*60)\n        print(\"Building audio-RTTM mapping...\")\n        print(\"=\"*60)\n        self.audio_rttm_pairs = build_audio_rttm_mapping(audio_dir, rttm_dir)\n        \n        # Parse RTTM files to build speaker segments\n        self.speaker_segments = self._parse_rttm_files()\n        self.speakers = list(self.speaker_segments.keys())\n        \n        # Create flat list for indexing\n        self.samples = []\n        for spk_id, segments in self.speaker_segments.items():\n            for seg in segments:\n                if seg['duration'] >= min_segment_length:\n                    self.samples.append((spk_id, seg))\n        \n        # Preload all audio files into memory if caching enabled\n        if self.cache_audio:\n            print(\"\\nüì¶ Caching audio files in memory...\")\n            self._cache_all_audio()\n        \n        print(f\"\\n‚úì Loaded {len(self.speakers)} speakers, {len(self.samples)} segments\")\n        print(f\"  Segment length: {segment_length}s, Min length: {min_segment_length}s\")\n        print(f\"  SpecAugment: {'enabled' if apply_augment else 'disabled'}\")\n        print(f\"  Audio augmentation: {'enabled' if use_audio_augment else 'disabled'}\")\n        print(f\"  Audio caching: {'enabled' if cache_audio else 'disabled'}\")\n    \n    def _parse_rttm_files(self) -> Dict[str, List[Dict]]:\n        \"\"\"\n        Parse RTTM files to extract speaker segments\n        Each audio file can have multiple speakers\n        \"\"\"\n        speaker_segments = {}\n        \n        print(f\"\\nParsing {len(self.audio_rttm_pairs)} audio-RTTM pairs...\")\n        \n        segments_found = 0\n        \n        for audio_path, rttm_path in tqdm(self.audio_rttm_pairs, desc=\"Parsing RTTM\"):\n            # Get file identifier for this pair\n            file_idx = audio_path.stem.split('_')[1]  # e.g., \"0\" from \"audio_0.wav\"\n            \n            with open(rttm_path, 'r') as f:\n                for line in f:\n                    line = line.strip()\n                    if not line or line.startswith('#'):\n                        continue\n                    \n                    parts = line.split()\n                    if len(parts) < 8 or parts[0] != 'SPEAKER':\n                        continue\n                    \n                    # RTTM format: SPEAKER <file_id> 1 <start> <duration> <NA> <NA> <speaker_id> <NA>\n                    file_id_in_rttm = parts[1]  # This might be different from our filename\n                    start_time = float(parts[3])\n                    duration = float(parts[4])\n                    speaker_id = parts[7]\n                    \n                    # Build unique speaker ID: use file index + speaker ID\n                    # This ensures speakers from different files are treated as different\n                    full_speaker_id = f\"file{file_idx}_spk{speaker_id}\"\n                    \n                    # Create segment info\n                    segment = {\n                        'file_idx': file_idx,\n                        'audio_path': audio_path,\n                        'start': start_time,\n                        'duration': duration,\n                        'speaker_id': speaker_id\n                    }\n                    \n                    if full_speaker_id not in speaker_segments:\n                        speaker_segments[full_speaker_id] = []\n                    \n                    speaker_segments[full_speaker_id].append(segment)\n                    segments_found += 1\n        \n        print(f\"Total segments found: {segments_found}\")\n        \n        # Filter speakers with at least 2 segments\n        original_speaker_count = len(speaker_segments)\n        speaker_segments = {\n            spk: segs for spk, segs in speaker_segments.items() \n            if len(segs) >= 2\n        }\n        \n        filtered_count = original_speaker_count - len(speaker_segments)\n        print(f\"Speakers after filtering (‚â•2 segments): {len(speaker_segments)}\")\n        if filtered_count > 0:\n            print(f\"  Removed {filtered_count} speakers with <2 segments\")\n        \n        # Print statistics\n        if speaker_segments:\n            seg_counts = [len(segs) for segs in speaker_segments.values()]\n            durations = [seg['duration'] for segs in speaker_segments.values() for seg in segs]\n            print(f\"\\nSegment statistics:\")\n            print(f\"  Per speaker - Min: {min(seg_counts)}, Max: {max(seg_counts)}, Avg: {np.mean(seg_counts):.1f}\")\n            print(f\"  Duration - Min: {min(durations):.1f}s, Max: {max(durations):.1f}s, Avg: {np.mean(durations):.1f}s\")\n        \n        return speaker_segments\n    \n    def _cache_all_audio(self):\n        \"\"\"Preload all audio files into memory\"\"\"\n        unique_audio_files = set()\n        for _, segment in self.samples:\n            unique_audio_files.add(segment['audio_path'])\n        \n        print(f\"Loading {len(unique_audio_files)} unique audio files...\")\n        for audio_path in tqdm(unique_audio_files, desc=\"Caching audio\"):\n            try:\n                waveform, sr = torchaudio.load(str(audio_path))\n                \n                # Resample if needed\n                if sr != self.sample_rate:\n                    resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n                    waveform = resampler(waveform)\n                \n                # Convert to mono\n                if waveform.shape[0] > 1:\n                    waveform = waveform.mean(dim=0, keepdim=True)\n                \n                self.audio_cache[str(audio_path)] = waveform.squeeze(0)\n            except Exception as e:\n                print(f\"\\nError caching {audio_path}: {e}\")\n        \n        print(f\"‚úì Cached {len(self.audio_cache)} audio files\")\n    \n    def _load_audio_segment(self, segment_info: Dict) -> torch.Tensor:\n        \"\"\"Load audio segment from file or cache based on RTTM timing\"\"\"\n        audio_path = segment_info['audio_path']\n        start_time = segment_info['start']\n        duration = segment_info['duration']\n        \n        try:\n            # Load from cache or file\n            if self.cache_audio and str(audio_path) in self.audio_cache:\n                waveform = self.audio_cache[str(audio_path)]\n            else:\n                waveform, sr = torchaudio.load(str(audio_path))\n                \n                # Resample if needed\n                if sr != self.sample_rate:\n                    resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n                    waveform = resampler(waveform)\n                \n                # Convert to mono\n                if waveform.shape[0] > 1:\n                    waveform = waveform.mean(dim=0, keepdim=True)\n                \n                waveform = waveform.squeeze(0)\n            \n            # Extract segment based on RTTM timing\n            start_sample = int(start_time * self.sample_rate)\n            duration_samples = int(duration * self.sample_rate)\n            end_sample = start_sample + duration_samples\n            \n            # Clip to valid range\n            start_sample = max(0, start_sample)\n            end_sample = min(len(waveform), end_sample)\n            \n            segment = waveform[start_sample:end_sample]\n            \n            # Crop or pad to target length\n            segment_len = len(segment)\n            \n            if segment_len < self.segment_samples:\n                # Pad if too short\n                padding = self.segment_samples - segment_len\n                segment = F.pad(segment.unsqueeze(0), (0, padding)).squeeze(0)\n            elif segment_len > self.segment_samples:\n                # Random crop if too long\n                max_start = segment_len - self.segment_samples\n                crop_start = random.randint(0, max_start)\n                segment = segment[crop_start:crop_start + self.segment_samples]\n            \n            return segment\n        \n        except Exception as e:\n            print(f\"\\nError loading {audio_path}: {e}\")\n            # Return silence if loading fails\n            return torch.zeros(self.segment_samples)\n    \n    def __len__(self) -> int:\n        return len(self.samples)\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n        \"\"\"\n        Returns:\n            anchor: (n_mels, frames)\n            positive: (n_mels, frames) - different segment from same speaker\n            speaker_id: int\n        \"\"\"\n        speaker_id, anchor_segment = self.samples[idx]\n        \n        # Load anchor\n        anchor_wav = self._load_audio_segment(anchor_segment)\n        \n        # Apply audio augmentation to anchor (NEW)\n        if self.use_audio_augment and self.audio_augment is not None:\n            anchor_wav = self.audio_augment(anchor_wav, prob=0.5)\n        \n        anchor_mel = self.audio_processor(anchor_wav)\n        \n        # Apply spec augmentation\n        if self.apply_augment and self.spec_augment is not None:\n            anchor_mel = self.spec_augment(anchor_mel)\n        \n        # Load positive (different segment from same speaker)\n        positive_segment = random.choice(self.speaker_segments[speaker_id])\n        \n        # Ensure it's different from anchor if possible\n        if len(self.speaker_segments[speaker_id]) > 1:\n            max_attempts = 10\n            for _ in range(max_attempts):\n                positive_segment = random.choice(self.speaker_segments[speaker_id])\n                if positive_segment['start'] != anchor_segment['start']:\n                    break\n        \n        positive_wav = self._load_audio_segment(positive_segment)\n        \n        # Apply DIFFERENT audio augmentation to positive (NEW)\n        if self.use_audio_augment and self.audio_augment is not None:\n            positive_wav = self.audio_augment(positive_wav, prob=0.5)\n        \n        positive_mel = self.audio_processor(positive_wav)\n        \n        # Apply different spec augmentation to positive\n        if self.apply_augment and self.spec_augment is not None:\n            positive_mel = self.spec_augment(positive_mel)\n        \n        # Convert speaker_id to numeric\n        speaker_idx = self.speakers.index(speaker_id)\n        \n        return anchor_mel, positive_mel, speaker_idx\n\n\nclass ConformerBlock(nn.Module):\n    \"\"\"Single Conformer block with multi-head attention and convolution\"\"\"\n    def __init__(self, d_model: int, n_heads: int, conv_kernel: int = 31, dropout: float = 0.1):\n        super().__init__()\n        \n        # Feed-forward module 1\n        self.ff1 = nn.Sequential(\n            nn.LayerNorm(d_model),\n            nn.Linear(d_model, d_model * 4),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model * 4, d_model),\n            nn.Dropout(dropout)\n        )\n        \n        # Multi-head self-attention\n        self.norm_attn = nn.LayerNorm(d_model)\n        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n        self.dropout_attn = nn.Dropout(dropout)\n        \n        # Convolution module\n        self.norm_conv = nn.LayerNorm(d_model)\n        self.conv = nn.Sequential(\n            nn.Conv1d(d_model, d_model * 2, 1),\n            nn.GLU(dim=1),\n            nn.Conv1d(d_model, d_model, conv_kernel, padding=conv_kernel//2, groups=d_model),\n            nn.BatchNorm1d(d_model),\n            nn.SiLU(),\n            nn.Conv1d(d_model, d_model, 1),\n            nn.Dropout(dropout)\n        )\n        \n        # Feed-forward module 2\n        self.ff2 = nn.Sequential(\n            nn.LayerNorm(d_model),\n            nn.Linear(d_model, d_model * 4),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model * 4, d_model),\n            nn.Dropout(dropout)\n        )\n        \n        self.norm_out = nn.LayerNorm(d_model)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: (batch, time, d_model)\n        Returns:\n            output: (batch, time, d_model)\n        \"\"\"\n        # FF1\n        x = x + 0.5 * self.ff1(x)\n        \n        # Attention\n        x_norm = self.norm_attn(x)\n        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n        x = x + self.dropout_attn(attn_out)\n        \n        # Convolution\n        x_norm = self.norm_conv(x)\n        x_conv = x_norm.transpose(1, 2)  # (batch, d_model, time)\n        x_conv = self.conv(x_conv)\n        x = x + x_conv.transpose(1, 2)\n        \n        # FF2\n        x = x + 0.5 * self.ff2(x)\n        \n        return self.norm_out(x)\n\n\nclass ConformerEncoder(nn.Module):\n    \"\"\"Conformer encoder for audio processing\"\"\"\n    def __init__(self, \n                 input_dim: int = 83,\n                 d_model: int = 256,\n                 n_layers: int = 8,\n                 n_heads: int = 4,\n                 conv_kernel: int = 31,\n                 dropout: float = 0.1,\n                 subsampling_factor: int = 4):\n        super().__init__()\n        \n        # Subsampling layer (reduce frame rate by 4x)\n        self.subsampling = nn.Sequential(\n            nn.Conv1d(input_dim, d_model, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1),\n            nn.ReLU()\n        )\n        \n        # Positional encoding\n        self.pos_encoding = PositionalEncoding(d_model, dropout)\n        \n        # Conformer blocks\n        self.blocks = nn.ModuleList([\n            ConformerBlock(d_model, n_heads, conv_kernel, dropout)\n            for _ in range(n_layers)\n        ])\n        \n        self.d_model = d_model\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: (batch, n_mels, time)\n        Returns:\n            output: (batch, time//4, d_model)\n        \"\"\"\n        # Subsampling\n        x = self.subsampling(x)  # (batch, d_model, time//4)\n        x = x.transpose(1, 2)  # (batch, time//4, d_model)\n        \n        # Positional encoding\n        x = self.pos_encoding(x)\n        \n        # Conformer blocks\n        for block in self.blocks:\n            x = block(x)\n        \n        return x\n\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding\"\"\"\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 10000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\n\nclass ProjectionHead(nn.Module):\n    \"\"\"Projection head for contrastive learning\"\"\"\n    def __init__(self, input_dim: int, hidden_dim: int = 256, output_dim: int = 128):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.net(x)\n\n\nclass ContraEENDEncoder(nn.Module):\n    \"\"\"Full encoder with projection head\"\"\"\n    def __init__(self, \n                 input_dim: int = 83,\n                 d_model: int = 256,\n                 n_layers: int = 8,\n                 n_heads: int = 4,\n                 projection_dim: int = 128):\n        super().__init__()\n        \n        self.encoder = ConformerEncoder(\n            input_dim=input_dim,\n            d_model=d_model,\n            n_layers=n_layers,\n            n_heads=n_heads\n        )\n        \n        # Temporal pooling for utterance-level representation\n        self.temporal_pool = nn.AdaptiveAvgPool1d(1)\n        \n        # Projection head\n        self.projection = ProjectionHead(d_model, d_model, projection_dim)\n    \n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Args:\n            x: (batch, n_mels, time)\n        Returns:\n            embeddings: (batch, d_model) - frame-level pooled\n            projections: (batch, projection_dim) - for contrastive loss\n        \"\"\"\n        # Encode\n        encoded = self.encoder(x)  # (batch, time//4, d_model)\n        \n        # Temporal pooling\n        pooled = self.temporal_pool(encoded.transpose(1, 2)).squeeze(-1)  # (batch, d_model)\n        \n        # Project\n        projected = self.projection(pooled)  # (batch, projection_dim)\n        \n        # L2 normalize projections\n        projected = F.normalize(projected, p=2, dim=1)\n        \n        return pooled, projected\n\n\nclass SupConLoss(nn.Module):\n    \"\"\"Supervised Contrastive Loss with hard negative mining\"\"\"\n    def __init__(self, temperature: float = 0.2, base_temperature: float = 0.2):\n        super().__init__()\n        self.temperature = temperature\n        self.base_temperature = base_temperature\n    \n    def forward(self, features: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n        device = features.device\n        batch_size = features.shape[0]\n        \n        labels = labels.contiguous().view(-1, 1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        \n        # Compute similarity matrix\n        anchor_dot_contrast = torch.div(\n            torch.matmul(features, features.T),\n            self.temperature\n        )\n        \n        # For numerical stability\n        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n        logits = anchor_dot_contrast - logits_max.detach()\n        \n        # Remove self-contrast\n        logits_mask = torch.scatter(\n            torch.ones_like(mask),\n            1,\n            torch.arange(batch_size).view(-1, 1).to(device),\n            0\n        )\n        mask = mask * logits_mask\n        \n        # HARD NEGATIVE MINING: Weight harder negatives more\n        exp_logits = torch.exp(logits) * logits_mask\n        \n        # Hard negative mask (only keep negatives, exclude self and positives)\n        hard_negative_mask = (1 - mask) * logits_mask\n        \n        # Weighted by difficulty (higher similarity = harder negative)\n        hard_weights = torch.exp(logits) * hard_negative_mask\n        hard_weights = hard_weights / (hard_weights.sum(1, keepdim=True) + 1e-8)\n        \n        # Reweight denominator with hard negatives\n        weighted_exp_logits = exp_logits * (1 + hard_weights)\n        \n        log_prob = logits - torch.log(weighted_exp_logits.sum(1, keepdim=True))\n        \n        # Compute mean over positives\n        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)\n        \n        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n        loss = loss.mean()\n        \n        return loss\n\nimport csv\nfrom datetime import datetime\n\nclass MetricsLogger:\n    \"\"\"CSV logger for tracking training metrics\"\"\"\n    def __init__(self, log_dir: str = './logs', experiment_name: str = 'contraeend'):\n        self.log_dir = Path(log_dir)\n        self.log_dir.mkdir(parents=True, exist_ok=True)\n        \n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        self.csv_path = self.log_dir / f'{experiment_name}_{timestamp}.csv'\n        \n        self.columns = [\n            'timestamp', 'epoch', 'phase', 'learning_rate',\n            'train_loss', 'val_loss',\n            'pos_sim_mean', 'neg_sim_mean', 'separation',\n            'accuracy', 'adjusted_rand_score', 'der', 'transition_rate',\n            'avg_segment_duration', 'num_segments', 'temporal_stability',\n            'per_speaker_accuracy_mean', 'per_speaker_accuracy_std',\n            'num_speakers_detected', 'num_speakers_true',\n            'has_temporal_modeling', 'oracle_speakers',\n            'filename', 'duration',\n            'batch_size', 'd_model', 'n_layers', 'temperature',\n        ]\n        \n        with open(self.csv_path, 'w', newline='') as f:\n            writer = csv.DictWriter(f, fieldnames=self.columns)\n            writer.writeheader()\n        \n        print(f\"üìä Metrics logger initialized: {self.csv_path}\")\n    \n    def log_epoch(self, epoch, phase, train_loss, val_loss, learning_rate,\n                  embedding_metrics=None, config=None):\n        \"\"\"Log metrics for one epoch\"\"\"\n        row = {\n            'timestamp': datetime.now().isoformat(),\n            'epoch': epoch,\n            'phase': phase,\n            'learning_rate': learning_rate,\n            'train_loss': train_loss,\n            'val_loss': val_loss,\n        }\n        \n        if embedding_metrics:\n            row.update({\n                'pos_sim_mean': embedding_metrics.get('pos_sim_mean', ''),\n                'neg_sim_mean': embedding_metrics.get('neg_sim_mean', ''),\n                'separation': embedding_metrics.get('separation', ''),\n            })\n        \n        if config:\n            row.update({\n                'batch_size': config.get('batch_size', ''),\n                'd_model': config.get('d_model', ''),\n                'n_layers': config.get('n_layers', ''),\n                'temperature': config.get('temperature', ''),\n            })\n        \n        with open(self.csv_path, 'a', newline='') as f:\n            writer = csv.DictWriter(f, fieldnames=self.columns)\n            writer.writerow(row)\n\n\nclass ContrastiveTrainer:\n    \"\"\"Trainer for phase 2 v2: Contrastive Pretraining\"\"\"\n    def __init__(self,\n                 model: nn.Module,\n                 train_loader: DataLoader,\n                 val_loader: DataLoader,\n                 num_epochs: int,\n                 device: str = 'cuda',\n                 learning_rate: float = 1e-3,\n                 weight_decay: float = 1e-4,\n                 temperature: float = 0.2,\n                 accumulation_steps: int = 4, \n                 log_dir: str = './logs',\n                 config: dict = None):\n        \n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        \n        # Loss\n        self.criterion = SupConLoss(temperature=temperature)\n        \n        # Optimizer\n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=learning_rate,\n            weight_decay=weight_decay,\n            betas=(0.9, 0.98)\n        )\n        \n        # Warmup + Cosine scheduler\n        self.warmup_epochs = 5\n        self.total_steps = len(train_loader) * num_epochs\n        self.warmup_steps = len(train_loader) * self.warmup_epochs\n        self.current_step = 0\n        \n        def lr_lambda(current_step):\n            if current_step < self.warmup_steps:\n                # Linear warmup\n                return float(current_step) / float(max(1, self.warmup_steps))\n            # Cosine decay after warmup\n            progress = float(current_step - self.warmup_steps) / float(max(1, self.total_steps - self.warmup_steps))\n            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n        \n        from torch.optim.lr_scheduler import LambdaLR\n        self.scheduler = LambdaLR(self.optimizer, lr_lambda)\n        \n        # Mixed precision\n        self.scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n        self.use_amp = device == 'cuda'\n        \n        # Early stopping\n        self.best_val_loss = float('inf')\n        self.patience = 10\n        self.patience_counter = 0\n        self.min_delta = 1e-4\n        self.accumulation_steps = accumulation_steps\n        self.logger = MetricsLogger(log_dir=log_dir, experiment_name='contraeend')\n        self.config = config\n        print(f\"Effective batch size: {train_loader.batch_size * 2 * accumulation_steps}\")\n    \n    \n    def train_epoch(self, epoch: int) -> float:\n        \"\"\"Train one epoch with gradient accumulation\"\"\"\n        self.model.train()\n        # Check GPU usage on first batch\n        if epoch == 1:\n            print(\"\\nüîç Checking GPU usage on first batch...\")\n        total_loss = 0\n        accumulated_loss = 0\n\n        # Timing\n        data_time = 0\n        forward_time = 0\n        backward_time = 0\n        \n        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch} [Train]')\n        for batch_idx, (anchor, positive, labels) in enumerate(pbar):\n            t0 = time.time()\n            \n            anchor = anchor.to(self.device)\n            positive = positive.to(self.device)\n            labels = labels.to(self.device)\n            combined = torch.cat([anchor, positive], dim=0)\n            combined_labels = torch.cat([labels, labels], dim=0)\n            \n            data_time += time.time() - t0\n            t1 = time.time()\n            \n            # Forward\n            with torch.cuda.amp.autocast(enabled=self.use_amp):\n                _, projections = self.model(combined)\n                loss = self.criterion(projections, combined_labels)\n                loss = loss / self.accumulation_steps\n            \n            forward_time += time.time() - t1\n            t2 = time.time()\n                \n            # Backward\n            if self.use_amp:\n                self.scaler.scale(loss).backward()\n            else:\n                loss.backward()\n            \n            accumulated_loss += loss.item()\n            backward_time += time.time() - t2\n            # Show timing on first epoch\n            if epoch == 1 and batch_idx == 10:\n                print(f\"\\n‚è±Ô∏è Timing breakdown (first 10 batches):\")\n                print(f\"  Data loading: {data_time/10:.2f}s per batch\")\n                print(f\"  Forward pass: {forward_time/10:.2f}s per batch\")\n                print(f\"  Backward pass: {backward_time/10:.2f}s per batch\")\n            \n            # Update weights every accumulation_steps\n            if (batch_idx + 1) % self.accumulation_steps == 0:\n                if self.use_amp:\n                    self.scaler.unscale_(self.optimizer)\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n                    self.scaler.step(self.optimizer)\n                    self.scaler.update()\n                else:\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n                    self.optimizer.step()\n                \n                self.optimizer.zero_grad()\n                \n                # Step scheduler\n                self.current_step += 1\n                self.scheduler.step()\n                \n                total_loss += accumulated_loss\n                current_lr = self.optimizer.param_groups[0]['lr']\n                pbar.set_postfix({\n                    'loss': f'{accumulated_loss:.4f}',\n                    'lr': f'{current_lr:.6f}'\n                })\n                accumulated_loss = 0\n        \n        avg_loss = total_loss / (len(self.train_loader) // self.accumulation_steps)\n        return avg_loss\n    \n    def validate(self, epoch: int) -> float:\n        \"\"\"Validate\"\"\"\n        self.model.eval()\n        total_loss = 0\n        \n        with torch.no_grad():\n            pbar = tqdm(self.val_loader, desc=f'Epoch {epoch} [Val]')\n            for anchor, positive, labels in pbar:\n                anchor = anchor.to(self.device)\n                positive = positive.to(self.device)\n                labels = labels.to(self.device)\n                \n                combined = torch.cat([anchor, positive], dim=0)\n                combined_labels = torch.cat([labels, labels], dim=0)\n                \n                with torch.cuda.amp.autocast(enabled=self.use_amp):\n                    _, projections = self.model(combined)\n                    loss = self.criterion(projections, combined_labels)\n                \n                total_loss += loss.item()\n                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        avg_loss = total_loss / len(self.val_loader)\n        return avg_loss\n    \n    def compute_embedding_metrics(self, loader: DataLoader) -> Dict[str, float]:\n        \"\"\"Compute cosine similarity metrics for validation\"\"\"\n        self.model.eval()\n        embeddings_list = []\n        labels_list = []\n        \n        with torch.no_grad():\n            for batch_idx, (anchor, positive, labels) in enumerate(loader):\n                anchor = anchor.to(self.device)\n                with torch.cuda.amp.autocast(enabled=self.use_amp):\n                    _, anchor_proj = self.model(anchor)\n                embeddings_list.append(anchor_proj.cpu())\n                labels_list.append(labels)\n                \n                if batch_idx >= 20:  # Limit samples for speed\n                    break\n        \n        if len(embeddings_list) == 0:\n            return {'pos_sim_mean': 0.0, 'neg_sim_mean': 0.0, 'separation': 0.0}\n        \n        embeddings = torch.cat(embeddings_list, dim=0)  # (N, projection_dim)\n        labels = torch.cat(labels_list, dim=0)  # (N,)\n        \n        # Compute cosine similarity matrix\n        sim_matrix = torch.matmul(embeddings, embeddings.T)  # Already normalized\n        \n        # Same speaker pairs (positive)\n        same_speaker_mask = (labels.unsqueeze(0) == labels.unsqueeze(1))\n        same_speaker_mask.fill_diagonal_(False)  # Exclude self\n        same_speaker_sims = sim_matrix[same_speaker_mask]\n        \n        # Different speaker pairs (negative)\n        diff_speaker_mask = ~same_speaker_mask\n        diff_speaker_mask.fill_diagonal_(False)\n        diff_speaker_sims = sim_matrix[diff_speaker_mask]\n        \n        metrics = {\n            'pos_sim_mean': same_speaker_sims.mean().item() if len(same_speaker_sims) > 0 else 0.0,\n            'neg_sim_mean': diff_speaker_sims.mean().item() if len(diff_speaker_sims) > 0 else 0.0,\n            'separation': (same_speaker_sims.mean() - diff_speaker_sims.mean()).item() if len(same_speaker_sims) > 0 else 0.0\n        }\n        \n        return metrics\n    \n    def save_checkpoint(self, epoch: int, loss: float, filepath: str):\n        \"\"\"Save training checkpoint\"\"\"\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'loss': loss,\n            'best_val_loss': self.best_val_loss,\n            'current_step': self.current_step,\n            'patience_counter': self.patience_counter\n        }, filepath)\n        print(f\"Checkpoint saved: {filepath}\")\n    \n    def load_checkpoint(self, filepath: str) -> int:\n        \"\"\"Resume from checkpoint\"\"\"\n        if not os.path.exists(filepath):\n            print(f\"No checkpoint found at {filepath}\")\n            return 0\n        \n        checkpoint = torch.load(filepath, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n        self.current_step = checkpoint.get('current_step', 0)\n        \n        # RESET patience for enhancement training\n        self.patience_counter = 0  # Force reset\n        \n        epoch = checkpoint['epoch']\n        print(f\"‚úì Resumed from epoch {epoch}, best val loss: {self.best_val_loss:.4f}\")\n        print(f\"‚ö†Ô∏è  Patience counter reset to 0 for enhancement training\")\n        return epoch\n        \n    def train(self, num_epochs: int, checkpoint_dir: str = './checkpoints', start_epoch: int = 0):\n        \"\"\"Full training loop with early stopping\"\"\"\n        Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n        \n        for epoch in range(start_epoch + 1, num_epochs + 1):\n            print(f\"\\n{'='*60}\")\n            print(f\"Epoch {epoch}/{num_epochs}\")\n            print(f\"{'='*60}\")\n            \n            # Train\n            train_loss = self.train_epoch(epoch)\n            print(f\"Train Loss: {train_loss:.4f}\")\n            \n            # Validate\n            val_loss = self.validate(epoch)\n            print(f\"Val Loss: {val_loss:.4f}\")\n            \n            # Compute embedding metrics\n            metrics = self.compute_embedding_metrics(self.val_loader)\n            print(f\"Pos Sim: {metrics['pos_sim_mean']:.4f} | Neg Sim: {metrics['neg_sim_mean']:.4f} | Separation: {metrics['separation']:.4f}\")\n            \n            current_lr = self.optimizer.param_groups[0]['lr']\n            print(f\"Learning Rate: {current_lr:.6f}\")\n\n            self.logger.log_epoch(\n                epoch=epoch, phase='pretrain', train_loss=train_loss,\n                val_loss=val_loss, learning_rate=current_lr,\n                embedding_metrics=metrics, config=self.config\n            )\n            \n            # Save checkpoint every epoch\n            checkpoint_path = Path(checkpoint_dir) / f'contraeend_epoch_{epoch}.pth'\n            self.save_checkpoint(epoch, val_loss, str(checkpoint_path))\n            \n            # Early stopping check\n            if val_loss < self.best_val_loss - self.min_delta:\n                self.best_val_loss = val_loss\n                self.patience_counter = 0\n                best_path = Path(checkpoint_dir) / 'contraeend_best.pth'\n                self.save_checkpoint(epoch, val_loss, str(best_path))\n                print(f\"‚úì New best model saved! Val Loss: {val_loss:.4f}\")\n            else:\n                self.patience_counter += 1\n                print(f\"Patience: {self.patience_counter}/{self.patience}\")\n                \n                if self.patience_counter >= self.patience:\n                    print(f\"\\n‚ö†Ô∏è Early stopping triggered after {epoch} epochs\")\n                    break\n        \n        print(f\"\\nTraining completed! Best validation loss: {self.best_val_loss:.4f}\")\n\ndef main():\n    \"\"\"Main training function for phase 2 v2\"\"\"\n    \n    # Set random seed for reproducibility\n    set_seed(42)\n\n    # Configuration\n    config = {\n        'audio_dir': '/kaggle/input/callhome/audio',\n        'rttm_dir': '/kaggle/input/callhome/labels',\n        'batch_size': 128,  # INCREASED from 64\n        'num_epochs': 200,   # Reduced since we have better augmentation\n        'learning_rate': 5e-5,\n        'weight_decay': 1e-4,\n        'temperature': 0.15,  # LOWERED from 0.2 (more strict)\n        'segment_length': 3.0,\n        'd_model': 128,\n        'n_layers': 6,\n        'n_heads': 4,\n        'projection_dim': 64,\n        'num_workers': 4,\n        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n        'resume_from': '/kaggle/input/contraeend/pytorch/default/1/contraeend_best.pth',  # Resume from best model\n        'cache_audio': True,\n        'use_audio_augment': True,  # NEW: Enable audio augmentation\n    }\n    \n    print(\"=\"*60)\n    print(\"ContraEEND - phase 2 v2: Contrastive Pretraining (FIXED)\")\n    print(\"=\"*60)\n    print(f\"Device: {config['device']}\")\n    print(f\"Audio dir: {config['audio_dir']}\")\n    print(f\"RTTM dir: {config['rttm_dir']}\")\n    print(f\"Batch Size: {config['batch_size']}\")\n    print(f\"Temperature: {config['temperature']}\")\n    print(f\"Model: Conformer ({config['n_layers']} layers, {config['d_model']} dim)\")\n    print(\"=\"*60)\n    \n    # Audio processor\n    audio_processor = AudioProcessor()\n    \n    # Dataset with RTTM parsing\n    print(\"\\nLoading datasets...\")\n    full_dataset = callhomeContrastiveDataset(\n        audio_dir=config['audio_dir'],\n        rttm_dir=config['rttm_dir'],\n        audio_processor=audio_processor,\n        segment_length=config['segment_length'],\n        apply_augment=True,\n        cache_audio=config['cache_audio']  # NEW: Pass cache parameter\n    )\n        \n    # Check if dataset is empty\n    if len(full_dataset) == 0:\n        print(\"\\n‚ùå ERROR: No valid samples found!\")\n        print(\"\\nPossible issues:\")\n        print(\"1. Audio files don't match RTTM file IDs\")\n        print(\"2. All segments are too short (< 2.0s)\")\n        print(\"3. No speakers have ‚â•2 segments\")\n        return\n    \n    # Dataset diagnostics\n    print(\"\\n\" + \"=\"*60)\n    print(\"üîç DATASET DIAGNOSTICS\")\n    print(\"=\"*60)\n    print(f\"Total samples: {len(full_dataset)}\")\n    print(f\"Total speakers: {len(full_dataset.speakers)}\")\n    \n    if len(full_dataset.speakers) > 0:\n        speaker_counts = {spk: len(segs) for spk, segs in full_dataset.speaker_segments.items()}\n        segments_per_speaker = list(speaker_counts.values())\n        \n        print(f\"Avg segments per speaker: {np.mean(segments_per_speaker):.1f}\")\n        print(f\"Min segments per speaker: {min(segments_per_speaker)}\")\n        print(f\"Max segments per speaker: {max(segments_per_speaker)}\")\n        \n        print(f\"\\nSample speakers: {full_dataset.speakers[:5]}\")\n    \n    # Check if dataset is sufficient\n    if len(full_dataset) < 200:\n        print(\"\\n‚ö†Ô∏è  WARNING: Dataset is small!\")\n        print(f\"   Current: {len(full_dataset)} samples\")\n        print(f\"   This may limit model performance\")\n    \n    if len(full_dataset.speakers) < 20:\n        print(\"\\n‚ö†Ô∏è  WARNING: Few speakers!\")\n        print(f\"   Current: {len(full_dataset.speakers)} speakers\")\n        print(f\"   Contrastive learning works best with 100+ speakers\")\n    \n    print(\"=\"*60 + \"\\n\")\n    \n    # Split for validation (90/10)\n    train_size = int(0.9 * len(full_dataset))\n    val_size = len(full_dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(\n        full_dataset, [train_size, val_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    print(f\"Train samples: {len(train_dataset)}\")\n    print(f\"Val samples: {len(val_dataset)}\")\n    \n    # Adjust batch size and accumulation if needed\n    if len(train_dataset) < config['batch_size'] * 4:\n        print(\"\\n‚ö†Ô∏è  Small dataset: adjusting batch size and disabling accumulation\")\n        config['batch_size'] = min(16, len(train_dataset) // 2)\n        accumulation_steps = 1\n    else:\n        accumulation_steps = 4\n    \n    print(f\"Batch size: {config['batch_size']}\")\n    print(f\"Accumulation steps: {accumulation_steps}\")\n    print(f\"Effective batch size: {config['batch_size'] * accumulation_steps * 2}\")\n    \n    # Data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=config['num_workers'],\n        pin_memory=True if config['device'] == 'cuda' else False,\n        drop_last=True if len(train_dataset) > config['batch_size'] else False,\n        persistent_workers=True if config['num_workers'] > 0 else False  # NEW\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers'],\n        pin_memory=True if config['device'] == 'cuda' else False,\n        persistent_workers=True if config['num_workers'] > 0 else False  # NEW\n    )\n        \n    print(f\"Train batches: {len(train_loader)}\")\n    print(f\"Val batches: {len(val_loader)}\")\n    \n    # Model\n    print(\"\\nInitializing model...\")\n    model = ContraEENDEncoder(\n        input_dim=83,\n        d_model=config['d_model'],\n        n_layers=config['n_layers'],\n        n_heads=config['n_heads'],\n        projection_dim=config['projection_dim']\n    )\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    \n    # Trainer\n    print(\"\\nInitializing trainer...\")\n    trainer = ContrastiveTrainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        num_epochs=config['num_epochs'],\n        device=config['device'],\n        learning_rate=config['learning_rate'],\n        weight_decay=config['weight_decay'],\n        temperature=config['temperature'],\n        accumulation_steps=8 # INCREASED from 4: Effective batch = 128*8*2 = 2048\n    )\n    \n    # Resume from checkpoint if specified\n    start_epoch = 0\n    if config['resume_from'] is not None and os.path.exists(config['resume_from']):\n        print(f\"\\nResuming from checkpoint: {config['resume_from']}\")\n        start_epoch = trainer.load_checkpoint(config['resume_from'])\n        # FORCE RESET PATIENCE\n        trainer.patience_counter = 0\n        trainer.patience = 15  # more patience\n        print(f\"‚ö†Ô∏è  Patience reset to 0/15 for enhancement training\")\n    \n    \n    # Train\n    print(\"\\nStarting training...\")\n    print(\"Features enabled:\")\n    print(\"=\"*60)\n    \n    trainer.train(\n        num_epochs=config['num_epochs'],\n        start_epoch=start_epoch\n    )\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"== Training Complete! ==\")\n    print(f\"Best Validation Loss: {trainer.best_val_loss:.4f}\")\n    print(\"=\"*60)\n\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:29:15.246259Z","iopub.execute_input":"2025-10-24T03:29:15.246477Z","iopub.status.idle":"2025-10-24T03:29:15.266850Z","shell.execute_reply.started":"2025-10-24T03:29:15.246460Z","shell.execute_reply":"2025-10-24T03:29:15.265920Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"\"\"\"\nContraEEND - phase 2 v2: Contrastive Pretraining (FIXED VERSION)\nPretrain encoder on callhome for speaker discrimination\n\"\"\"\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom pathlib import Path\nfrom typing import Tuple, Dict, List\nimport random\nfrom tqdm import tqdm\nimport math\nimport time\ndef set_seed(seed=42):\n    \"\"\"Set random seed for reproducibility\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\ndef setup_device():\n    \"\"\"Setup device with comprehensive CUDA checking for Kaggle\"\"\"\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        print(f\"‚úÖ CUDA is available!\")\n        print(f\"üöÄ Using GPU: {torch.cuda.get_device_name(0)}\")\n        print(\n            f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n        )\n        print(f\"üî• CUDA Version: {torch.version.cuda}\")\n\n        # Set memory allocation strategy for Kaggle\n        torch.cuda.empty_cache()\n        if hasattr(torch.cuda, \"set_memory_fraction\"):\n            torch.cuda.set_memory_fraction(0.8)  # Use 80% of GPU memory\n\n    else:\n        device = torch.device(\"cpu\")\n        print(\"‚ö†Ô∏è  CUDA not available, using CPU\")\n        print(\"üí° Consider enabling GPU in Kaggle: Settings -> Accelerator -> GPU\")\n\n    return device\n\nsetup_device()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:29:15.267717Z","iopub.execute_input":"2025-10-24T03:29:15.267945Z","iopub.status.idle":"2025-10-24T03:29:19.798879Z","shell.execute_reply.started":"2025-10-24T03:29:15.267926Z","shell.execute_reply":"2025-10-24T03:29:19.798322Z"}},"outputs":[{"name":"stdout","text":"‚úÖ CUDA is available!\nüöÄ Using GPU: Tesla P100-PCIE-16GB\nüíæ GPU Memory: 17.1 GB\nüî• CUDA Version: 12.4\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda', index=0)"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"# 1. AUDIO PROCESSING","metadata":{}},{"cell_type":"code","source":"class AudioProcessor:\n    \"\"\"Unified audio processing pipeline\"\"\"\n    def __init__(self, \n                 sample_rate: int = 16000,\n                 n_fft: int = 400,  # 25ms at 16kHz\n                 hop_length: int = 160,  # 10ms at 16kHz\n                 n_mels: int = 83,\n                 win_length: int = 400):\n        self.sample_rate = sample_rate\n        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n            sample_rate=sample_rate,\n            n_fft=n_fft,\n            hop_length=hop_length,\n            win_length=win_length,\n            n_mels=n_mels,\n            f_min=20,\n            f_max=sample_rate // 2\n        )\n    \n    def __call__(self, waveform: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            waveform: (channels, time) or (time,)\n        Returns:\n            log_mel: (n_mels, frames)\n        \"\"\"\n        if waveform.dim() == 1:\n            waveform = waveform.unsqueeze(0)\n        \n        # Ensure mono\n        if waveform.shape[0] > 1:\n            waveform = waveform.mean(dim=0, keepdim=True)\n        \n        # Compute mel spectrogram\n        mel = self.mel_transform(waveform)\n        \n        # Log scaling with small epsilon for stability\n        log_mel = torch.log(mel + 1e-6)\n        \n        return log_mel.squeeze(0)  # (n_mels, frames)\n\n\nclass SpecAugment(nn.Module):\n    \"\"\"SpecAugment for contrastive learning\"\"\"\n    def __init__(self, freq_mask_param=27, time_mask_param=100, n_freq_masks=2, n_time_masks=2):\n        super().__init__()\n        self.freq_mask_param = freq_mask_param\n        self.time_mask_param = time_mask_param\n        self.n_freq_masks = n_freq_masks\n        self.n_time_masks = n_time_masks\n    \n    def forward(self, mel: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            mel: (n_mels, time)\n        Returns:\n            augmented: (n_mels, time)\n        \"\"\"\n        mel = mel.clone()\n        n_mels, n_frames = mel.shape\n        \n        # Frequency masking\n        for _ in range(self.n_freq_masks):\n            f = random.randint(0, self.freq_mask_param)\n            f0 = random.randint(0, max(0, n_mels - f))\n            mel[f0:f0+f, :] = 0\n        \n        # Time masking\n        for _ in range(self.n_time_masks):\n            t = random.randint(0, min(self.time_mask_param, max(1, n_frames - 1)))\n            t0 = random.randint(0, max(0, n_frames - t))\n            mel[:, t0:t0+t] = 0\n        \n        return mel\n\nclass AudioAugmentation:\n    \"\"\"Advanced audio augmentation for speaker discrimination\"\"\"\n    def __init__(self, sample_rate=16000):\n        self.sample_rate = sample_rate\n    \n    def time_stretch(self, waveform: torch.Tensor, rate: float = None) -> torch.Tensor:\n        \"\"\"Time stretching (speed perturbation)\"\"\"\n        if rate is None:\n            rate = random.choice([0.9, 1.0, 1.1])\n        \n        if rate == 1.0:\n            return waveform\n        \n        # Simple resampling-based time stretch\n        original_length = waveform.shape[0]\n        stretched_length = int(original_length / rate)\n        \n        if stretched_length > 0:\n            stretched = F.interpolate(\n                waveform.unsqueeze(0).unsqueeze(0),\n                size=stretched_length,\n                mode='linear',\n                align_corners=False\n            ).squeeze()\n            \n            # Crop or pad to original length\n            if stretched.shape[0] > original_length:\n                return stretched[:original_length]\n            else:\n                padding = original_length - stretched.shape[0]\n                return F.pad(stretched, (0, padding))\n        \n        return waveform\n    \n    def pitch_shift(self, waveform: torch.Tensor, n_steps: int = None) -> torch.Tensor:\n        \"\"\"Pitch shifting\"\"\"\n        if n_steps is None:\n            n_steps = random.choice([-2, -1, 0, 1, 2])\n        \n        if n_steps == 0:\n            return waveform\n        \n        # Approximate pitch shift via time stretch + resampling\n        rate = 2 ** (n_steps / 12)\n        shifted = self.time_stretch(waveform, rate)\n        \n        return shifted\n    \n    def add_noise(self, waveform: torch.Tensor, snr_db: float = None) -> torch.Tensor:\n        \"\"\"Add Gaussian noise\"\"\"\n        if snr_db is None:\n            snr_db = random.uniform(15, 30)  # SNR between 15-30 dB\n        \n        signal_power = waveform.pow(2).mean()\n        snr_linear = 10 ** (snr_db / 10)\n        noise_power = signal_power / snr_linear\n        \n        noise = torch.randn_like(waveform) * torch.sqrt(noise_power)\n        \n        return waveform + noise\n    \n    def __call__(self, waveform: torch.Tensor, prob: float = 0.5) -> torch.Tensor:\n        \"\"\"Apply random augmentation\"\"\"\n        if random.random() < prob:\n            aug_type = random.choice(['time_stretch', 'pitch_shift', 'noise'])\n            \n            if aug_type == 'time_stretch':\n                return self.time_stretch(waveform)\n            elif aug_type == 'pitch_shift':\n                return self.pitch_shift(waveform)\n            elif aug_type == 'noise':\n                return self.add_noise(waveform)\n        \n        return waveform","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:29:19.800338Z","iopub.execute_input":"2025-10-24T03:29:19.800630Z","iopub.status.idle":"2025-10-24T03:29:19.815339Z","shell.execute_reply.started":"2025-10-24T03:29:19.800613Z","shell.execute_reply":"2025-10-24T03:29:19.814585Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# 2. DATASET","metadata":{}},{"cell_type":"code","source":"def build_audio_rttm_mapping(audio_dir: str, rttm_dir: str) -> List[Tuple[Path, Path]]:\n    \"\"\"\n    Build mapping between audio files and RTTM files by index\n    audio_0.wav <-> labels_0.rttm\n    \"\"\"\n    audio_dir = Path(audio_dir)\n    rttm_dir = Path(rttm_dir)\n    \n    # Get all audio and rttm files\n    audio_files = sorted(audio_dir.glob('audio_*.wav'))\n    rttm_files = sorted(rttm_dir.glob('labels_*.rttm'))\n    \n    # Also try other extensions\n    if len(audio_files) == 0:\n        for ext in ['.flac', '.sph', '.mp3']:\n            audio_files = sorted(audio_dir.glob(f'audio_*{ext}'))\n            if len(audio_files) > 0:\n                break\n    \n    print(f\"\\nFound {len(audio_files)} audio files\")\n    print(f\"Found {len(rttm_files)} RTTM files\")\n    \n    if len(audio_files) == 0:\n        raise ValueError(f\"No audio files found in {audio_dir}\")\n    if len(rttm_files) == 0:\n        raise ValueError(f\"No RTTM files found in {rttm_dir}\")\n    \n    # Extract indices and match\n    audio_map = {}\n    for audio_file in audio_files:\n        # Extract index from \"audio_123.wav\"\n        try:\n            idx = int(audio_file.stem.split('_')[1])\n            audio_map[idx] = audio_file\n        except (IndexError, ValueError):\n            print(f\"Warning: Cannot parse index from {audio_file.name}\")\n    \n    rttm_map = {}\n    for rttm_file in rttm_files:\n        # Extract index from \"labels_123.rttm\"\n        try:\n            idx = int(rttm_file.stem.split('_')[1])\n            rttm_map[idx] = rttm_file\n        except (IndexError, ValueError):\n            print(f\"Warning: Cannot parse index from {rttm_file.name}\")\n    \n    # Match by index\n    pairs = []\n    for idx in sorted(audio_map.keys()):\n        if idx in rttm_map:\n            pairs.append((audio_map[idx], rttm_map[idx]))\n        else:\n            print(f\"Warning: No RTTM file for audio index {idx}\")\n    \n    print(f\"Matched {len(pairs)} audio-RTTM pairs\")\n    \n    if len(pairs) == 0:\n        raise ValueError(\"No matching audio-RTTM pairs found\")\n    \n    # Show examples\n    print(f\"\\nExample pairs:\")\n    for audio_file, rttm_file in pairs[:3]:\n        print(f\"  {audio_file.name} <-> {rttm_file.name}\")\n    \n    return pairs\n\n\nclass callhomeContrastiveDataset(Dataset):\n    \"\"\"\n    Callhome dataset for contrastive learning\n    Parses RTTM files to extract speaker segments\n    Handles indexed naming: audio_0.wav <-> labels_0.rttm\n    \n    OPTIMIZATION: Caches audio files in memory for faster loading\n    \"\"\"\n    def __init__(self, \n             audio_dir: str,\n             rttm_dir: str,\n             audio_processor: AudioProcessor,\n             segment_length: float = 3.0,\n             sample_rate: int = 16000,\n             min_segment_length: float = 2.0,\n             apply_augment: bool = True,\n             cache_audio: bool = True,\n             use_audio_augment: bool = True):  # NEW\n    \n        self.audio_dir = Path(audio_dir)\n        self.rttm_dir = Path(rttm_dir)\n        self.audio_processor = audio_processor\n        self.segment_length = segment_length\n        self.segment_samples = int(segment_length * sample_rate)\n        self.min_samples = int(min_segment_length * sample_rate)\n        self.sample_rate = sample_rate\n        self.apply_augment = apply_augment\n        self.cache_audio = cache_audio\n        self.audio_cache = {}\n        \n        # Spec augmentation\n        self.spec_augment = SpecAugment(\n            freq_mask_param=15,\n            time_mask_param=50,\n            n_freq_masks=1,\n            n_time_masks=1\n        ) if apply_augment else None\n        \n        # Audio augmentation (NEW)\n        self.use_audio_augment = use_audio_augment\n        self.audio_augment = AudioAugmentation(sample_rate) if use_audio_augment else None\n        \n        # Build audio-RTTM pairs by index\n        print(\"\\n\" + \"=\"*60)\n        print(\"Building audio-RTTM mapping...\")\n        print(\"=\"*60)\n        self.audio_rttm_pairs = build_audio_rttm_mapping(audio_dir, rttm_dir)\n        \n        # Parse RTTM files to build speaker segments\n        self.speaker_segments = self._parse_rttm_files()\n        self.speakers = list(self.speaker_segments.keys())\n        \n        # Create flat list for indexing\n        self.samples = []\n        for spk_id, segments in self.speaker_segments.items():\n            for seg in segments:\n                if seg['duration'] >= min_segment_length:\n                    self.samples.append((spk_id, seg))\n        \n        # Preload all audio files into memory if caching enabled\n        if self.cache_audio:\n            print(\"\\nüì¶ Caching audio files in memory...\")\n            self._cache_all_audio()\n        \n        print(f\"\\n‚úì Loaded {len(self.speakers)} speakers, {len(self.samples)} segments\")\n        print(f\"  Segment length: {segment_length}s, Min length: {min_segment_length}s\")\n        print(f\"  SpecAugment: {'enabled' if apply_augment else 'disabled'}\")\n        print(f\"  Audio augmentation: {'enabled' if use_audio_augment else 'disabled'}\")\n        print(f\"  Audio caching: {'enabled' if cache_audio else 'disabled'}\")\n    \n    def _parse_rttm_files(self) -> Dict[str, List[Dict]]:\n        \"\"\"\n        Parse RTTM files to extract speaker segments\n        Each audio file can have multiple speakers\n        \"\"\"\n        speaker_segments = {}\n        \n        print(f\"\\nParsing {len(self.audio_rttm_pairs)} audio-RTTM pairs...\")\n        \n        segments_found = 0\n        \n        for audio_path, rttm_path in tqdm(self.audio_rttm_pairs, desc=\"Parsing RTTM\"):\n            # Get file identifier for this pair\n            file_idx = audio_path.stem.split('_')[1]  # e.g., \"0\" from \"audio_0.wav\"\n            \n            with open(rttm_path, 'r') as f:\n                for line in f:\n                    line = line.strip()\n                    if not line or line.startswith('#'):\n                        continue\n                    \n                    parts = line.split()\n                    if len(parts) < 8 or parts[0] != 'SPEAKER':\n                        continue\n                    \n                    # RTTM format: SPEAKER <file_id> 1 <start> <duration> <NA> <NA> <speaker_id> <NA>\n                    file_id_in_rttm = parts[1]  # This might be different from our filename\n                    start_time = float(parts[3])\n                    duration = float(parts[4])\n                    speaker_id = parts[7]\n                    \n                    # Build unique speaker ID: use file index + speaker ID\n                    # This ensures speakers from different files are treated as different\n                    full_speaker_id = f\"file{file_idx}_spk{speaker_id}\"\n                    \n                    # Create segment info\n                    segment = {\n                        'file_idx': file_idx,\n                        'audio_path': audio_path,\n                        'start': start_time,\n                        'duration': duration,\n                        'speaker_id': speaker_id\n                    }\n                    \n                    if full_speaker_id not in speaker_segments:\n                        speaker_segments[full_speaker_id] = []\n                    \n                    speaker_segments[full_speaker_id].append(segment)\n                    segments_found += 1\n        \n        print(f\"Total segments found: {segments_found}\")\n        \n        # Filter speakers with at least 2 segments\n        original_speaker_count = len(speaker_segments)\n        speaker_segments = {\n            spk: segs for spk, segs in speaker_segments.items() \n            if len(segs) >= 2\n        }\n        \n        filtered_count = original_speaker_count - len(speaker_segments)\n        print(f\"Speakers after filtering (‚â•2 segments): {len(speaker_segments)}\")\n        if filtered_count > 0:\n            print(f\"  Removed {filtered_count} speakers with <2 segments\")\n        \n        # Print statistics\n        if speaker_segments:\n            seg_counts = [len(segs) for segs in speaker_segments.values()]\n            durations = [seg['duration'] for segs in speaker_segments.values() for seg in segs]\n            print(f\"\\nSegment statistics:\")\n            print(f\"  Per speaker - Min: {min(seg_counts)}, Max: {max(seg_counts)}, Avg: {np.mean(seg_counts):.1f}\")\n            print(f\"  Duration - Min: {min(durations):.1f}s, Max: {max(durations):.1f}s, Avg: {np.mean(durations):.1f}s\")\n        \n        return speaker_segments\n    \n    def _cache_all_audio(self):\n        \"\"\"Preload all audio files into memory\"\"\"\n        unique_audio_files = set()\n        for _, segment in self.samples:\n            unique_audio_files.add(segment['audio_path'])\n        \n        print(f\"Loading {len(unique_audio_files)} unique audio files...\")\n        for audio_path in tqdm(unique_audio_files, desc=\"Caching audio\"):\n            try:\n                waveform, sr = torchaudio.load(str(audio_path))\n                \n                # Resample if needed\n                if sr != self.sample_rate:\n                    resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n                    waveform = resampler(waveform)\n                \n                # Convert to mono\n                if waveform.shape[0] > 1:\n                    waveform = waveform.mean(dim=0, keepdim=True)\n                \n                self.audio_cache[str(audio_path)] = waveform.squeeze(0)\n            except Exception as e:\n                print(f\"\\nError caching {audio_path}: {e}\")\n        \n        print(f\"‚úì Cached {len(self.audio_cache)} audio files\")\n    \n    def _load_audio_segment(self, segment_info: Dict) -> torch.Tensor:\n        \"\"\"Load audio segment from file or cache based on RTTM timing\"\"\"\n        audio_path = segment_info['audio_path']\n        start_time = segment_info['start']\n        duration = segment_info['duration']\n        \n        try:\n            # Load from cache or file\n            if self.cache_audio and str(audio_path) in self.audio_cache:\n                waveform = self.audio_cache[str(audio_path)]\n            else:\n                waveform, sr = torchaudio.load(str(audio_path))\n                \n                # Resample if needed\n                if sr != self.sample_rate:\n                    resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n                    waveform = resampler(waveform)\n                \n                # Convert to mono\n                if waveform.shape[0] > 1:\n                    waveform = waveform.mean(dim=0, keepdim=True)\n                \n                waveform = waveform.squeeze(0)\n            \n            # Extract segment based on RTTM timing\n            start_sample = int(start_time * self.sample_rate)\n            duration_samples = int(duration * self.sample_rate)\n            end_sample = start_sample + duration_samples\n            \n            # Clip to valid range\n            start_sample = max(0, start_sample)\n            end_sample = min(len(waveform), end_sample)\n            \n            segment = waveform[start_sample:end_sample]\n            \n            # Crop or pad to target length\n            segment_len = len(segment)\n            \n            if segment_len < self.segment_samples:\n                # Pad if too short\n                padding = self.segment_samples - segment_len\n                segment = F.pad(segment.unsqueeze(0), (0, padding)).squeeze(0)\n            elif segment_len > self.segment_samples:\n                # Random crop if too long\n                max_start = segment_len - self.segment_samples\n                crop_start = random.randint(0, max_start)\n                segment = segment[crop_start:crop_start + self.segment_samples]\n            \n            return segment\n        \n        except Exception as e:\n            print(f\"\\nError loading {audio_path}: {e}\")\n            # Return silence if loading fails\n            return torch.zeros(self.segment_samples)\n    \n    def __len__(self) -> int:\n        return len(self.samples)\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n        \"\"\"\n        Returns:\n            anchor: (n_mels, frames)\n            positive: (n_mels, frames) - different segment from same speaker\n            speaker_id: int\n        \"\"\"\n        speaker_id, anchor_segment = self.samples[idx]\n        \n        # Load anchor\n        anchor_wav = self._load_audio_segment(anchor_segment)\n        \n        # Apply audio augmentation to anchor (NEW)\n        if self.use_audio_augment and self.audio_augment is not None:\n            anchor_wav = self.audio_augment(anchor_wav, prob=0.5)\n        \n        anchor_mel = self.audio_processor(anchor_wav)\n        \n        # Apply spec augmentation\n        if self.apply_augment and self.spec_augment is not None:\n            anchor_mel = self.spec_augment(anchor_mel)\n        \n        # Load positive (different segment from same speaker)\n        positive_segment = random.choice(self.speaker_segments[speaker_id])\n        \n        # Ensure it's different from anchor if possible\n        if len(self.speaker_segments[speaker_id]) > 1:\n            max_attempts = 10\n            for _ in range(max_attempts):\n                positive_segment = random.choice(self.speaker_segments[speaker_id])\n                if positive_segment['start'] != anchor_segment['start']:\n                    break\n        \n        positive_wav = self._load_audio_segment(positive_segment)\n        \n        # Apply DIFFERENT audio augmentation to positive (NEW)\n        if self.use_audio_augment and self.audio_augment is not None:\n            positive_wav = self.audio_augment(positive_wav, prob=0.5)\n        \n        positive_mel = self.audio_processor(positive_wav)\n        \n        # Apply different spec augmentation to positive\n        if self.apply_augment and self.spec_augment is not None:\n            positive_mel = self.spec_augment(positive_mel)\n        \n        # Convert speaker_id to numeric\n        speaker_idx = self.speakers.index(speaker_id)\n        \n        return anchor_mel, positive_mel, speaker_idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:29:19.816338Z","iopub.execute_input":"2025-10-24T03:29:19.817172Z","iopub.status.idle":"2025-10-24T03:29:19.846812Z","shell.execute_reply.started":"2025-10-24T03:29:19.817146Z","shell.execute_reply":"2025-10-24T03:29:19.846141Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# 3. MODEL ARCHITECTURE","metadata":{}},{"cell_type":"code","source":"class ConformerBlock(nn.Module):\n    \"\"\"Single Conformer block with multi-head attention and convolution\"\"\"\n    def __init__(self, d_model: int, n_heads: int, conv_kernel: int = 31, dropout: float = 0.1):\n        super().__init__()\n        \n        # Feed-forward module 1\n        self.ff1 = nn.Sequential(\n            nn.LayerNorm(d_model),\n            nn.Linear(d_model, d_model * 4),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model * 4, d_model),\n            nn.Dropout(dropout)\n        )\n        \n        # Multi-head self-attention\n        self.norm_attn = nn.LayerNorm(d_model)\n        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n        self.dropout_attn = nn.Dropout(dropout)\n        \n        # Convolution module\n        self.norm_conv = nn.LayerNorm(d_model)\n        self.conv = nn.Sequential(\n            nn.Conv1d(d_model, d_model * 2, 1),\n            nn.GLU(dim=1),\n            nn.Conv1d(d_model, d_model, conv_kernel, padding=conv_kernel//2, groups=d_model),\n            nn.BatchNorm1d(d_model),\n            nn.SiLU(),\n            nn.Conv1d(d_model, d_model, 1),\n            nn.Dropout(dropout)\n        )\n        \n        # Feed-forward module 2\n        self.ff2 = nn.Sequential(\n            nn.LayerNorm(d_model),\n            nn.Linear(d_model, d_model * 4),\n            nn.SiLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model * 4, d_model),\n            nn.Dropout(dropout)\n        )\n        \n        self.norm_out = nn.LayerNorm(d_model)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: (batch, time, d_model)\n        Returns:\n            output: (batch, time, d_model)\n        \"\"\"\n        # FF1\n        x = x + 0.5 * self.ff1(x)\n        \n        # Attention\n        x_norm = self.norm_attn(x)\n        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n        x = x + self.dropout_attn(attn_out)\n        \n        # Convolution\n        x_norm = self.norm_conv(x)\n        x_conv = x_norm.transpose(1, 2)  # (batch, d_model, time)\n        x_conv = self.conv(x_conv)\n        x = x + x_conv.transpose(1, 2)\n        \n        # FF2\n        x = x + 0.5 * self.ff2(x)\n        \n        return self.norm_out(x)\n\n\nclass ConformerEncoder(nn.Module):\n    \"\"\"Conformer encoder for audio processing\"\"\"\n    def __init__(self, \n                 input_dim: int = 83,\n                 d_model: int = 256,\n                 n_layers: int = 8,\n                 n_heads: int = 4,\n                 conv_kernel: int = 31,\n                 dropout: float = 0.1,\n                 subsampling_factor: int = 4):\n        super().__init__()\n        \n        # Subsampling layer (reduce frame rate by 4x)\n        self.subsampling = nn.Sequential(\n            nn.Conv1d(input_dim, d_model, kernel_size=3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1),\n            nn.ReLU()\n        )\n        \n        # Positional encoding\n        self.pos_encoding = PositionalEncoding(d_model, dropout)\n        \n        # Conformer blocks\n        self.blocks = nn.ModuleList([\n            ConformerBlock(d_model, n_heads, conv_kernel, dropout)\n            for _ in range(n_layers)\n        ])\n        \n        self.d_model = d_model\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x: (batch, n_mels, time)\n        Returns:\n            output: (batch, time//4, d_model)\n        \"\"\"\n        # Subsampling\n        x = self.subsampling(x)  # (batch, d_model, time//4)\n        x = x.transpose(1, 2)  # (batch, time//4, d_model)\n        \n        # Positional encoding\n        x = self.pos_encoding(x)\n        \n        # Conformer blocks\n        for block in self.blocks:\n            x = block(x)\n        \n        return x\n\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding\"\"\"\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 10000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        \n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(1, max_len, d_model)\n        pe[0, :, 0::2] = torch.sin(position * div_term)\n        pe[0, :, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\n\nclass ProjectionHead(nn.Module):\n    \"\"\"Projection head for contrastive learning\"\"\"\n    def __init__(self, input_dim: int, hidden_dim: int = 256, output_dim: int = 128):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)\n        )\n    \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.net(x)\n\n\nclass ContraEENDEncoder(nn.Module):\n    \"\"\"Full encoder with projection head\"\"\"\n    def __init__(self, \n                 input_dim: int = 83,\n                 d_model: int = 256,\n                 n_layers: int = 8,\n                 n_heads: int = 4,\n                 projection_dim: int = 128):\n        super().__init__()\n        \n        self.encoder = ConformerEncoder(\n            input_dim=input_dim,\n            d_model=d_model,\n            n_layers=n_layers,\n            n_heads=n_heads\n        )\n        \n        # Temporal pooling for utterance-level representation\n        self.temporal_pool = nn.AdaptiveAvgPool1d(1)\n        \n        # Projection head\n        self.projection = ProjectionHead(d_model, d_model, projection_dim)\n    \n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Args:\n            x: (batch, n_mels, time)\n        Returns:\n            embeddings: (batch, d_model) - frame-level pooled\n            projections: (batch, projection_dim) - for contrastive loss\n        \"\"\"\n        # Encode\n        encoded = self.encoder(x)  # (batch, time//4, d_model)\n        \n        # Temporal pooling\n        pooled = self.temporal_pool(encoded.transpose(1, 2)).squeeze(-1)  # (batch, d_model)\n        \n        # Project\n        projected = self.projection(pooled)  # (batch, projection_dim)\n        \n        # L2 normalize projections\n        projected = F.normalize(projected, p=2, dim=1)\n        \n        return pooled, projected","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:29:19.847704Z","iopub.execute_input":"2025-10-24T03:29:19.847957Z","iopub.status.idle":"2025-10-24T03:29:19.866075Z","shell.execute_reply.started":"2025-10-24T03:29:19.847941Z","shell.execute_reply":"2025-10-24T03:29:19.865349Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# 4. CONTRASTIVE LOSS\n","metadata":{}},{"cell_type":"code","source":"class SupConLoss(nn.Module):\n    \"\"\"Supervised Contrastive Loss with hard negative mining\"\"\"\n    def __init__(self, temperature: float = 0.2, base_temperature: float = 0.2):\n        super().__init__()\n        self.temperature = temperature\n        self.base_temperature = base_temperature\n    \n    def forward(self, features: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n        device = features.device\n        batch_size = features.shape[0]\n        \n        labels = labels.contiguous().view(-1, 1)\n        mask = torch.eq(labels, labels.T).float().to(device)\n        \n        # Compute similarity matrix\n        anchor_dot_contrast = torch.div(\n            torch.matmul(features, features.T),\n            self.temperature\n        )\n        \n        # For numerical stability\n        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n        logits = anchor_dot_contrast - logits_max.detach()\n        \n        # Remove self-contrast\n        logits_mask = torch.scatter(\n            torch.ones_like(mask),\n            1,\n            torch.arange(batch_size).view(-1, 1).to(device),\n            0\n        )\n        mask = mask * logits_mask\n        \n        # HARD NEGATIVE MINING: Weight harder negatives more\n        exp_logits = torch.exp(logits) * logits_mask\n        \n        # Hard negative mask (only keep negatives, exclude self and positives)\n        hard_negative_mask = (1 - mask) * logits_mask\n        \n        # Weighted by difficulty (higher similarity = harder negative)\n        hard_weights = torch.exp(logits) * hard_negative_mask\n        hard_weights = hard_weights / (hard_weights.sum(1, keepdim=True) + 1e-8)\n        \n        # Reweight denominator with hard negatives\n        weighted_exp_logits = exp_logits * (1 + hard_weights)\n        \n        log_prob = logits - torch.log(weighted_exp_logits.sum(1, keepdim=True))\n        \n        # Compute mean over positives\n        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-8)\n        \n        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n        loss = loss.mean()\n        \n        return loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:29:19.866906Z","iopub.execute_input":"2025-10-24T03:29:19.867385Z","iopub.status.idle":"2025-10-24T03:29:19.883635Z","shell.execute_reply.started":"2025-10-24T03:29:19.867361Z","shell.execute_reply":"2025-10-24T03:29:19.882926Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# 5. Training","metadata":{}},{"cell_type":"code","source":"import csv\nfrom datetime import datetime\n\nclass MetricsLogger:\n    \"\"\"CSV logger for tracking training metrics\"\"\"\n    def __init__(self, log_dir: str = './logs', experiment_name: str = 'contraeend'):\n        self.log_dir = Path(log_dir)\n        self.log_dir.mkdir(parents=True, exist_ok=True)\n        \n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        self.csv_path = self.log_dir / f'{experiment_name}_{timestamp}.csv'\n        \n        self.columns = [\n            'timestamp', 'epoch', 'phase', 'learning_rate',\n            'train_loss', 'val_loss',\n            'pos_sim_mean', 'neg_sim_mean', 'separation',\n            'accuracy', 'adjusted_rand_score', 'der', 'transition_rate',\n            'avg_segment_duration', 'num_segments', 'temporal_stability',\n            'per_speaker_accuracy_mean', 'per_speaker_accuracy_std',\n            'num_speakers_detected', 'num_speakers_true',\n            'has_temporal_modeling', 'oracle_speakers',\n            'filename', 'duration',\n            'batch_size', 'd_model', 'n_layers', 'temperature',\n        ]\n        \n        with open(self.csv_path, 'w', newline='') as f:\n            writer = csv.DictWriter(f, fieldnames=self.columns)\n            writer.writeheader()\n        \n        print(f\"üìä Metrics logger initialized: {self.csv_path}\")\n    \n    def log_epoch(self, epoch, phase, train_loss, val_loss, learning_rate,\n                  embedding_metrics=None, config=None):\n        \"\"\"Log metrics for one epoch\"\"\"\n        row = {\n            'timestamp': datetime.now().isoformat(),\n            'epoch': epoch,\n            'phase': phase,\n            'learning_rate': learning_rate,\n            'train_loss': train_loss,\n            'val_loss': val_loss,\n        }\n        \n        if embedding_metrics:\n            row.update({\n                'pos_sim_mean': embedding_metrics.get('pos_sim_mean', ''),\n                'neg_sim_mean': embedding_metrics.get('neg_sim_mean', ''),\n                'separation': embedding_metrics.get('separation', ''),\n            })\n        \n        if config:\n            row.update({\n                'batch_size': config.get('batch_size', ''),\n                'd_model': config.get('d_model', ''),\n                'n_layers': config.get('n_layers', ''),\n                'temperature': config.get('temperature', ''),\n            })\n        \n        with open(self.csv_path, 'a', newline='') as f:\n            writer = csv.DictWriter(f, fieldnames=self.columns)\n            writer.writerow(row)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:29:19.884440Z","iopub.execute_input":"2025-10-24T03:29:19.884672Z","iopub.status.idle":"2025-10-24T03:29:19.900789Z","shell.execute_reply.started":"2025-10-24T03:29:19.884649Z","shell.execute_reply":"2025-10-24T03:29:19.900096Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class ContrastiveTrainer:\n    \"\"\"Trainer for phase 2 v2: Contrastive Pretraining\"\"\"\n    def __init__(self,\n                 model: nn.Module,\n                 train_loader: DataLoader,\n                 val_loader: DataLoader,\n                 num_epochs: int,\n                 device: str = 'cuda',\n                 learning_rate: float = 1e-3,\n                 weight_decay: float = 1e-4,\n                 temperature: float = 0.2,\n                 accumulation_steps: int = 4, \n                 log_dir: str = './logs',\n                 config: dict = None):\n        \n        self.model = model.to(device)\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n        \n        # Loss\n        self.criterion = SupConLoss(temperature=temperature)\n        \n        # Optimizer\n        self.optimizer = torch.optim.AdamW(\n            model.parameters(),\n            lr=learning_rate,\n            weight_decay=weight_decay,\n            betas=(0.9, 0.98)\n        )\n        \n        # Warmup + Cosine scheduler\n        self.warmup_epochs = 5\n        self.total_steps = len(train_loader) * num_epochs\n        self.warmup_steps = len(train_loader) * self.warmup_epochs\n        self.current_step = 0\n        \n        def lr_lambda(current_step):\n            if current_step < self.warmup_steps:\n                # Linear warmup\n                return float(current_step) / float(max(1, self.warmup_steps))\n            # Cosine decay after warmup\n            progress = float(current_step - self.warmup_steps) / float(max(1, self.total_steps - self.warmup_steps))\n            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n        \n        from torch.optim.lr_scheduler import LambdaLR\n        self.scheduler = LambdaLR(self.optimizer, lr_lambda)\n        \n        # Mixed precision\n        self.scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n        self.use_amp = device == 'cuda'\n        \n        # Early stopping\n        self.best_val_loss = float('inf')\n        self.patience = 10\n        self.patience_counter = 0\n        self.min_delta = 1e-4\n        self.accumulation_steps = accumulation_steps\n        self.logger = MetricsLogger(log_dir=log_dir, experiment_name='contraeend')\n        self.config = config\n        print(f\"Effective batch size: {train_loader.batch_size * 2 * accumulation_steps}\")\n    \n    \n    def train_epoch(self, epoch: int) -> float:\n        \"\"\"Train one epoch with gradient accumulation\"\"\"\n        self.model.train()\n        # Check GPU usage on first batch\n        if epoch == 1:\n            print(\"\\nüîç Checking GPU usage on first batch...\")\n        total_loss = 0\n        accumulated_loss = 0\n\n        # Timing\n        data_time = 0\n        forward_time = 0\n        backward_time = 0\n        \n        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch} [Train]')\n        for batch_idx, (anchor, positive, labels) in enumerate(pbar):\n            t0 = time.time()\n            \n            anchor = anchor.to(self.device)\n            positive = positive.to(self.device)\n            labels = labels.to(self.device)\n            combined = torch.cat([anchor, positive], dim=0)\n            combined_labels = torch.cat([labels, labels], dim=0)\n            \n            data_time += time.time() - t0\n            t1 = time.time()\n            \n            # Forward\n            with torch.cuda.amp.autocast(enabled=self.use_amp):\n                _, projections = self.model(combined)\n                loss = self.criterion(projections, combined_labels)\n                loss = loss / self.accumulation_steps\n            \n            forward_time += time.time() - t1\n            t2 = time.time()\n                \n            # Backward\n            if self.use_amp:\n                self.scaler.scale(loss).backward()\n            else:\n                loss.backward()\n            \n            accumulated_loss += loss.item()\n            backward_time += time.time() - t2\n            # Show timing on first epoch\n            if epoch == 1 and batch_idx == 10:\n                print(f\"\\n‚è±Ô∏è Timing breakdown (first 10 batches):\")\n                print(f\"  Data loading: {data_time/10:.2f}s per batch\")\n                print(f\"  Forward pass: {forward_time/10:.2f}s per batch\")\n                print(f\"  Backward pass: {backward_time/10:.2f}s per batch\")\n            \n            # Update weights every accumulation_steps\n            if (batch_idx + 1) % self.accumulation_steps == 0:\n                if self.use_amp:\n                    self.scaler.unscale_(self.optimizer)\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n                    self.scaler.step(self.optimizer)\n                    self.scaler.update()\n                else:\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n                    self.optimizer.step()\n                \n                self.optimizer.zero_grad()\n                \n                # Step scheduler\n                self.current_step += 1\n                self.scheduler.step()\n                \n                total_loss += accumulated_loss\n                current_lr = self.optimizer.param_groups[0]['lr']\n                pbar.set_postfix({\n                    'loss': f'{accumulated_loss:.4f}',\n                    'lr': f'{current_lr:.6f}'\n                })\n                accumulated_loss = 0\n        \n        avg_loss = total_loss / (len(self.train_loader) // self.accumulation_steps)\n        return avg_loss\n    \n    def validate(self, epoch: int) -> float:\n        \"\"\"Validate\"\"\"\n        self.model.eval()\n        total_loss = 0\n        \n        with torch.no_grad():\n            pbar = tqdm(self.val_loader, desc=f'Epoch {epoch} [Val]')\n            for anchor, positive, labels in pbar:\n                anchor = anchor.to(self.device)\n                positive = positive.to(self.device)\n                labels = labels.to(self.device)\n                \n                combined = torch.cat([anchor, positive], dim=0)\n                combined_labels = torch.cat([labels, labels], dim=0)\n                \n                with torch.cuda.amp.autocast(enabled=self.use_amp):\n                    _, projections = self.model(combined)\n                    loss = self.criterion(projections, combined_labels)\n                \n                total_loss += loss.item()\n                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n        \n        avg_loss = total_loss / len(self.val_loader)\n        return avg_loss\n    \n    def compute_embedding_metrics(self, loader: DataLoader) -> Dict[str, float]:\n        \"\"\"Compute cosine similarity metrics for validation\"\"\"\n        self.model.eval()\n        embeddings_list = []\n        labels_list = []\n        \n        with torch.no_grad():\n            for batch_idx, (anchor, positive, labels) in enumerate(loader):\n                anchor = anchor.to(self.device)\n                with torch.cuda.amp.autocast(enabled=self.use_amp):\n                    _, anchor_proj = self.model(anchor)\n                embeddings_list.append(anchor_proj.cpu())\n                labels_list.append(labels)\n                \n                if batch_idx >= 20:  # Limit samples for speed\n                    break\n        \n        if len(embeddings_list) == 0:\n            return {'pos_sim_mean': 0.0, 'neg_sim_mean': 0.0, 'separation': 0.0}\n        \n        embeddings = torch.cat(embeddings_list, dim=0)  # (N, projection_dim)\n        labels = torch.cat(labels_list, dim=0)  # (N,)\n        \n        # Compute cosine similarity matrix\n        sim_matrix = torch.matmul(embeddings, embeddings.T)  # Already normalized\n        \n        # Same speaker pairs (positive)\n        same_speaker_mask = (labels.unsqueeze(0) == labels.unsqueeze(1))\n        same_speaker_mask.fill_diagonal_(False)  # Exclude self\n        same_speaker_sims = sim_matrix[same_speaker_mask]\n        \n        # Different speaker pairs (negative)\n        diff_speaker_mask = ~same_speaker_mask\n        diff_speaker_mask.fill_diagonal_(False)\n        diff_speaker_sims = sim_matrix[diff_speaker_mask]\n        \n        metrics = {\n            'pos_sim_mean': same_speaker_sims.mean().item() if len(same_speaker_sims) > 0 else 0.0,\n            'neg_sim_mean': diff_speaker_sims.mean().item() if len(diff_speaker_sims) > 0 else 0.0,\n            'separation': (same_speaker_sims.mean() - diff_speaker_sims.mean()).item() if len(same_speaker_sims) > 0 else 0.0\n        }\n        \n        return metrics\n    \n    def save_checkpoint(self, epoch: int, loss: float, filepath: str):\n        \"\"\"Save training checkpoint\"\"\"\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': self.model.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict(),\n            'loss': loss,\n            'best_val_loss': self.best_val_loss,\n            'current_step': self.current_step,\n            'patience_counter': self.patience_counter\n        }, filepath)\n        print(f\"Checkpoint saved: {filepath}\")\n    \n    def load_checkpoint(self, filepath: str) -> int:\n        \"\"\"Resume from checkpoint\"\"\"\n        if not os.path.exists(filepath):\n            print(f\"No checkpoint found at {filepath}\")\n            return 0\n        \n        checkpoint = torch.load(filepath, map_location=self.device)\n        self.model.load_state_dict(checkpoint['model_state_dict'])\n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n        self.current_step = checkpoint.get('current_step', 0)\n        \n        # RESET patience for enhancement training\n        self.patience_counter = 0  # Force reset\n        \n        epoch = checkpoint['epoch']\n        print(f\"‚úì Resumed from epoch {epoch}, best val loss: {self.best_val_loss:.4f}\")\n        print(f\"‚ö†Ô∏è  Patience counter reset to 0 for enhancement training\")\n        return epoch\n        \n    def train(self, num_epochs: int, checkpoint_dir: str = './checkpoints', start_epoch: int = 0):\n        \"\"\"Full training loop with early stopping\"\"\"\n        Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n        \n        for epoch in range(start_epoch + 1, num_epochs + 1):\n            print(f\"\\n{'='*60}\")\n            print(f\"Epoch {epoch}/{num_epochs}\")\n            print(f\"{'='*60}\")\n            \n            # Train\n            train_loss = self.train_epoch(epoch)\n            print(f\"Train Loss: {train_loss:.4f}\")\n            \n            # Validate\n            val_loss = self.validate(epoch)\n            print(f\"Val Loss: {val_loss:.4f}\")\n            \n            # Compute embedding metrics\n            metrics = self.compute_embedding_metrics(self.val_loader)\n            print(f\"Pos Sim: {metrics['pos_sim_mean']:.4f} | Neg Sim: {metrics['neg_sim_mean']:.4f} | Separation: {metrics['separation']:.4f}\")\n            \n            current_lr = self.optimizer.param_groups[0]['lr']\n            print(f\"Learning Rate: {current_lr:.6f}\")\n\n            self.logger.log_epoch(\n                epoch=epoch, phase='pretrain', train_loss=train_loss,\n                val_loss=val_loss, learning_rate=current_lr,\n                embedding_metrics=metrics, config=self.config\n            )\n            \n            # Save checkpoint every epoch\n            checkpoint_path = Path(checkpoint_dir) / f'contraeend_epoch_{epoch}.pth'\n            self.save_checkpoint(epoch, val_loss, str(checkpoint_path))\n            \n            # Early stopping check\n            if val_loss < self.best_val_loss - self.min_delta:\n                self.best_val_loss = val_loss\n                self.patience_counter = 0\n                best_path = Path(checkpoint_dir) / 'contraeend_best.pth'\n                self.save_checkpoint(epoch, val_loss, str(best_path))\n                print(f\"‚úì New best model saved! Val Loss: {val_loss:.4f}\")\n            else:\n                self.patience_counter += 1\n                print(f\"Patience: {self.patience_counter}/{self.patience}\")\n                \n                if self.patience_counter >= self.patience:\n                    print(f\"\\n‚ö†Ô∏è Early stopping triggered after {epoch} epochs\")\n                    break\n        \n        print(f\"\\nTraining completed! Best validation loss: {self.best_val_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:29:19.901626Z","iopub.execute_input":"2025-10-24T03:29:19.901917Z","iopub.status.idle":"2025-10-24T03:29:19.929674Z","shell.execute_reply.started":"2025-10-24T03:29:19.901887Z","shell.execute_reply":"2025-10-24T03:29:19.928943Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# 6. Training Script","metadata":{}},{"cell_type":"code","source":"def main():\n    \"\"\"Main training function for phase 2 v2\"\"\"\n    \n    # Set random seed for reproducibility\n    set_seed(42)\n\n    # Configuration\n    config = {\n        'audio_dir': '/kaggle/input/callhome/audio',\n        'rttm_dir': '/kaggle/input/callhome/labels',\n        'batch_size': 128,  # INCREASED from 64\n        'num_epochs': 200,   # Reduced since we have better augmentation\n        'learning_rate': 5e-5,\n        'weight_decay': 1e-4,\n        'temperature': 0.15,  # LOWERED from 0.2 (more strict)\n        'segment_length': 3.0,\n        'd_model': 128,\n        'n_layers': 6,\n        'n_heads': 4,\n        'projection_dim': 64,\n        'num_workers': 4,\n        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n        'resume_from': '/kaggle/input/contraeend/pytorch/default/1/contraeend_best.pth',  # Resume from best model\n        'cache_audio': True,\n        'use_audio_augment': True,  # NEW: Enable audio augmentation\n    }\n    \n    print(\"=\"*60)\n    print(\"ContraEEND - phase 2 v2: Contrastive Pretraining (FIXED)\")\n    print(\"=\"*60)\n    print(f\"Device: {config['device']}\")\n    print(f\"Audio dir: {config['audio_dir']}\")\n    print(f\"RTTM dir: {config['rttm_dir']}\")\n    print(f\"Batch Size: {config['batch_size']}\")\n    print(f\"Temperature: {config['temperature']}\")\n    print(f\"Model: Conformer ({config['n_layers']} layers, {config['d_model']} dim)\")\n    print(\"=\"*60)\n    \n    # Audio processor\n    audio_processor = AudioProcessor()\n    \n    # Dataset with RTTM parsing\n    print(\"\\nLoading datasets...\")\n    full_dataset = callhomeContrastiveDataset(\n        audio_dir=config['audio_dir'],\n        rttm_dir=config['rttm_dir'],\n        audio_processor=audio_processor,\n        segment_length=config['segment_length'],\n        apply_augment=True,\n        cache_audio=config['cache_audio']  # NEW: Pass cache parameter\n    )\n        \n    # Check if dataset is empty\n    if len(full_dataset) == 0:\n        print(\"\\n‚ùå ERROR: No valid samples found!\")\n        print(\"\\nPossible issues:\")\n        print(\"1. Audio files don't match RTTM file IDs\")\n        print(\"2. All segments are too short (< 2.0s)\")\n        print(\"3. No speakers have ‚â•2 segments\")\n        return\n    \n    # Dataset diagnostics\n    print(\"\\n\" + \"=\"*60)\n    print(\"üîç DATASET DIAGNOSTICS\")\n    print(\"=\"*60)\n    print(f\"Total samples: {len(full_dataset)}\")\n    print(f\"Total speakers: {len(full_dataset.speakers)}\")\n    \n    if len(full_dataset.speakers) > 0:\n        speaker_counts = {spk: len(segs) for spk, segs in full_dataset.speaker_segments.items()}\n        segments_per_speaker = list(speaker_counts.values())\n        \n        print(f\"Avg segments per speaker: {np.mean(segments_per_speaker):.1f}\")\n        print(f\"Min segments per speaker: {min(segments_per_speaker)}\")\n        print(f\"Max segments per speaker: {max(segments_per_speaker)}\")\n        \n        print(f\"\\nSample speakers: {full_dataset.speakers[:5]}\")\n    \n    # Check if dataset is sufficient\n    if len(full_dataset) < 200:\n        print(\"\\n‚ö†Ô∏è  WARNING: Dataset is small!\")\n        print(f\"   Current: {len(full_dataset)} samples\")\n        print(f\"   This may limit model performance\")\n    \n    if len(full_dataset.speakers) < 20:\n        print(\"\\n‚ö†Ô∏è  WARNING: Few speakers!\")\n        print(f\"   Current: {len(full_dataset.speakers)} speakers\")\n        print(f\"   Contrastive learning works best with 100+ speakers\")\n    \n    print(\"=\"*60 + \"\\n\")\n    \n    # Split for validation (90/10)\n    train_size = int(0.9 * len(full_dataset))\n    val_size = len(full_dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(\n        full_dataset, [train_size, val_size],\n        generator=torch.Generator().manual_seed(42)\n    )\n    \n    print(f\"Train samples: {len(train_dataset)}\")\n    print(f\"Val samples: {len(val_dataset)}\")\n    \n    # Adjust batch size and accumulation if needed\n    if len(train_dataset) < config['batch_size'] * 4:\n        print(\"\\n‚ö†Ô∏è  Small dataset: adjusting batch size and disabling accumulation\")\n        config['batch_size'] = min(16, len(train_dataset) // 2)\n        accumulation_steps = 1\n    else:\n        accumulation_steps = 4\n    \n    print(f\"Batch size: {config['batch_size']}\")\n    print(f\"Accumulation steps: {accumulation_steps}\")\n    print(f\"Effective batch size: {config['batch_size'] * accumulation_steps * 2}\")\n    \n    # Data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config['batch_size'],\n        shuffle=True,\n        num_workers=config['num_workers'],\n        pin_memory=True if config['device'] == 'cuda' else False,\n        drop_last=True if len(train_dataset) > config['batch_size'] else False,\n        persistent_workers=True if config['num_workers'] > 0 else False  # NEW\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=config['batch_size'],\n        shuffle=False,\n        num_workers=config['num_workers'],\n        pin_memory=True if config['device'] == 'cuda' else False,\n        persistent_workers=True if config['num_workers'] > 0 else False  # NEW\n    )\n        \n    print(f\"Train batches: {len(train_loader)}\")\n    print(f\"Val batches: {len(val_loader)}\")\n    \n    # Model\n    print(\"\\nInitializing model...\")\n    model = ContraEENDEncoder(\n        input_dim=83,\n        d_model=config['d_model'],\n        n_layers=config['n_layers'],\n        n_heads=config['n_heads'],\n        projection_dim=config['projection_dim']\n    )\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    \n    # Trainer\n    print(\"\\nInitializing trainer...\")\n    trainer = ContrastiveTrainer(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        num_epochs=config['num_epochs'],\n        device=config['device'],\n        learning_rate=config['learning_rate'],\n        weight_decay=config['weight_decay'],\n        temperature=config['temperature'],\n        accumulation_steps=8 # INCREASED from 4: Effective batch = 128*8*2 = 2048\n    )\n    \n    # Resume from checkpoint if specified\n    start_epoch = 0\n    if config['resume_from'] is not None and os.path.exists(config['resume_from']):\n        print(f\"\\nResuming from checkpoint: {config['resume_from']}\")\n        start_epoch = trainer.load_checkpoint(config['resume_from'])\n        # FORCE RESET PATIENCE\n        trainer.patience_counter = 0\n        trainer.patience = 15  # more patience\n        print(f\"‚ö†Ô∏è  Patience reset to 0/15 for enhancement training\")\n    \n    \n    # Train\n    print(\"\\nStarting training...\")\n    print(\"Features enabled:\")\n    print(\"=\"*60)\n    \n    trainer.train(\n        num_epochs=config['num_epochs'],\n        start_epoch=start_epoch\n    )\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"== Training Complete! ==\")\n    print(f\"Best Validation Loss: {trainer.best_val_loss:.4f}\")\n    print(\"=\"*60)\n\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T03:29:19.931684Z","iopub.execute_input":"2025-10-24T03:29:19.931850Z","iopub.status.idle":"2025-10-24T04:17:21.350786Z","shell.execute_reply.started":"2025-10-24T03:29:19.931837Z","shell.execute_reply":"2025-10-24T04:17:21.349538Z"}},"outputs":[{"name":"stdout","text":"============================================================\nContraEEND - Phase 1: Contrastive Pretraining (FIXED)\n============================================================\nDevice: cuda\nAudio dir: /kaggle/input/callhome/audio\nRTTM dir: /kaggle/input/callhome/labels\nBatch Size: 128\nTemperature: 0.15\nModel: Conformer (6 layers, 128 dim)\n============================================================\n\nLoading datasets...\n\n============================================================\nBuilding audio-RTTM mapping...\n============================================================\n\nFound 140 audio files\nFound 140 RTTM files\nMatched 140 audio-RTTM pairs\n\nExample pairs:\n  audio_0.wav <-> labels_0.rttm\n  audio_1.wav <-> labels_1.rttm\n  audio_2.wav <-> labels_2.rttm\n\nParsing 140 audio-RTTM pairs...\n","output_type":"stream"},{"name":"stderr","text":"Parsing RTTM: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:00<00:00, 172.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Total segments found: 33471\nSpeakers after filtering (‚â•2 segments): 289\n  Removed 5 speakers with <2 segments\n\nSegment statistics:\n  Per speaker - Min: 2, Max: 226, Avg: 115.8\n  Duration - Min: 0.0s, Max: 21.1s, Avg: 2.1s\n\nüì¶ Caching audio files in memory...\nLoading 140 unique audio files...\n","output_type":"stream"},{"name":"stderr","text":"Caching audio: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140/140 [00:41<00:00,  3.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"‚úì Cached 140 audio files\n\n‚úì Loaded 289 speakers, 12876 segments\n  Segment length: 3.0s, Min length: 2.0s\n  SpecAugment: enabled\n  Audio augmentation: enabled\n  Audio caching: enabled\n\n============================================================\nüîç DATASET DIAGNOSTICS\n============================================================\nTotal samples: 12876\nTotal speakers: 289\nAvg segments per speaker: 115.8\nMin segments per speaker: 2\nMax segments per speaker: 226\n\nSample speakers: ['file0_spkA', 'file0_spkB', 'file1_spkA', 'file1_spkB', 'file2_spkB']\n============================================================\n\nTrain samples: 11588\nVal samples: 1288\nBatch size: 128\nAccumulation steps: 4\nEffective batch size: 1024\nTrain batches: 90\nVal batches: 11\n\nInitializing model...\nTotal parameters: 2,413,888\nTrainable parameters: 2,413,888\n\nInitializing trainer...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3561524113.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else None\n","output_type":"stream"},{"name":"stdout","text":"üìä Metrics logger initialized: logs/contraeend_20251024_033005.csv\nEffective batch size: 2048\n\nResuming from checkpoint: /kaggle/input/contraeend/pytorch/default/1/contraeend_best.pth\n‚úì Resumed from epoch 83, best val loss: 1.8993\n‚ö†Ô∏è  Patience counter reset to 0 for enhancement training\n‚ö†Ô∏è  Patience reset to 0/15 for enhancement training\n\nStarting training...\nFeatures enabled:\n============================================================\n\n============================================================\nEpoch 84/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 84 [Train]:   0%|          | 0/90 [00:00<?, ?it/s]/tmp/ipykernel_36/3561524113.py:92: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=self.use_amp):\nEpoch 84 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.36it/s, loss=2.2362, lr=0.000458]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.4208\n","output_type":"stream"},{"name":"stderr","text":"Epoch 84 [Val]:   0%|          | 0/11 [00:00<?, ?it/s]/tmp/ipykernel_36/3561524113.py:158: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=self.use_amp):\nEpoch 84 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.32it/s, loss=0.4872]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 2.0381\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/3561524113.py:177: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=self.use_amp):\n","output_type":"stream"},{"name":"stdout","text":"Pos Sim: 0.7280 | Neg Sim: 0.0090 | Separation: 0.7190\nLearning Rate: 0.000458\nCheckpoint saved: checkpoints/contraeend_epoch_84.pth\nPatience: 1/15\n\n============================================================\nEpoch 85/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 85 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.66it/s, loss=2.0872, lr=0.000457]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.1253\n","output_type":"stream"},{"name":"stderr","text":"Epoch 85 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.36it/s, loss=0.4705]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.9136\nPos Sim: 0.7612 | Neg Sim: 0.0057 | Separation: 0.7555\nLearning Rate: 0.000457\nCheckpoint saved: checkpoints/contraeend_epoch_85.pth\nPatience: 2/15\n\n============================================================\nEpoch 86/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 86 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:18<00:00,  4.77it/s, loss=1.9109, lr=0.000457]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 2.0034\n","output_type":"stream"},{"name":"stderr","text":"Epoch 86 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.26it/s, loss=0.4677]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.9148\nPos Sim: 0.7660 | Neg Sim: 0.0033 | Separation: 0.7628\nLearning Rate: 0.000457\nCheckpoint saved: checkpoints/contraeend_epoch_86.pth\nPatience: 3/15\n\n============================================================\nEpoch 87/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 87 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.68it/s, loss=1.8949, lr=0.000457]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9580\n","output_type":"stream"},{"name":"stderr","text":"Epoch 87 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.40it/s, loss=0.3797]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.8521\nPos Sim: 0.7529 | Neg Sim: 0.0041 | Separation: 0.7488\nLearning Rate: 0.000457\nCheckpoint saved: checkpoints/contraeend_epoch_87.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.8521\n\n============================================================\nEpoch 88/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 88 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:18<00:00,  4.75it/s, loss=1.8997, lr=0.000457]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.9174\n","output_type":"stream"},{"name":"stderr","text":"Epoch 88 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.39it/s, loss=0.4107]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.7992\nPos Sim: 0.7847 | Neg Sim: 0.0052 | Separation: 0.7794\nLearning Rate: 0.000457\nCheckpoint saved: checkpoints/contraeend_epoch_88.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.7992\n\n============================================================\nEpoch 89/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 89 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:18<00:00,  4.82it/s, loss=1.8797, lr=0.000456]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8968\n","output_type":"stream"},{"name":"stderr","text":"Epoch 89 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.36it/s, loss=0.8494]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.8457\nPos Sim: 0.7936 | Neg Sim: 0.0027 | Separation: 0.7909\nLearning Rate: 0.000456\nCheckpoint saved: checkpoints/contraeend_epoch_89.pth\nPatience: 1/15\n\n============================================================\nEpoch 90/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 90 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:18<00:00,  4.81it/s, loss=1.8758, lr=0.000456]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8691\n","output_type":"stream"},{"name":"stderr","text":"Epoch 90 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.03it/s, loss=0.5443]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.8051\nPos Sim: 0.7762 | Neg Sim: 0.0032 | Separation: 0.7730\nLearning Rate: 0.000456\nCheckpoint saved: checkpoints/contraeend_epoch_90.pth\nPatience: 2/15\n\n============================================================\nEpoch 91/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 91 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:18<00:00,  4.86it/s, loss=1.7773, lr=0.000456]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8322\n","output_type":"stream"},{"name":"stderr","text":"Epoch 91 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:01<00:00,  5.67it/s, loss=0.2604]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.7381\nPos Sim: 0.7868 | Neg Sim: 0.0020 | Separation: 0.7849\nLearning Rate: 0.000456\nCheckpoint saved: checkpoints/contraeend_epoch_91.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.7381\n\n============================================================\nEpoch 92/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 92 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:18<00:00,  4.80it/s, loss=1.8168, lr=0.000456]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8273\n","output_type":"stream"},{"name":"stderr","text":"Epoch 92 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:01<00:00,  5.52it/s, loss=0.7587]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.8012\nPos Sim: 0.7826 | Neg Sim: 0.0025 | Separation: 0.7801\nLearning Rate: 0.000456\nCheckpoint saved: checkpoints/contraeend_epoch_92.pth\nPatience: 1/15\n\n============================================================\nEpoch 93/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 93 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:18<00:00,  4.77it/s, loss=1.8122, lr=0.000455]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8188\n","output_type":"stream"},{"name":"stderr","text":"Epoch 93 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.49it/s, loss=0.3023]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.7097\nPos Sim: 0.7919 | Neg Sim: 0.0042 | Separation: 0.7877\nLearning Rate: 0.000455\nCheckpoint saved: checkpoints/contraeend_epoch_93.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.7097\n\n============================================================\nEpoch 94/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 94 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.73it/s, loss=1.7847, lr=0.000455]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8189\n","output_type":"stream"},{"name":"stderr","text":"Epoch 94 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.36it/s, loss=0.3932]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.7601\nPos Sim: 0.7806 | Neg Sim: 0.0043 | Separation: 0.7762\nLearning Rate: 0.000455\nCheckpoint saved: checkpoints/contraeend_epoch_94.pth\nPatience: 1/15\n\n============================================================\nEpoch 95/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 95 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:18<00:00,  4.79it/s, loss=1.7645, lr=0.000455]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.8074\n","output_type":"stream"},{"name":"stderr","text":"Epoch 95 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.40it/s, loss=0.4560]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.7562\nPos Sim: 0.7963 | Neg Sim: 0.0033 | Separation: 0.7930\nLearning Rate: 0.000455\nCheckpoint saved: checkpoints/contraeend_epoch_95.pth\nPatience: 2/15\n\n============================================================\nEpoch 96/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 96 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:18<00:00,  4.81it/s, loss=1.7602, lr=0.000454]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7911\n","output_type":"stream"},{"name":"stderr","text":"Epoch 96 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.22it/s, loss=0.7154]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6901\nPos Sim: 0.8052 | Neg Sim: 0.0022 | Separation: 0.8031\nLearning Rate: 0.000454\nCheckpoint saved: checkpoints/contraeend_epoch_96.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.6901\n\n============================================================\nEpoch 97/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 97 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:18<00:00,  4.75it/s, loss=1.7997, lr=0.000454]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7759\n","output_type":"stream"},{"name":"stderr","text":"Epoch 97 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.83it/s, loss=0.4194]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.7035\nPos Sim: 0.7984 | Neg Sim: 0.0024 | Separation: 0.7960\nLearning Rate: 0.000454\nCheckpoint saved: checkpoints/contraeend_epoch_97.pth\nPatience: 1/15\n\n============================================================\nEpoch 98/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 98 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.69it/s, loss=1.7586, lr=0.000454]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7806\n","output_type":"stream"},{"name":"stderr","text":"Epoch 98 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.40it/s, loss=0.7430]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6965\nPos Sim: 0.8027 | Neg Sim: 0.0008 | Separation: 0.8019\nLearning Rate: 0.000454\nCheckpoint saved: checkpoints/contraeend_epoch_98.pth\nPatience: 2/15\n\n============================================================\nEpoch 99/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 99 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:18<00:00,  4.80it/s, loss=1.7315, lr=0.000454]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7507\n","output_type":"stream"},{"name":"stderr","text":"Epoch 99 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.35it/s, loss=0.4772]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6951\nPos Sim: 0.7960 | Neg Sim: 0.0035 | Separation: 0.7925\nLearning Rate: 0.000454\nCheckpoint saved: checkpoints/contraeend_epoch_99.pth\nPatience: 3/15\n\n============================================================\nEpoch 100/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 100 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.60it/s, loss=1.7572, lr=0.000453]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7594\n","output_type":"stream"},{"name":"stderr","text":"Epoch 100 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.18it/s, loss=1.0919]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.7616\nPos Sim: 0.7744 | Neg Sim: 0.0034 | Separation: 0.7710\nLearning Rate: 0.000453\nCheckpoint saved: checkpoints/contraeend_epoch_100.pth\nPatience: 4/15\n\n============================================================\nEpoch 101/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 101 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.65it/s, loss=1.7357, lr=0.000453]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7621\n","output_type":"stream"},{"name":"stderr","text":"Epoch 101 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.24it/s, loss=0.1688]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.7012\nPos Sim: 0.7967 | Neg Sim: 0.0046 | Separation: 0.7921\nLearning Rate: 0.000453\nCheckpoint saved: checkpoints/contraeend_epoch_101.pth\nPatience: 5/15\n\n============================================================\nEpoch 102/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 102 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.50it/s, loss=1.6995, lr=0.000453]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7562\n","output_type":"stream"},{"name":"stderr","text":"Epoch 102 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.04it/s, loss=0.4925]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6458\nPos Sim: 0.8169 | Neg Sim: 0.0002 | Separation: 0.8166\nLearning Rate: 0.000453\nCheckpoint saved: checkpoints/contraeend_epoch_102.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.6458\n\n============================================================\nEpoch 103/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 103 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.46it/s, loss=1.6979, lr=0.000452]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7223\n","output_type":"stream"},{"name":"stderr","text":"Epoch 103 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.09it/s, loss=0.7831]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6801\nPos Sim: 0.7972 | Neg Sim: 0.0019 | Separation: 0.7953\nLearning Rate: 0.000452\nCheckpoint saved: checkpoints/contraeend_epoch_103.pth\nPatience: 1/15\n\n============================================================\nEpoch 104/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 104 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.48it/s, loss=1.7335, lr=0.000452]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7235\n","output_type":"stream"},{"name":"stderr","text":"Epoch 104 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.99it/s, loss=0.4983]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6435\nPos Sim: 0.8063 | Neg Sim: 0.0015 | Separation: 0.8048\nLearning Rate: 0.000452\nCheckpoint saved: checkpoints/contraeend_epoch_104.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.6435\n\n============================================================\nEpoch 105/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 105 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.46it/s, loss=1.7182, lr=0.000452]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7292\n","output_type":"stream"},{"name":"stderr","text":"Epoch 105 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.99it/s, loss=0.3849]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6339\nPos Sim: 0.8110 | Neg Sim: 0.0007 | Separation: 0.8102\nLearning Rate: 0.000452\nCheckpoint saved: checkpoints/contraeend_epoch_105.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.6339\n\n============================================================\nEpoch 106/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 106 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.50it/s, loss=1.7170, lr=0.000452]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.7015\n","output_type":"stream"},{"name":"stderr","text":"Epoch 106 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.09it/s, loss=0.1753]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6531\nPos Sim: 0.7883 | Neg Sim: 0.0008 | Separation: 0.7875\nLearning Rate: 0.000452\nCheckpoint saved: checkpoints/contraeend_epoch_106.pth\nPatience: 1/15\n\n============================================================\nEpoch 107/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 107 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.47it/s, loss=1.6393, lr=0.000451]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6987\n","output_type":"stream"},{"name":"stderr","text":"Epoch 107 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.07it/s, loss=0.3907]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6563\nPos Sim: 0.8127 | Neg Sim: 0.0036 | Separation: 0.8091\nLearning Rate: 0.000451\nCheckpoint saved: checkpoints/contraeend_epoch_107.pth\nPatience: 2/15\n\n============================================================\nEpoch 108/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 108 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.43it/s, loss=1.6439, lr=0.000451]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6974\n","output_type":"stream"},{"name":"stderr","text":"Epoch 108 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.14it/s, loss=0.6497]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6434\nPos Sim: 0.8186 | Neg Sim: 0.0018 | Separation: 0.8167\nLearning Rate: 0.000451\nCheckpoint saved: checkpoints/contraeend_epoch_108.pth\nPatience: 3/15\n\n============================================================\nEpoch 109/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 109 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.46it/s, loss=1.7062, lr=0.000451]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6929\n","output_type":"stream"},{"name":"stderr","text":"Epoch 109 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.99it/s, loss=0.5929]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6524\nPos Sim: 0.7869 | Neg Sim: 0.0015 | Separation: 0.7855\nLearning Rate: 0.000451\nCheckpoint saved: checkpoints/contraeend_epoch_109.pth\nPatience: 4/15\n\n============================================================\nEpoch 110/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 110 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.41it/s, loss=1.6424, lr=0.000450]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6746\n","output_type":"stream"},{"name":"stderr","text":"Epoch 110 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.94it/s, loss=0.3562]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6480\nPos Sim: 0.8055 | Neg Sim: 0.0011 | Separation: 0.8044\nLearning Rate: 0.000450\nCheckpoint saved: checkpoints/contraeend_epoch_110.pth\nPatience: 5/15\n\n============================================================\nEpoch 111/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 111 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.37it/s, loss=1.7165, lr=0.000450]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6881\n","output_type":"stream"},{"name":"stderr","text":"Epoch 111 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.89it/s, loss=0.7499]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6159\nPos Sim: 0.8109 | Neg Sim: 0.0019 | Separation: 0.8090\nLearning Rate: 0.000450\nCheckpoint saved: checkpoints/contraeend_epoch_111.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.6159\n\n============================================================\nEpoch 112/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 112 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.33it/s, loss=1.6399, lr=0.000450]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6725\n","output_type":"stream"},{"name":"stderr","text":"Epoch 112 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.03it/s, loss=0.3469]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5774\nPos Sim: 0.8210 | Neg Sim: 0.0001 | Separation: 0.8208\nLearning Rate: 0.000450\nCheckpoint saved: checkpoints/contraeend_epoch_112.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.5774\n\n============================================================\nEpoch 113/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 113 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.50it/s, loss=1.7104, lr=0.000449]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6642\n","output_type":"stream"},{"name":"stderr","text":"Epoch 113 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.87it/s, loss=0.2197]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5374\nPos Sim: 0.7973 | Neg Sim: 0.0003 | Separation: 0.7970\nLearning Rate: 0.000449\nCheckpoint saved: checkpoints/contraeend_epoch_113.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.5374\n\n============================================================\nEpoch 114/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 114 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.37it/s, loss=1.7116, lr=0.000449]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6485\n","output_type":"stream"},{"name":"stderr","text":"Epoch 114 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.02it/s, loss=0.2343]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5858\nPos Sim: 0.8058 | Neg Sim: 0.0035 | Separation: 0.8023\nLearning Rate: 0.000449\nCheckpoint saved: checkpoints/contraeend_epoch_114.pth\nPatience: 1/15\n\n============================================================\nEpoch 115/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 115 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.42it/s, loss=1.6686, lr=0.000449]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6541\n","output_type":"stream"},{"name":"stderr","text":"Epoch 115 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.86it/s, loss=0.5895]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.6121\nPos Sim: 0.8103 | Neg Sim: 0.0011 | Separation: 0.8093\nLearning Rate: 0.000449\nCheckpoint saved: checkpoints/contraeend_epoch_115.pth\nPatience: 2/15\n\n============================================================\nEpoch 116/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 116 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.57it/s, loss=1.6313, lr=0.000449]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6376\n","output_type":"stream"},{"name":"stderr","text":"Epoch 116 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.17it/s, loss=0.3218]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5620\nPos Sim: 0.8224 | Neg Sim: 0.0002 | Separation: 0.8222\nLearning Rate: 0.000449\nCheckpoint saved: checkpoints/contraeend_epoch_116.pth\nPatience: 3/15\n\n============================================================\nEpoch 117/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 117 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.54it/s, loss=1.6225, lr=0.000448]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6378\n","output_type":"stream"},{"name":"stderr","text":"Epoch 117 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.05it/s, loss=0.3343]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5290\nPos Sim: 0.8249 | Neg Sim: 0.0007 | Separation: 0.8242\nLearning Rate: 0.000448\nCheckpoint saved: checkpoints/contraeend_epoch_117.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.5290\n\n============================================================\nEpoch 118/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 118 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.47it/s, loss=1.5966, lr=0.000448]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6202\n","output_type":"stream"},{"name":"stderr","text":"Epoch 118 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.09it/s, loss=0.2917]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5549\nPos Sim: 0.8294 | Neg Sim: 0.0009 | Separation: 0.8285\nLearning Rate: 0.000448\nCheckpoint saved: checkpoints/contraeend_epoch_118.pth\nPatience: 1/15\n\n============================================================\nEpoch 119/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 119 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.54it/s, loss=1.6314, lr=0.000448]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6390\n","output_type":"stream"},{"name":"stderr","text":"Epoch 119 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.17it/s, loss=0.4577]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5541\nPos Sim: 0.8088 | Neg Sim: 0.0001 | Separation: 0.8086\nLearning Rate: 0.000448\nCheckpoint saved: checkpoints/contraeend_epoch_119.pth\nPatience: 2/15\n\n============================================================\nEpoch 120/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 120 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.57it/s, loss=1.6786, lr=0.000447]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6053\n","output_type":"stream"},{"name":"stderr","text":"Epoch 120 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.25it/s, loss=0.3096]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5636\nPos Sim: 0.8134 | Neg Sim: 0.0016 | Separation: 0.8118\nLearning Rate: 0.000447\nCheckpoint saved: checkpoints/contraeend_epoch_120.pth\nPatience: 3/15\n\n============================================================\nEpoch 121/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 121 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.47it/s, loss=1.5797, lr=0.000447]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6082\n","output_type":"stream"},{"name":"stderr","text":"Epoch 121 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.03it/s, loss=0.3384]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5561\nPos Sim: 0.8066 | Neg Sim: 0.0020 | Separation: 0.8046\nLearning Rate: 0.000447\nCheckpoint saved: checkpoints/contraeend_epoch_121.pth\nPatience: 4/15\n\n============================================================\nEpoch 122/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 122 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.51it/s, loss=1.6158, lr=0.000447]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.6050\n","output_type":"stream"},{"name":"stderr","text":"Epoch 122 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.94it/s, loss=0.2877]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5197\nPos Sim: 0.8233 | Neg Sim: -0.0003 | Separation: 0.8236\nLearning Rate: 0.000447\nCheckpoint saved: checkpoints/contraeend_epoch_122.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.5197\n\n============================================================\nEpoch 123/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 123 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.54it/s, loss=1.5530, lr=0.000446]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5927\n","output_type":"stream"},{"name":"stderr","text":"Epoch 123 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.08it/s, loss=0.5895]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5457\nPos Sim: 0.8148 | Neg Sim: 0.0011 | Separation: 0.8137\nLearning Rate: 0.000446\nCheckpoint saved: checkpoints/contraeend_epoch_123.pth\nPatience: 1/15\n\n============================================================\nEpoch 124/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 124 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.55it/s, loss=1.5548, lr=0.000446]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5899\n","output_type":"stream"},{"name":"stderr","text":"Epoch 124 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.88it/s, loss=0.2197]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5067\nPos Sim: 0.8174 | Neg Sim: 0.0011 | Separation: 0.8163\nLearning Rate: 0.000446\nCheckpoint saved: checkpoints/contraeend_epoch_124.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.5067\n\n============================================================\nEpoch 125/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 125 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.61it/s, loss=1.5706, lr=0.000446]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5862\n","output_type":"stream"},{"name":"stderr","text":"Epoch 125 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.27it/s, loss=0.3940]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5360\nPos Sim: 0.8196 | Neg Sim: 0.0028 | Separation: 0.8168\nLearning Rate: 0.000446\nCheckpoint saved: checkpoints/contraeend_epoch_125.pth\nPatience: 1/15\n\n============================================================\nEpoch 126/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 126 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.61it/s, loss=1.6131, lr=0.000446]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5768\n","output_type":"stream"},{"name":"stderr","text":"Epoch 126 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.31it/s, loss=0.3238]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5426\nPos Sim: 0.8236 | Neg Sim: 0.0006 | Separation: 0.8230\nLearning Rate: 0.000446\nCheckpoint saved: checkpoints/contraeend_epoch_126.pth\nPatience: 2/15\n\n============================================================\nEpoch 127/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 127 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.58it/s, loss=1.5179, lr=0.000445]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5622\n","output_type":"stream"},{"name":"stderr","text":"Epoch 127 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.21it/s, loss=0.4712]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5642\nPos Sim: 0.8130 | Neg Sim: 0.0004 | Separation: 0.8126\nLearning Rate: 0.000445\nCheckpoint saved: checkpoints/contraeend_epoch_127.pth\nPatience: 3/15\n\n============================================================\nEpoch 128/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 128 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.40it/s, loss=1.5417, lr=0.000445]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5567\n","output_type":"stream"},{"name":"stderr","text":"Epoch 128 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.07it/s, loss=0.2920]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5408\nPos Sim: 0.8134 | Neg Sim: 0.0004 | Separation: 0.8130\nLearning Rate: 0.000445\nCheckpoint saved: checkpoints/contraeend_epoch_128.pth\nPatience: 4/15\n\n============================================================\nEpoch 129/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 129 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.50it/s, loss=1.5337, lr=0.000445]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5625\n","output_type":"stream"},{"name":"stderr","text":"Epoch 129 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.97it/s, loss=0.7451]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5202\nPos Sim: 0.8338 | Neg Sim: -0.0006 | Separation: 0.8344\nLearning Rate: 0.000445\nCheckpoint saved: checkpoints/contraeend_epoch_129.pth\nPatience: 5/15\n\n============================================================\nEpoch 130/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 130 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.58it/s, loss=1.5837, lr=0.000444]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5702\n","output_type":"stream"},{"name":"stderr","text":"Epoch 130 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.15it/s, loss=0.3870]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5631\nPos Sim: 0.8141 | Neg Sim: 0.0031 | Separation: 0.8110\nLearning Rate: 0.000444\nCheckpoint saved: checkpoints/contraeend_epoch_130.pth\nPatience: 6/15\n\n============================================================\nEpoch 131/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 131 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.50it/s, loss=1.6131, lr=0.000444]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5737\n","output_type":"stream"},{"name":"stderr","text":"Epoch 131 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.12it/s, loss=0.4057]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5423\nPos Sim: 0.8063 | Neg Sim: 0.0001 | Separation: 0.8062\nLearning Rate: 0.000444\nCheckpoint saved: checkpoints/contraeend_epoch_131.pth\nPatience: 7/15\n\n============================================================\nEpoch 132/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 132 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.54it/s, loss=1.5360, lr=0.000444]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5474\n","output_type":"stream"},{"name":"stderr","text":"Epoch 132 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.06it/s, loss=0.1852]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4284\nPos Sim: 0.8250 | Neg Sim: -0.0009 | Separation: 0.8259\nLearning Rate: 0.000444\nCheckpoint saved: checkpoints/contraeend_epoch_132.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.4284\n\n============================================================\nEpoch 133/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 133 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.55it/s, loss=1.5617, lr=0.000443]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5632\n","output_type":"stream"},{"name":"stderr","text":"Epoch 133 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.95it/s, loss=0.2913]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5212\nPos Sim: 0.8187 | Neg Sim: 0.0026 | Separation: 0.8161\nLearning Rate: 0.000443\nCheckpoint saved: checkpoints/contraeend_epoch_133.pth\nPatience: 1/15\n\n============================================================\nEpoch 134/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 134 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.64it/s, loss=1.6118, lr=0.000443]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5445\n","output_type":"stream"},{"name":"stderr","text":"Epoch 134 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.11it/s, loss=0.4324]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4747\nPos Sim: 0.8233 | Neg Sim: 0.0014 | Separation: 0.8219\nLearning Rate: 0.000443\nCheckpoint saved: checkpoints/contraeend_epoch_134.pth\nPatience: 2/15\n\n============================================================\nEpoch 135/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 135 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.54it/s, loss=1.5647, lr=0.000443]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5553\n","output_type":"stream"},{"name":"stderr","text":"Epoch 135 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.05it/s, loss=0.3477]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4844\nPos Sim: 0.8155 | Neg Sim: 0.0009 | Separation: 0.8147\nLearning Rate: 0.000443\nCheckpoint saved: checkpoints/contraeend_epoch_135.pth\nPatience: 3/15\n\n============================================================\nEpoch 136/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 136 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.52it/s, loss=1.4347, lr=0.000442]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5330\n","output_type":"stream"},{"name":"stderr","text":"Epoch 136 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.15it/s, loss=0.2395]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5001\nPos Sim: 0.8178 | Neg Sim: 0.0026 | Separation: 0.8152\nLearning Rate: 0.000442\nCheckpoint saved: checkpoints/contraeend_epoch_136.pth\nPatience: 4/15\n\n============================================================\nEpoch 137/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 137 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.49it/s, loss=1.5084, lr=0.000442]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5195\n","output_type":"stream"},{"name":"stderr","text":"Epoch 137 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.02it/s, loss=0.5712]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5663\nPos Sim: 0.8149 | Neg Sim: 0.0029 | Separation: 0.8120\nLearning Rate: 0.000442\nCheckpoint saved: checkpoints/contraeend_epoch_137.pth\nPatience: 5/15\n\n============================================================\nEpoch 138/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 138 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.56it/s, loss=1.5346, lr=0.000442]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5244\n","output_type":"stream"},{"name":"stderr","text":"Epoch 138 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.09it/s, loss=0.5105]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4822\nPos Sim: 0.8357 | Neg Sim: 0.0024 | Separation: 0.8333\nLearning Rate: 0.000442\nCheckpoint saved: checkpoints/contraeend_epoch_138.pth\nPatience: 6/15\n\n============================================================\nEpoch 139/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 139 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.54it/s, loss=1.5142, lr=0.000441]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4995\n","output_type":"stream"},{"name":"stderr","text":"Epoch 139 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.16it/s, loss=0.6228]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5074\nPos Sim: 0.8272 | Neg Sim: -0.0004 | Separation: 0.8276\nLearning Rate: 0.000441\nCheckpoint saved: checkpoints/contraeend_epoch_139.pth\nPatience: 7/15\n\n============================================================\nEpoch 140/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 140 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.59it/s, loss=1.5457, lr=0.000441]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5369\n","output_type":"stream"},{"name":"stderr","text":"Epoch 140 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.22it/s, loss=0.2679]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4899\nPos Sim: 0.8148 | Neg Sim: 0.0026 | Separation: 0.8122\nLearning Rate: 0.000441\nCheckpoint saved: checkpoints/contraeend_epoch_140.pth\nPatience: 8/15\n\n============================================================\nEpoch 141/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 141 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.54it/s, loss=1.5370, lr=0.000441]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5240\n","output_type":"stream"},{"name":"stderr","text":"Epoch 141 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.98it/s, loss=0.1960]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4727\nPos Sim: 0.8309 | Neg Sim: -0.0007 | Separation: 0.8316\nLearning Rate: 0.000441\nCheckpoint saved: checkpoints/contraeend_epoch_141.pth\nPatience: 9/15\n\n============================================================\nEpoch 142/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 142 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.62it/s, loss=1.5342, lr=0.000441]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5012\n","output_type":"stream"},{"name":"stderr","text":"Epoch 142 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.14it/s, loss=0.3149]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4568\nPos Sim: 0.8228 | Neg Sim: -0.0004 | Separation: 0.8232\nLearning Rate: 0.000441\nCheckpoint saved: checkpoints/contraeend_epoch_142.pth\nPatience: 10/15\n\n============================================================\nEpoch 143/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 143 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.52it/s, loss=1.5297, lr=0.000440]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5146\n","output_type":"stream"},{"name":"stderr","text":"Epoch 143 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.96it/s, loss=0.5472]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.5412\nPos Sim: 0.8245 | Neg Sim: 0.0033 | Separation: 0.8212\nLearning Rate: 0.000440\nCheckpoint saved: checkpoints/contraeend_epoch_143.pth\nPatience: 11/15\n\n============================================================\nEpoch 144/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 144 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.59it/s, loss=1.4415, lr=0.000440]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5122\n","output_type":"stream"},{"name":"stderr","text":"Epoch 144 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.16it/s, loss=0.6552]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4764\nPos Sim: 0.8181 | Neg Sim: 0.0000 | Separation: 0.8181\nLearning Rate: 0.000440\nCheckpoint saved: checkpoints/contraeend_epoch_144.pth\nPatience: 12/15\n\n============================================================\nEpoch 145/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 145 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.52it/s, loss=1.5058, lr=0.000440]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.5016\n","output_type":"stream"},{"name":"stderr","text":"Epoch 145 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.16it/s, loss=0.1275]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3965\nPos Sim: 0.8556 | Neg Sim: -0.0012 | Separation: 0.8568\nLearning Rate: 0.000440\nCheckpoint saved: checkpoints/contraeend_epoch_145.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.3965\n\n============================================================\nEpoch 146/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 146 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.53it/s, loss=1.4497, lr=0.000439]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4794\n","output_type":"stream"},{"name":"stderr","text":"Epoch 146 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.02it/s, loss=0.1972]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4612\nPos Sim: 0.8294 | Neg Sim: 0.0014 | Separation: 0.8281\nLearning Rate: 0.000439\nCheckpoint saved: checkpoints/contraeend_epoch_146.pth\nPatience: 1/15\n\n============================================================\nEpoch 147/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 147 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.53it/s, loss=1.4953, lr=0.000439]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4847\n","output_type":"stream"},{"name":"stderr","text":"Epoch 147 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.05it/s, loss=0.6808]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4880\nPos Sim: 0.8446 | Neg Sim: 0.0017 | Separation: 0.8429\nLearning Rate: 0.000439\nCheckpoint saved: checkpoints/contraeend_epoch_147.pth\nPatience: 2/15\n\n============================================================\nEpoch 148/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 148 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.49it/s, loss=1.4533, lr=0.000439]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4833\n","output_type":"stream"},{"name":"stderr","text":"Epoch 148 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.81it/s, loss=0.3807]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4296\nPos Sim: 0.8357 | Neg Sim: 0.0001 | Separation: 0.8356\nLearning Rate: 0.000439\nCheckpoint saved: checkpoints/contraeend_epoch_148.pth\nPatience: 3/15\n\n============================================================\nEpoch 149/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 149 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.56it/s, loss=1.5022, lr=0.000438]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4716\n","output_type":"stream"},{"name":"stderr","text":"Epoch 149 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.18it/s, loss=0.6569]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4898\nPos Sim: 0.8407 | Neg Sim: -0.0004 | Separation: 0.8410\nLearning Rate: 0.000438\nCheckpoint saved: checkpoints/contraeend_epoch_149.pth\nPatience: 4/15\n\n============================================================\nEpoch 150/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 150 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.56it/s, loss=1.5257, lr=0.000438]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4678\n","output_type":"stream"},{"name":"stderr","text":"Epoch 150 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.37it/s, loss=0.1860]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4437\nPos Sim: 0.8449 | Neg Sim: 0.0003 | Separation: 0.8446\nLearning Rate: 0.000438\nCheckpoint saved: checkpoints/contraeend_epoch_150.pth\nPatience: 5/15\n\n============================================================\nEpoch 151/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 151 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.48it/s, loss=1.5341, lr=0.000438]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4627\n","output_type":"stream"},{"name":"stderr","text":"Epoch 151 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.06it/s, loss=0.4383]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3995\nPos Sim: 0.8478 | Neg Sim: -0.0011 | Separation: 0.8489\nLearning Rate: 0.000438\nCheckpoint saved: checkpoints/contraeend_epoch_151.pth\nPatience: 6/15\n\n============================================================\nEpoch 152/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 152 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.54it/s, loss=1.4901, lr=0.000437]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4599\n","output_type":"stream"},{"name":"stderr","text":"Epoch 152 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.03it/s, loss=0.2780]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4742\nPos Sim: 0.8216 | Neg Sim: 0.0025 | Separation: 0.8191\nLearning Rate: 0.000437\nCheckpoint saved: checkpoints/contraeend_epoch_152.pth\nPatience: 7/15\n\n============================================================\nEpoch 153/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 153 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.50it/s, loss=1.4629, lr=0.000437]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4527\n","output_type":"stream"},{"name":"stderr","text":"Epoch 153 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.95it/s, loss=0.2254]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4293\nPos Sim: 0.8211 | Neg Sim: 0.0017 | Separation: 0.8194\nLearning Rate: 0.000437\nCheckpoint saved: checkpoints/contraeend_epoch_153.pth\nPatience: 8/15\n\n============================================================\nEpoch 154/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 154 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.49it/s, loss=1.4562, lr=0.000437]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4527\n","output_type":"stream"},{"name":"stderr","text":"Epoch 154 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.14it/s, loss=0.5637]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4649\nPos Sim: 0.8196 | Neg Sim: 0.0021 | Separation: 0.8175\nLearning Rate: 0.000437\nCheckpoint saved: checkpoints/contraeend_epoch_154.pth\nPatience: 9/15\n\n============================================================\nEpoch 155/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 155 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.50it/s, loss=1.4544, lr=0.000436]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4668\n","output_type":"stream"},{"name":"stderr","text":"Epoch 155 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.86it/s, loss=0.6974]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4342\nPos Sim: 0.8278 | Neg Sim: -0.0008 | Separation: 0.8286\nLearning Rate: 0.000436\nCheckpoint saved: checkpoints/contraeend_epoch_155.pth\nPatience: 10/15\n\n============================================================\nEpoch 156/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 156 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.55it/s, loss=1.4523, lr=0.000436]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4563\n","output_type":"stream"},{"name":"stderr","text":"Epoch 156 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.07it/s, loss=0.5806]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4391\nPos Sim: 0.8432 | Neg Sim: -0.0007 | Separation: 0.8438\nLearning Rate: 0.000436\nCheckpoint saved: checkpoints/contraeend_epoch_156.pth\nPatience: 11/15\n\n============================================================\nEpoch 157/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 157 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.55it/s, loss=1.4310, lr=0.000436]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4375\n","output_type":"stream"},{"name":"stderr","text":"Epoch 157 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.24it/s, loss=0.1511]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4427\nPos Sim: 0.8268 | Neg Sim: 0.0009 | Separation: 0.8260\nLearning Rate: 0.000436\nCheckpoint saved: checkpoints/contraeend_epoch_157.pth\nPatience: 12/15\n\n============================================================\nEpoch 158/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 158 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.56it/s, loss=1.4403, lr=0.000435]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4529\n","output_type":"stream"},{"name":"stderr","text":"Epoch 158 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.07it/s, loss=0.2231]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3920\nPos Sim: 0.8391 | Neg Sim: -0.0000 | Separation: 0.8391\nLearning Rate: 0.000435\nCheckpoint saved: checkpoints/contraeend_epoch_158.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.3920\n\n============================================================\nEpoch 159/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 159 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.51it/s, loss=1.4743, lr=0.000435]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4570\n","output_type":"stream"},{"name":"stderr","text":"Epoch 159 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.12it/s, loss=0.3858]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4015\nPos Sim: 0.8446 | Neg Sim: 0.0011 | Separation: 0.8435\nLearning Rate: 0.000435\nCheckpoint saved: checkpoints/contraeend_epoch_159.pth\nPatience: 1/15\n\n============================================================\nEpoch 160/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 160 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.46it/s, loss=1.4443, lr=0.000435]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4451\n","output_type":"stream"},{"name":"stderr","text":"Epoch 160 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.01it/s, loss=0.1990]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3951\nPos Sim: 0.8398 | Neg Sim: -0.0009 | Separation: 0.8406\nLearning Rate: 0.000435\nCheckpoint saved: checkpoints/contraeend_epoch_160.pth\nPatience: 2/15\n\n============================================================\nEpoch 161/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 161 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.45it/s, loss=1.4107, lr=0.000434]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4154\n","output_type":"stream"},{"name":"stderr","text":"Epoch 161 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.24it/s, loss=0.1151]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3822\nPos Sim: 0.8446 | Neg Sim: -0.0003 | Separation: 0.8449\nLearning Rate: 0.000434\nCheckpoint saved: checkpoints/contraeend_epoch_161.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.3822\n\n============================================================\nEpoch 162/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 162 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.30it/s, loss=1.3856, lr=0.000434]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4183\n","output_type":"stream"},{"name":"stderr","text":"Epoch 162 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.39it/s, loss=0.2410]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4217\nPos Sim: 0.8264 | Neg Sim: -0.0007 | Separation: 0.8272\nLearning Rate: 0.000434\nCheckpoint saved: checkpoints/contraeend_epoch_162.pth\nPatience: 1/15\n\n============================================================\nEpoch 163/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 163 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.29it/s, loss=1.4271, lr=0.000434]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4414\n","output_type":"stream"},{"name":"stderr","text":"Epoch 163 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.74it/s, loss=0.4136]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3749\nPos Sim: 0.8491 | Neg Sim: -0.0008 | Separation: 0.8499\nLearning Rate: 0.000434\nCheckpoint saved: checkpoints/contraeend_epoch_163.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.3749\n\n============================================================\nEpoch 164/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 164 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.43it/s, loss=1.4123, lr=0.000433]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4160\n","output_type":"stream"},{"name":"stderr","text":"Epoch 164 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.90it/s, loss=0.3314]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3854\nPos Sim: 0.8387 | Neg Sim: 0.0006 | Separation: 0.8380\nLearning Rate: 0.000433\nCheckpoint saved: checkpoints/contraeend_epoch_164.pth\nPatience: 1/15\n\n============================================================\nEpoch 165/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 165 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.44it/s, loss=1.4526, lr=0.000433]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4176\n","output_type":"stream"},{"name":"stderr","text":"Epoch 165 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.86it/s, loss=0.1493]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3732\nPos Sim: 0.8338 | Neg Sim: 0.0005 | Separation: 0.8333\nLearning Rate: 0.000433\nCheckpoint saved: checkpoints/contraeend_epoch_165.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.3732\n\n============================================================\nEpoch 166/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 166 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.39it/s, loss=1.4882, lr=0.000433]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4242\n","output_type":"stream"},{"name":"stderr","text":"Epoch 166 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.04it/s, loss=0.1057]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4351\nPos Sim: 0.8349 | Neg Sim: 0.0001 | Separation: 0.8348\nLearning Rate: 0.000433\nCheckpoint saved: checkpoints/contraeend_epoch_166.pth\nPatience: 1/15\n\n============================================================\nEpoch 167/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 167 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.47it/s, loss=1.3954, lr=0.000432]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4152\n","output_type":"stream"},{"name":"stderr","text":"Epoch 167 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.08it/s, loss=0.1620]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4025\nPos Sim: 0.8369 | Neg Sim: 0.0006 | Separation: 0.8363\nLearning Rate: 0.000432\nCheckpoint saved: checkpoints/contraeend_epoch_167.pth\nPatience: 2/15\n\n============================================================\nEpoch 168/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 168 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.53it/s, loss=1.4231, lr=0.000432]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4104\n","output_type":"stream"},{"name":"stderr","text":"Epoch 168 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.04it/s, loss=0.2198]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3463\nPos Sim: 0.8579 | Neg Sim: -0.0005 | Separation: 0.8583\nLearning Rate: 0.000432\nCheckpoint saved: checkpoints/contraeend_epoch_168.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.3463\n\n============================================================\nEpoch 169/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 169 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.43it/s, loss=1.4282, lr=0.000432]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4059\n","output_type":"stream"},{"name":"stderr","text":"Epoch 169 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.95it/s, loss=0.2672]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3599\nPos Sim: 0.8318 | Neg Sim: -0.0005 | Separation: 0.8324\nLearning Rate: 0.000432\nCheckpoint saved: checkpoints/contraeend_epoch_169.pth\nPatience: 1/15\n\n============================================================\nEpoch 170/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 170 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.41it/s, loss=1.3776, lr=0.000431]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3993\n","output_type":"stream"},{"name":"stderr","text":"Epoch 170 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.05it/s, loss=0.2427]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3953\nPos Sim: 0.8492 | Neg Sim: 0.0009 | Separation: 0.8483\nLearning Rate: 0.000431\nCheckpoint saved: checkpoints/contraeend_epoch_170.pth\nPatience: 2/15\n\n============================================================\nEpoch 171/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 171 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.42it/s, loss=1.4774, lr=0.000431]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.4097\n","output_type":"stream"},{"name":"stderr","text":"Epoch 171 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.94it/s, loss=0.0767]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3964\nPos Sim: 0.8287 | Neg Sim: 0.0036 | Separation: 0.8251\nLearning Rate: 0.000431\nCheckpoint saved: checkpoints/contraeend_epoch_171.pth\nPatience: 3/15\n\n============================================================\nEpoch 172/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 172 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.55it/s, loss=1.3398, lr=0.000431]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3932\n","output_type":"stream"},{"name":"stderr","text":"Epoch 172 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.08it/s, loss=0.1060]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3926\nPos Sim: 0.8424 | Neg Sim: 0.0002 | Separation: 0.8422\nLearning Rate: 0.000431\nCheckpoint saved: checkpoints/contraeend_epoch_172.pth\nPatience: 4/15\n\n============================================================\nEpoch 173/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 173 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.56it/s, loss=1.4113, lr=0.000430]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3954\n","output_type":"stream"},{"name":"stderr","text":"Epoch 173 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.06it/s, loss=0.1288]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3933\nPos Sim: 0.8503 | Neg Sim: -0.0011 | Separation: 0.8514\nLearning Rate: 0.000430\nCheckpoint saved: checkpoints/contraeend_epoch_173.pth\nPatience: 5/15\n\n============================================================\nEpoch 174/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 174 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.48it/s, loss=1.3727, lr=0.000430]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3833\n","output_type":"stream"},{"name":"stderr","text":"Epoch 174 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.21it/s, loss=0.2080]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3471\nPos Sim: 0.8347 | Neg Sim: -0.0003 | Separation: 0.8350\nLearning Rate: 0.000430\nCheckpoint saved: checkpoints/contraeend_epoch_174.pth\nPatience: 6/15\n\n============================================================\nEpoch 175/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 175 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.51it/s, loss=1.4065, lr=0.000430]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3862\n","output_type":"stream"},{"name":"stderr","text":"Epoch 175 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.13it/s, loss=0.4552]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3591\nPos Sim: 0.8533 | Neg Sim: -0.0001 | Separation: 0.8534\nLearning Rate: 0.000430\nCheckpoint saved: checkpoints/contraeend_epoch_175.pth\nPatience: 7/15\n\n============================================================\nEpoch 176/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 176 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.54it/s, loss=1.3578, lr=0.000429]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3997\n","output_type":"stream"},{"name":"stderr","text":"Epoch 176 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.09it/s, loss=0.5582]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3490\nPos Sim: 0.8490 | Neg Sim: -0.0009 | Separation: 0.8499\nLearning Rate: 0.000429\nCheckpoint saved: checkpoints/contraeend_epoch_176.pth\nPatience: 8/15\n\n============================================================\nEpoch 177/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 177 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.51it/s, loss=1.3686, lr=0.000429]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3734\n","output_type":"stream"},{"name":"stderr","text":"Epoch 177 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.09it/s, loss=0.4290]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4249\nPos Sim: 0.8303 | Neg Sim: 0.0002 | Separation: 0.8301\nLearning Rate: 0.000429\nCheckpoint saved: checkpoints/contraeend_epoch_177.pth\nPatience: 9/15\n\n============================================================\nEpoch 178/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 178 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.50it/s, loss=1.4045, lr=0.000429]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3768\n","output_type":"stream"},{"name":"stderr","text":"Epoch 178 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.10it/s, loss=0.1115]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3048\nPos Sim: 0.8601 | Neg Sim: -0.0006 | Separation: 0.8607\nLearning Rate: 0.000429\nCheckpoint saved: checkpoints/contraeend_epoch_178.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.3048\n\n============================================================\nEpoch 179/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 179 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.46it/s, loss=1.3866, lr=0.000428]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3698\n","output_type":"stream"},{"name":"stderr","text":"Epoch 179 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.84it/s, loss=0.2852]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3284\nPos Sim: 0.8590 | Neg Sim: -0.0019 | Separation: 0.8609\nLearning Rate: 0.000428\nCheckpoint saved: checkpoints/contraeend_epoch_179.pth\nPatience: 1/15\n\n============================================================\nEpoch 180/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 180 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.47it/s, loss=1.3509, lr=0.000428]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3699\n","output_type":"stream"},{"name":"stderr","text":"Epoch 180 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.08it/s, loss=0.2063]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4062\nPos Sim: 0.8402 | Neg Sim: -0.0000 | Separation: 0.8402\nLearning Rate: 0.000428\nCheckpoint saved: checkpoints/contraeend_epoch_180.pth\nPatience: 2/15\n\n============================================================\nEpoch 181/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 181 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.58it/s, loss=1.3562, lr=0.000428]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3786\n","output_type":"stream"},{"name":"stderr","text":"Epoch 181 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.07it/s, loss=0.0816]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3802\nPos Sim: 0.8373 | Neg Sim: 0.0022 | Separation: 0.8351\nLearning Rate: 0.000428\nCheckpoint saved: checkpoints/contraeend_epoch_181.pth\nPatience: 3/15\n\n============================================================\nEpoch 182/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 182 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.49it/s, loss=1.3618, lr=0.000427]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3561\n","output_type":"stream"},{"name":"stderr","text":"Epoch 182 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.24it/s, loss=0.1379]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3665\nPos Sim: 0.8275 | Neg Sim: -0.0004 | Separation: 0.8279\nLearning Rate: 0.000427\nCheckpoint saved: checkpoints/contraeend_epoch_182.pth\nPatience: 4/15\n\n============================================================\nEpoch 183/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 183 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.55it/s, loss=1.3669, lr=0.000427]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3559\n","output_type":"stream"},{"name":"stderr","text":"Epoch 183 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.15it/s, loss=0.2408]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.2953\nPos Sim: 0.8584 | Neg Sim: -0.0013 | Separation: 0.8598\nLearning Rate: 0.000427\nCheckpoint saved: checkpoints/contraeend_epoch_183.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.2953\n\n============================================================\nEpoch 184/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 184 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.48it/s, loss=1.3742, lr=0.000427]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3516\n","output_type":"stream"},{"name":"stderr","text":"Epoch 184 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.19it/s, loss=0.6997]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3703\nPos Sim: 0.8512 | Neg Sim: -0.0006 | Separation: 0.8518\nLearning Rate: 0.000427\nCheckpoint saved: checkpoints/contraeend_epoch_184.pth\nPatience: 1/15\n\n============================================================\nEpoch 185/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 185 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.51it/s, loss=1.3392, lr=0.000426]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3537\n","output_type":"stream"},{"name":"stderr","text":"Epoch 185 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.01it/s, loss=0.5061]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3936\nPos Sim: 0.8370 | Neg Sim: -0.0009 | Separation: 0.8379\nLearning Rate: 0.000426\nCheckpoint saved: checkpoints/contraeend_epoch_185.pth\nPatience: 2/15\n\n============================================================\nEpoch 186/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 186 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.51it/s, loss=1.3502, lr=0.000426]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3474\n","output_type":"stream"},{"name":"stderr","text":"Epoch 186 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.20it/s, loss=0.1371]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3119\nPos Sim: 0.8433 | Neg Sim: -0.0006 | Separation: 0.8439\nLearning Rate: 0.000426\nCheckpoint saved: checkpoints/contraeend_epoch_186.pth\nPatience: 3/15\n\n============================================================\nEpoch 187/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 187 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.54it/s, loss=1.4014, lr=0.000425]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3368\n","output_type":"stream"},{"name":"stderr","text":"Epoch 187 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.14it/s, loss=0.0940]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3310\nPos Sim: 0.8419 | Neg Sim: -0.0004 | Separation: 0.8424\nLearning Rate: 0.000425\nCheckpoint saved: checkpoints/contraeend_epoch_187.pth\nPatience: 4/15\n\n============================================================\nEpoch 188/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 188 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.53it/s, loss=1.3763, lr=0.000425]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3385\n","output_type":"stream"},{"name":"stderr","text":"Epoch 188 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.06it/s, loss=0.5713]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3504\nPos Sim: 0.8512 | Neg Sim: -0.0003 | Separation: 0.8515\nLearning Rate: 0.000425\nCheckpoint saved: checkpoints/contraeend_epoch_188.pth\nPatience: 5/15\n\n============================================================\nEpoch 189/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 189 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.59it/s, loss=1.3090, lr=0.000425]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3272\n","output_type":"stream"},{"name":"stderr","text":"Epoch 189 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.18it/s, loss=0.1840]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3475\nPos Sim: 0.8590 | Neg Sim: -0.0001 | Separation: 0.8590\nLearning Rate: 0.000425\nCheckpoint saved: checkpoints/contraeend_epoch_189.pth\nPatience: 6/15\n\n============================================================\nEpoch 190/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 190 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.57it/s, loss=1.4078, lr=0.000424]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3363\n","output_type":"stream"},{"name":"stderr","text":"Epoch 190 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.09it/s, loss=0.1210]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3371\nPos Sim: 0.8507 | Neg Sim: -0.0011 | Separation: 0.8517\nLearning Rate: 0.000424\nCheckpoint saved: checkpoints/contraeend_epoch_190.pth\nPatience: 7/15\n\n============================================================\nEpoch 191/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 191 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.62it/s, loss=1.3106, lr=0.000424]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3565\n","output_type":"stream"},{"name":"stderr","text":"Epoch 191 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.18it/s, loss=0.1702]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3504\nPos Sim: 0.8451 | Neg Sim: 0.0001 | Separation: 0.8451\nLearning Rate: 0.000424\nCheckpoint saved: checkpoints/contraeend_epoch_191.pth\nPatience: 8/15\n\n============================================================\nEpoch 192/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 192 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.40it/s, loss=1.2824, lr=0.000424]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3346\n","output_type":"stream"},{"name":"stderr","text":"Epoch 192 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.10it/s, loss=0.2650]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3031\nPos Sim: 0.8511 | Neg Sim: -0.0005 | Separation: 0.8516\nLearning Rate: 0.000424\nCheckpoint saved: checkpoints/contraeend_epoch_192.pth\nPatience: 9/15\n\n============================================================\nEpoch 193/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 193 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.54it/s, loss=1.3726, lr=0.000423]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3194\n","output_type":"stream"},{"name":"stderr","text":"Epoch 193 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.02it/s, loss=0.2372]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3311\nPos Sim: 0.8364 | Neg Sim: -0.0008 | Separation: 0.8372\nLearning Rate: 0.000423\nCheckpoint saved: checkpoints/contraeend_epoch_193.pth\nPatience: 10/15\n\n============================================================\nEpoch 194/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 194 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.51it/s, loss=1.3523, lr=0.000423]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3229\n","output_type":"stream"},{"name":"stderr","text":"Epoch 194 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.00it/s, loss=0.5397]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.4075\nPos Sim: 0.8553 | Neg Sim: -0.0002 | Separation: 0.8555\nLearning Rate: 0.000423\nCheckpoint saved: checkpoints/contraeend_epoch_194.pth\nPatience: 11/15\n\n============================================================\nEpoch 195/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 195 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.50it/s, loss=1.3337, lr=0.000423]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3112\n","output_type":"stream"},{"name":"stderr","text":"Epoch 195 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.77it/s, loss=0.1800]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.2771\nPos Sim: 0.8642 | Neg Sim: -0.0019 | Separation: 0.8661\nLearning Rate: 0.000423\nCheckpoint saved: checkpoints/contraeend_epoch_195.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.2771\n\n============================================================\nEpoch 196/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 196 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:20<00:00,  4.45it/s, loss=1.3333, lr=0.000422]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3196\n","output_type":"stream"},{"name":"stderr","text":"Epoch 196 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  4.82it/s, loss=0.4547]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3035\nPos Sim: 0.8599 | Neg Sim: -0.0013 | Separation: 0.8612\nLearning Rate: 0.000422\nCheckpoint saved: checkpoints/contraeend_epoch_196.pth\nPatience: 1/15\n\n============================================================\nEpoch 197/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 197 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.63it/s, loss=1.3576, lr=0.000422]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3251\n","output_type":"stream"},{"name":"stderr","text":"Epoch 197 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.16it/s, loss=0.3749]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3823\nPos Sim: 0.8359 | Neg Sim: 0.0007 | Separation: 0.8353\nLearning Rate: 0.000422\nCheckpoint saved: checkpoints/contraeend_epoch_197.pth\nPatience: 2/15\n\n============================================================\nEpoch 198/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 198 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.59it/s, loss=1.3209, lr=0.000422]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3189\n","output_type":"stream"},{"name":"stderr","text":"Epoch 198 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.17it/s, loss=0.2518]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.2632\nPos Sim: 0.8615 | Neg Sim: -0.0012 | Separation: 0.8626\nLearning Rate: 0.000422\nCheckpoint saved: checkpoints/contraeend_epoch_198.pth\nCheckpoint saved: checkpoints/contraeend_best.pth\n‚úì New best model saved! Val Loss: 1.2632\n\n============================================================\nEpoch 199/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 199 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.57it/s, loss=1.2914, lr=0.000421]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3109\n","output_type":"stream"},{"name":"stderr","text":"Epoch 199 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.22it/s, loss=0.4781]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.3712\nPos Sim: 0.8596 | Neg Sim: -0.0005 | Separation: 0.8600\nLearning Rate: 0.000421\nCheckpoint saved: checkpoints/contraeend_epoch_199.pth\nPatience: 1/15\n\n============================================================\nEpoch 200/200\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 200 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 90/90 [00:19<00:00,  4.58it/s, loss=1.3062, lr=0.000421]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 1.3203\n","output_type":"stream"},{"name":"stderr","text":"Epoch 200 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:02<00:00,  5.18it/s, loss=0.4280]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 1.2726\nPos Sim: 0.8543 | Neg Sim: -0.0016 | Separation: 0.8559\nLearning Rate: 0.000421\nCheckpoint saved: checkpoints/contraeend_epoch_200.pth\nPatience: 2/15\n\nTraining completed! Best validation loss: 1.2632\n\n============================================================\n== Training Complete! ==\nBest Validation Loss: 1.2632\n============================================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!ls /kaggle/working/checkpoints","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T04:17:21.353243Z","iopub.execute_input":"2025-10-24T04:17:21.353570Z","iopub.status.idle":"2025-10-24T04:17:21.597105Z","shell.execute_reply.started":"2025-10-24T04:17:21.353541Z","shell.execute_reply":"2025-10-24T04:17:21.596381Z"}},"outputs":[{"name":"stdout","text":"contraeend_best.pth\t  contraeend_epoch_139.pth  contraeend_epoch_179.pth\ncontraeend_epoch_100.pth  contraeend_epoch_140.pth  contraeend_epoch_180.pth\ncontraeend_epoch_101.pth  contraeend_epoch_141.pth  contraeend_epoch_181.pth\ncontraeend_epoch_102.pth  contraeend_epoch_142.pth  contraeend_epoch_182.pth\ncontraeend_epoch_103.pth  contraeend_epoch_143.pth  contraeend_epoch_183.pth\ncontraeend_epoch_104.pth  contraeend_epoch_144.pth  contraeend_epoch_184.pth\ncontraeend_epoch_105.pth  contraeend_epoch_145.pth  contraeend_epoch_185.pth\ncontraeend_epoch_106.pth  contraeend_epoch_146.pth  contraeend_epoch_186.pth\ncontraeend_epoch_107.pth  contraeend_epoch_147.pth  contraeend_epoch_187.pth\ncontraeend_epoch_108.pth  contraeend_epoch_148.pth  contraeend_epoch_188.pth\ncontraeend_epoch_109.pth  contraeend_epoch_149.pth  contraeend_epoch_189.pth\ncontraeend_epoch_110.pth  contraeend_epoch_150.pth  contraeend_epoch_190.pth\ncontraeend_epoch_111.pth  contraeend_epoch_151.pth  contraeend_epoch_191.pth\ncontraeend_epoch_112.pth  contraeend_epoch_152.pth  contraeend_epoch_192.pth\ncontraeend_epoch_113.pth  contraeend_epoch_153.pth  contraeend_epoch_193.pth\ncontraeend_epoch_114.pth  contraeend_epoch_154.pth  contraeend_epoch_194.pth\ncontraeend_epoch_115.pth  contraeend_epoch_155.pth  contraeend_epoch_195.pth\ncontraeend_epoch_116.pth  contraeend_epoch_156.pth  contraeend_epoch_196.pth\ncontraeend_epoch_117.pth  contraeend_epoch_157.pth  contraeend_epoch_197.pth\ncontraeend_epoch_118.pth  contraeend_epoch_158.pth  contraeend_epoch_198.pth\ncontraeend_epoch_119.pth  contraeend_epoch_159.pth  contraeend_epoch_199.pth\ncontraeend_epoch_120.pth  contraeend_epoch_160.pth  contraeend_epoch_200.pth\ncontraeend_epoch_121.pth  contraeend_epoch_161.pth  contraeend_epoch_84.pth\ncontraeend_epoch_122.pth  contraeend_epoch_162.pth  contraeend_epoch_85.pth\ncontraeend_epoch_123.pth  contraeend_epoch_163.pth  contraeend_epoch_86.pth\ncontraeend_epoch_124.pth  contraeend_epoch_164.pth  contraeend_epoch_87.pth\ncontraeend_epoch_125.pth  contraeend_epoch_165.pth  contraeend_epoch_88.pth\ncontraeend_epoch_126.pth  contraeend_epoch_166.pth  contraeend_epoch_89.pth\ncontraeend_epoch_127.pth  contraeend_epoch_167.pth  contraeend_epoch_90.pth\ncontraeend_epoch_128.pth  contraeend_epoch_168.pth  contraeend_epoch_91.pth\ncontraeend_epoch_129.pth  contraeend_epoch_169.pth  contraeend_epoch_92.pth\ncontraeend_epoch_130.pth  contraeend_epoch_170.pth  contraeend_epoch_93.pth\ncontraeend_epoch_131.pth  contraeend_epoch_171.pth  contraeend_epoch_94.pth\ncontraeend_epoch_132.pth  contraeend_epoch_172.pth  contraeend_epoch_95.pth\ncontraeend_epoch_133.pth  contraeend_epoch_173.pth  contraeend_epoch_96.pth\ncontraeend_epoch_134.pth  contraeend_epoch_174.pth  contraeend_epoch_97.pth\ncontraeend_epoch_135.pth  contraeend_epoch_175.pth  contraeend_epoch_98.pth\ncontraeend_epoch_136.pth  contraeend_epoch_176.pth  contraeend_epoch_99.pth\ncontraeend_epoch_137.pth  contraeend_epoch_177.pth\ncontraeend_epoch_138.pth  contraeend_epoch_178.pth\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!cp /kaggle/working/checkpoints/contraeend_best.pth /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T04:17:21.598431Z","iopub.execute_input":"2025-10-24T04:17:21.598696Z","iopub.status.idle":"2025-10-24T04:17:21.869254Z","shell.execute_reply.started":"2025-10-24T04:17:21.598670Z","shell.execute_reply":"2025-10-24T04:17:21.868210Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# !zip -r checkpoint.zip /kaggle/working/checkpoints/contraeend_best.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T04:17:21.870505Z","iopub.execute_input":"2025-10-24T04:17:21.870773Z","iopub.status.idle":"2025-10-24T04:17:21.874746Z","shell.execute_reply.started":"2025-10-24T04:17:21.870750Z","shell.execute_reply":"2025-10-24T04:17:21.874041Z"}},"outputs":[],"execution_count":13}]}