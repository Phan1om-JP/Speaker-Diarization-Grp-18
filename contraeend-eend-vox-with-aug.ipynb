{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-28T11:28:06.156164Z",
     "iopub.status.busy": "2025-10-28T11:28:06.155376Z",
     "iopub.status.idle": "2025-10-28T12:52:09.394075Z",
     "shell.execute_reply": "2025-10-28T12:52:09.393232Z",
     "shell.execute_reply.started": "2025-10-28T11:28:06.156128Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CUDA is available!\n",
      "ðŸš€ Using GPU: Tesla P100-PCIE-16GB\n",
      "ðŸ’¾ GPU Memory: 17.1 GB\n",
      "============================================================\n",
      "Simplified Phase 3: Proven Techniques Only\n",
      "============================================================\n",
      "Improvements over Phase 2:\n",
      "  âœ“ Attention pooling for speaker embeddings\n",
      "  âœ“ Overlap-weighted PIT loss\n",
      "  âœ“ Boundary smoothing (inference)\n",
      "============================================================\n",
      "Device: cuda\n",
      "Batch Size: 8\n",
      "Chunk Size: 20.0s\n",
      "============================================================\n",
      "\n",
      "Loading dataset...\n",
      "\n",
      "Found 216 audio files\n",
      "Found 216 RTTM files\n",
      "Matched 216 audio-RTTM pairs\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating chunks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 216/216 [00:08<00:00, 24.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 4277 chunks from 216 conversations\n",
      "  Chunk size: 20.0s, Overlap: 3.0s, Stride: 17.0s\n",
      "Train chunks: 3849\n",
      "Val chunks: 428\n",
      "\n",
      "Initializing Simplified Phase 3 model...\n",
      "\n",
      "ðŸ“¦ Loading Phase 2 checkpoint from /kaggle/input/2/pytorch/default/1/contraeend_phase2_best.pth\n",
      "âš ï¸  Skipping decoder.output_proj.3.weight\n",
      "âš ï¸  Skipping decoder.output_proj.3.bias\n",
      "âœ“ Phase 2 weights loaded (overlap_detector randomly initialized)\n",
      "Total parameters: 3,233,608\n",
      "\n",
      "Initializing trainer...\n",
      "âœ“ CSV log created: logs/training_log_20251028_112818.csv\n",
      "\n",
      "Starting training...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Training Configuration:\n",
      "  Contrastive Weight: 0.05\n",
      "  CSV Log: logs/training_log_20251028_112818.csv\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Epoch 1/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [03:00<00:00,  2.67it/s, pit=0.1180, ovlp=0.2227, cont=5.6054, total=0.4521]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1291, Contrast: 5.8374, Total: 1.0166\n",
      "\n",
      "ðŸŽ¯ Calibrating adaptive thresholds...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Speaker 0: threshold=0.360, F1=0.426\n",
      "  Speaker 1: threshold=0.200, F1=0.372\n",
      "  Speaker 2: threshold=0.220, F1=0.294\n",
      "  Speaker 3: threshold=0.280, F1=0.212\n",
      "  Speaker 4: threshold=0.260, F1=0.150\n",
      "  Speaker 5: threshold=0.200, F1=0.053\n",
      "  Silence threshold: 0.390, F1=0.393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:49<00:00,  1.10it/s, loss=0.3789]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1177, Contrast: 5.8634, Total: 0.4108\n",
      "DER: 15.00%\n",
      "JER: 0.8595\n",
      "Overlap - P: 0.514, R: 0.261, F1: 0.346\n",
      "FA Rate: 0.630, Miss Rate: 0.083\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.017s, RTF: 0.011\n",
      "Model Size - Total: 3.23M params, Checkpoint: 42.0 MB\n",
      "Learning Rate: 0.000020\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_1.pth\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_best_der.pth\n",
      "âœ“ New best DER! 15.00%\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_best_loss.pth\n",
      "âœ“ New best loss! 0.4108\n",
      "\n",
      "============================================================\n",
      "Epoch 2/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:57<00:00,  2.71it/s, pit=0.1205, ovlp=0.1359, cont=6.1479, total=0.3243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1174, Contrast: 5.8386, Total: 0.7360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:49<00:00,  1.09it/s, loss=0.3647]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1176, Contrast: 5.8648, Total: 0.4109\n",
      "DER: 15.25%\n",
      "JER: 0.8839\n",
      "Overlap - P: 0.418, R: 0.553, F1: 0.477\n",
      "FA Rate: 0.713, Miss Rate: 0.042\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.016s, RTF: 0.011\n",
      "Learning Rate: 0.000020\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_2.pth\n",
      "Patience: 1/10\n",
      "\n",
      "============================================================\n",
      "Epoch 3/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:57<00:00,  2.70it/s, pit=0.1152, ovlp=0.1907, cont=5.4319, total=0.4012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1144, Contrast: 5.8502, Total: 0.6844\n",
      "\n",
      "ðŸŽ¯ Calibrating adaptive thresholds...\n",
      "  Speaker 0: threshold=0.280, F1=0.425\n",
      "  Speaker 1: threshold=0.200, F1=0.373\n",
      "  Speaker 2: threshold=0.300, F1=0.299\n",
      "  Speaker 3: threshold=0.240, F1=0.212\n",
      "  Speaker 4: threshold=0.220, F1=0.139\n",
      "  Speaker 5: threshold=0.200, F1=0.064\n",
      "  Silence threshold: 0.390, F1=0.388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:48<00:00,  1.12it/s, loss=0.3626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1145, Contrast: 5.8648, Total: 0.4077\n",
      "DER: 15.34%\n",
      "JER: 0.8834\n",
      "Overlap - P: 0.387, R: 0.648, F1: 0.484\n",
      "FA Rate: 0.668, Miss Rate: 0.061\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.015s, RTF: 0.011\n",
      "Learning Rate: 0.000020\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_3.pth\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_best_loss.pth\n",
      "âœ“ New best loss! 0.4077\n",
      "\n",
      "============================================================\n",
      "Epoch 4/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:57<00:00,  2.71it/s, pit=0.1148, ovlp=0.2473, cont=5.5131, total=0.4857]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1130, Contrast: 5.8490, Total: 0.6542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:48<00:00,  1.12it/s, loss=0.3584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1129, Contrast: 5.8644, Total: 0.4061\n",
      "DER: 15.39%\n",
      "JER: 0.8807\n",
      "Overlap - P: 0.466, R: 0.645, F1: 0.541\n",
      "FA Rate: 0.642, Miss Rate: 0.079\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.015s, RTF: 0.011\n",
      "Learning Rate: 0.000019\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_4.pth\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_best_loss.pth\n",
      "âœ“ New best loss! 0.4061\n",
      "\n",
      "============================================================\n",
      "Epoch 5/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:57<00:00,  2.71it/s, pit=0.1058, ovlp=0.1524, cont=6.1100, total=0.3344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1106, Contrast: 5.8489, Total: 0.6315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:47<00:00,  1.14it/s, loss=0.3549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1126, Contrast: 5.8645, Total: 0.4058\n",
      "DER: 15.84%\n",
      "JER: 0.8734\n",
      "Overlap - P: 0.381, R: 0.722, F1: 0.499\n",
      "FA Rate: 0.575, Miss Rate: 0.078\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.019s, RTF: 0.011\n",
      "Learning Rate: 0.000019\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_5.pth\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_best_loss.pth\n",
      "âœ“ New best loss! 0.4058\n",
      "\n",
      "============================================================\n",
      "Epoch 6/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:58<00:00,  2.70it/s, pit=0.1080, ovlp=0.1451, cont=5.7776, total=0.3258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1111, Contrast: 5.8542, Total: 0.6196\n",
      "\n",
      "ðŸŽ¯ Calibrating adaptive thresholds...\n",
      "  Speaker 0: threshold=0.200, F1=0.426\n",
      "  Speaker 1: threshold=0.200, F1=0.376\n",
      "  Speaker 2: threshold=0.300, F1=0.300\n",
      "  Speaker 3: threshold=0.200, F1=0.216\n",
      "  Speaker 4: threshold=0.260, F1=0.146\n",
      "  Speaker 5: threshold=0.200, F1=0.074\n",
      "  Silence threshold: 0.360, F1=0.454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:49<00:00,  1.09it/s, loss=0.3575]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1127, Contrast: 5.8636, Total: 0.4059\n",
      "DER: 15.85%\n",
      "JER: 0.8747\n",
      "Overlap - P: 0.543, R: 0.632, F1: 0.584\n",
      "FA Rate: 0.609, Miss Rate: 0.054\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.015s, RTF: 0.011\n",
      "Learning Rate: 0.000018\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_6.pth\n",
      "Patience: 1/10\n",
      "\n",
      "============================================================\n",
      "Epoch 7/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:59<00:00,  2.69it/s, pit=0.1090, ovlp=0.1057, cont=5.6345, total=0.2675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1105, Contrast: 5.8401, Total: 0.6052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:49<00:00,  1.09it/s, loss=0.3556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1121, Contrast: 5.8638, Total: 0.4053\n",
      "DER: 16.06%\n",
      "JER: 0.8744\n",
      "Overlap - P: 0.527, R: 0.639, F1: 0.577\n",
      "FA Rate: 0.595, Miss Rate: 0.047\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.015s, RTF: 0.011\n",
      "Learning Rate: 0.000018\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_7.pth\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_best_loss.pth\n",
      "âœ“ New best loss! 0.4053\n",
      "\n",
      "============================================================\n",
      "Epoch 8/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:57<00:00,  2.72it/s, pit=0.0931, ovlp=0.0616, cont=6.2140, total=0.1855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1094, Contrast: 5.8506, Total: 0.5961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:48<00:00,  1.12it/s, loss=0.3626]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1130, Contrast: 5.8632, Total: 0.4061\n",
      "DER: 16.55%\n",
      "JER: 0.8643\n",
      "Overlap - P: 0.476, R: 0.701, F1: 0.567\n",
      "FA Rate: 0.593, Miss Rate: 0.057\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.015s, RTF: 0.011\n",
      "Learning Rate: 0.000017\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_8.pth\n",
      "Patience: 1/10\n",
      "\n",
      "============================================================\n",
      "Epoch 9/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:57<00:00,  2.72it/s, pit=0.1207, ovlp=0.2470, cont=5.3852, total=0.4912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1082, Contrast: 5.8478, Total: 0.5841\n",
      "\n",
      "ðŸŽ¯ Calibrating adaptive thresholds...\n",
      "  Speaker 0: threshold=0.200, F1=0.397\n",
      "  Speaker 1: threshold=0.200, F1=0.372\n",
      "  Speaker 2: threshold=0.280, F1=0.301\n",
      "  Speaker 3: threshold=0.200, F1=0.215\n",
      "  Speaker 4: threshold=0.280, F1=0.161\n",
      "  Speaker 5: threshold=0.200, F1=0.115\n",
      "  Silence threshold: 0.370, F1=0.456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:48<00:00,  1.12it/s, loss=0.3559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1126, Contrast: 5.8644, Total: 0.4058\n",
      "DER: 16.40%\n",
      "JER: 0.8855\n",
      "Overlap - P: 0.508, R: 0.697, F1: 0.588\n",
      "FA Rate: 0.577, Miss Rate: 0.071\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.019s, RTF: 0.011\n",
      "Learning Rate: 0.000016\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_9.pth\n",
      "Patience: 2/10\n",
      "\n",
      "============================================================\n",
      "Epoch 10/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:56<00:00,  2.72it/s, pit=0.0906, ovlp=0.1070, cont=5.1261, total=0.2511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1084, Contrast: 5.8628, Total: 0.5797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:46<00:00,  1.16it/s, loss=0.3613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1123, Contrast: 5.8649, Total: 0.4055\n",
      "DER: 16.32%\n",
      "JER: 0.8750\n",
      "Overlap - P: 0.490, R: 0.712, F1: 0.580\n",
      "FA Rate: 0.514, Miss Rate: 0.096\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.015s, RTF: 0.011\n",
      "Learning Rate: 0.000015\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_10.pth\n",
      "Patience: 3/10\n",
      "\n",
      "============================================================\n",
      "Epoch 11/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:58<00:00,  2.70it/s, pit=0.1081, ovlp=0.0650, cont=6.2207, total=0.2056]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1071, Contrast: 5.8450, Total: 0.5704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:48<00:00,  1.12it/s, loss=0.3561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1112, Contrast: 5.8657, Total: 0.4045\n",
      "DER: 16.40%\n",
      "JER: 0.8875\n",
      "Overlap - P: 0.545, R: 0.669, F1: 0.601\n",
      "FA Rate: 0.550, Miss Rate: 0.069\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.015s, RTF: 0.011\n",
      "Learning Rate: 0.000014\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_11.pth\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_best_loss.pth\n",
      "âœ“ New best loss! 0.4045\n",
      "\n",
      "============================================================\n",
      "Epoch 12/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:56<00:00,  2.72it/s, pit=0.0961, ovlp=0.1092, cont=5.6412, total=0.2599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1075, Contrast: 5.8482, Total: 0.5682\n",
      "\n",
      "ðŸŽ¯ Calibrating adaptive thresholds...\n",
      "  Speaker 0: threshold=0.200, F1=0.404\n",
      "  Speaker 1: threshold=0.200, F1=0.363\n",
      "  Speaker 2: threshold=0.320, F1=0.304\n",
      "  Speaker 3: threshold=0.200, F1=0.215\n",
      "  Speaker 4: threshold=0.280, F1=0.157\n",
      "  Speaker 5: threshold=0.200, F1=0.124\n",
      "  Silence threshold: 0.360, F1=0.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:48<00:00,  1.12it/s, loss=0.3639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1127, Contrast: 5.8647, Total: 0.4059\n",
      "DER: 16.60%\n",
      "JER: 0.8723\n",
      "Overlap - P: 0.447, R: 0.749, F1: 0.560\n",
      "FA Rate: 0.582, Miss Rate: 0.053\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.027s, RTF: 0.011\n",
      "Learning Rate: 0.000013\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_12.pth\n",
      "Patience: 1/10\n",
      "\n",
      "============================================================\n",
      "Epoch 13/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:56<00:00,  2.73it/s, pit=0.0783, ovlp=0.0606, cont=5.5148, total=0.1692]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1073, Contrast: 5.8486, Total: 0.5603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:48<00:00,  1.12it/s, loss=0.3643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1140, Contrast: 5.8651, Total: 0.4073\n",
      "DER: 16.52%\n",
      "JER: 0.8758\n",
      "Overlap - P: 0.526, R: 0.674, F1: 0.591\n",
      "FA Rate: 0.549, Miss Rate: 0.069\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.180s, Model: 0.021s, RTF: 0.011\n",
      "Learning Rate: 0.000012\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_13.pth\n",
      "Patience: 2/10\n",
      "\n",
      "============================================================\n",
      "Epoch 14/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:56<00:00,  2.72it/s, pit=0.0852, ovlp=0.0365, cont=5.8679, total=0.1400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1070, Contrast: 5.8347, Total: 0.5558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:47<00:00,  1.13it/s, loss=0.3646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1122, Contrast: 5.8655, Total: 0.4055\n",
      "DER: 16.49%\n",
      "JER: 0.8707\n",
      "Overlap - P: 0.559, R: 0.660, F1: 0.605\n",
      "FA Rate: 0.533, Miss Rate: 0.073\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.016s, RTF: 0.011\n",
      "Learning Rate: 0.000011\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_14.pth\n",
      "Patience: 3/10\n",
      "\n",
      "============================================================\n",
      "Epoch 15/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:56<00:00,  2.72it/s, pit=0.1003, ovlp=0.0877, cont=5.9588, total=0.2318]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1064, Contrast: 5.8448, Total: 0.5539\n",
      "\n",
      "ðŸŽ¯ Calibrating adaptive thresholds...\n",
      "  Speaker 0: threshold=0.200, F1=0.399\n",
      "  Speaker 1: threshold=0.240, F1=0.366\n",
      "  Speaker 2: threshold=0.340, F1=0.304\n",
      "  Speaker 3: threshold=0.240, F1=0.226\n",
      "  Speaker 4: threshold=0.260, F1=0.174\n",
      "  Speaker 5: threshold=0.200, F1=0.138\n",
      "  Silence threshold: 0.350, F1=0.487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:46<00:00,  1.16it/s, loss=0.3607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1127, Contrast: 5.8641, Total: 0.4059\n",
      "DER: 16.51%\n",
      "JER: 0.8738\n",
      "Overlap - P: 0.578, R: 0.625, F1: 0.601\n",
      "FA Rate: 0.554, Miss Rate: 0.063\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.015s, RTF: 0.011\n",
      "Learning Rate: 0.000011\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_15.pth\n",
      "Patience: 4/10\n",
      "\n",
      "============================================================\n",
      "Epoch 16/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:57<00:00,  2.71it/s, pit=0.1110, ovlp=0.1021, cont=6.2165, total=0.2641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1057, Contrast: 5.8402, Total: 0.5491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:47<00:00,  1.14it/s, loss=0.3720]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1114, Contrast: 5.8655, Total: 0.4047\n",
      "DER: 16.15%\n",
      "JER: 0.8561\n",
      "Overlap - P: 0.561, R: 0.678, F1: 0.614\n",
      "FA Rate: 0.540, Miss Rate: 0.073\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.015s, RTF: 0.011\n",
      "Learning Rate: 0.000010\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_16.pth\n",
      "Patience: 5/10\n",
      "\n",
      "============================================================\n",
      "Epoch 17/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:56<00:00,  2.72it/s, pit=0.0938, ovlp=0.0666, cont=6.2177, total=0.1938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1058, Contrast: 5.8556, Total: 0.5502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:46<00:00,  1.16it/s, loss=0.3681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1126, Contrast: 5.8651, Total: 0.4058\n",
      "DER: 16.37%\n",
      "JER: 0.8664\n",
      "Overlap - P: 0.563, R: 0.674, F1: 0.614\n",
      "FA Rate: 0.532, Miss Rate: 0.075\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.015s, RTF: 0.011\n",
      "Learning Rate: 0.000009\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_17.pth\n",
      "Patience: 6/10\n",
      "\n",
      "============================================================\n",
      "Epoch 18/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:58<00:00,  2.70it/s, pit=0.1405, ovlp=0.1381, cont=6.2007, total=0.3477]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1057, Contrast: 5.8244, Total: 0.5419\n",
      "\n",
      "ðŸŽ¯ Calibrating adaptive thresholds...\n",
      "  Speaker 0: threshold=0.200, F1=0.411\n",
      "  Speaker 1: threshold=0.220, F1=0.368\n",
      "  Speaker 2: threshold=0.320, F1=0.307\n",
      "  Speaker 3: threshold=0.200, F1=0.222\n",
      "  Speaker 4: threshold=0.280, F1=0.173\n",
      "  Speaker 5: threshold=0.200, F1=0.120\n",
      "  Silence threshold: 0.330, F1=0.494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:49<00:00,  1.10it/s, loss=0.3700]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1115, Contrast: 5.8655, Total: 0.4047\n",
      "DER: 16.14%\n",
      "JER: 0.8599\n",
      "Overlap - P: 0.575, R: 0.666, F1: 0.617\n",
      "FA Rate: 0.584, Miss Rate: 0.044\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.015s, RTF: 0.011\n",
      "Learning Rate: 0.000008\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_18.pth\n",
      "Patience: 7/10\n",
      "\n",
      "============================================================\n",
      "Epoch 19/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:58<00:00,  2.69it/s, pit=0.0994, ovlp=0.1040, cont=5.8503, total=0.2554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1048, Contrast: 5.8466, Total: 0.5394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:49<00:00,  1.10it/s, loss=0.3683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1142, Contrast: 5.8664, Total: 0.4075\n",
      "DER: 16.61%\n",
      "JER: 0.8633\n",
      "Overlap - P: 0.538, R: 0.725, F1: 0.618\n",
      "FA Rate: 0.634, Miss Rate: 0.027\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.015s, RTF: 0.011\n",
      "Learning Rate: 0.000007\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_19.pth\n",
      "Patience: 8/10\n",
      "\n",
      "============================================================\n",
      "Epoch 20/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:59<00:00,  2.68it/s, pit=0.1979, ovlp=0.2561, cont=5.9141, total=0.5820]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1057, Contrast: 5.8630, Total: 0.5396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:48<00:00,  1.11it/s, loss=0.3705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1114, Contrast: 5.8657, Total: 0.4047\n",
      "DER: 16.33%\n",
      "JER: 0.8669\n",
      "Overlap - P: 0.552, R: 0.699, F1: 0.617\n",
      "FA Rate: 0.569, Miss Rate: 0.052\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.020s, RTF: 0.011\n",
      "Learning Rate: 0.000006\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_20.pth\n",
      "Patience: 9/10\n",
      "\n",
      "============================================================\n",
      "Epoch 21/30\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 481/481 [02:56<00:00,  2.72it/s, pit=0.0985, ovlp=0.1585, cont=6.1930, total=0.3361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - PIT: 0.1050, Contrast: 5.8542, Total: 0.5370\n",
      "\n",
      "ðŸŽ¯ Calibrating adaptive thresholds...\n",
      "  Speaker 0: threshold=0.200, F1=0.377\n",
      "  Speaker 1: threshold=0.200, F1=0.365\n",
      "  Speaker 2: threshold=0.340, F1=0.305\n",
      "  Speaker 3: threshold=0.200, F1=0.225\n",
      "  Speaker 4: threshold=0.280, F1=0.171\n",
      "  Speaker 5: threshold=0.200, F1=0.130\n",
      "  Silence threshold: 0.350, F1=0.495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54/54 [00:47<00:00,  1.15it/s, loss=0.3645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val - PIT: 0.1130, Contrast: 5.8659, Total: 0.4062\n",
      "DER: 16.54%\n",
      "JER: 0.8706\n",
      "Overlap - P: 0.639, R: 0.605, F1: 0.622\n",
      "FA Rate: 0.557, Miss Rate: 0.056\n",
      "Measuring inference time...\n",
      "Inference - Mel: 0.200s, Model: 0.016s, RTF: 0.011\n",
      "Learning Rate: 0.000005\n",
      "Checkpoint saved: checkpoints_phase3_simple/phase3_epoch_21.pth\n",
      "Patience: 10/10\n",
      "\n",
      "âš ï¸ Early stopping triggered after 21 epochs\n",
      "\n",
      "============================================================\n",
      "Training Complete!\n",
      "Best Val Loss: 0.4045\n",
      "Best DER: 15.00%\n",
      "CSV Log: logs/training_log_20251028_112818.csv\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Simplified Phase 3 Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ContraEEND - Phase 3: Adaptive Chunk-wise EEND with Memory\n",
    "Memory-augmented processing for long-form conversations\n",
    "\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, List, Optional\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from itertools import permutations\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import BatchSampler\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import time\n",
    "# ============================================================================\n",
    "# REUSE FROM PHASE 1 & 2\n",
    "# ============================================================================\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def setup_device():\n",
    "    \"\"\"Setup device with comprehensive CUDA checking\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"âœ… CUDA is available!\")\n",
    "        print(f\"ðŸš€ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"âš ï¸  CUDA not available, using CPU\")\n",
    "    return device\n",
    "\n",
    "setup_device()\n",
    "\n",
    "class AudioProcessor:\n",
    "    \"\"\"Unified audio processing pipeline\"\"\"\n",
    "    def __init__(self, sample_rate: int = 16000, n_fft: int = 400,\n",
    "                 hop_length: int = 160, n_mels: int = 83, win_length: int = 400):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.hop_length = hop_length\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length,\n",
    "            win_length=win_length, n_mels=n_mels, f_min=20, f_max=sample_rate // 2\n",
    "        )\n",
    "    \n",
    "    def __call__(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        if waveform.dim() == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        mel = self.mel_transform(waveform)\n",
    "        log_mel = torch.log(mel + 1e-6)\n",
    "        return log_mel.squeeze(0)\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    \"\"\"Single Conformer block\"\"\"\n",
    "    def __init__(self, d_model: int, n_heads: int, conv_kernel: int = 31, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.ff1 = nn.Sequential(\n",
    "            nn.LayerNorm(d_model), nn.Linear(d_model, d_model * 4), nn.SiLU(),\n",
    "            nn.Dropout(dropout), nn.Linear(d_model * 4, d_model), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm_attn = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.dropout_attn = nn.Dropout(dropout)\n",
    "        self.norm_conv = nn.LayerNorm(d_model)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(d_model, d_model * 2, 1), nn.GLU(dim=1),\n",
    "            nn.Conv1d(d_model, d_model, conv_kernel, padding=conv_kernel//2, groups=d_model),\n",
    "            nn.BatchNorm1d(d_model), nn.SiLU(), nn.Conv1d(d_model, d_model, 1), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.ff2 = nn.Sequential(\n",
    "            nn.LayerNorm(d_model), nn.Linear(d_model, d_model * 4), nn.SiLU(),\n",
    "            nn.Dropout(dropout), nn.Linear(d_model * 4, d_model), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.norm_out = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + 0.5 * self.ff1(x)\n",
    "        x_norm = self.norm_attn(x)\n",
    "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
    "        x = x + self.dropout_attn(attn_out)\n",
    "        x_norm = self.norm_conv(x)\n",
    "        x_conv = self.conv(x_norm.transpose(1, 2))\n",
    "        x = x + x_conv.transpose(1, 2)\n",
    "        x = x + 0.5 * self.ff2(x)\n",
    "        return self.norm_out(x)\n",
    "\n",
    "class ConformerEncoder(nn.Module):\n",
    "    \"\"\"Conformer encoder with exact output size calculation\"\"\"\n",
    "    def __init__(self, input_dim: int = 83, d_model: int = 128, n_layers: int = 6,\n",
    "                 n_heads: int = 4, conv_kernel: int = 31, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.subsampling = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, d_model, kernel_size=3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.pos_encoding = PositionalEncoding(d_model, dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            ConformerBlock(d_model, n_heads, conv_kernel, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_output_frames_static(input_frames: int) -> int:\n",
    "        \"\"\"Static method to compute exact output frames\"\"\"\n",
    "        frames = ((input_frames + 2 * 1 - 3) // 2) + 1\n",
    "        frames = ((frames + 2 * 1 - 3) // 2) + 1\n",
    "        return frames\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.subsampling(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding\"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 10000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class EENDDecoder(nn.Module):\n",
    "    \"\"\"EEND decoder with attention pooling\"\"\"\n",
    "    def __init__(self, d_model: int = 128, num_speakers: int = 6, n_layers: int = 2,\n",
    "                 n_heads: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.num_speakers = num_speakers\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            ConformerBlock(d_model, n_heads, conv_kernel=31, dropout=dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Add attention pooling\n",
    "        self.attention_pool = AttentionPooling(d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, num_speakers),\n",
    "        )\n",
    "    \n",
    "    def forward(self, encoded: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoded: (batch, frames, d_model)\n",
    "        Returns:\n",
    "            logits: (batch, frames, num_speakers)\n",
    "        \"\"\"\n",
    "        x = encoded\n",
    "        \n",
    "        # Decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        # Project to speaker logits\n",
    "        logits = self.output_proj(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class ContrastiveHead(nn.Module):\n",
    "    \"\"\"Frame-level contrastive head\"\"\"\n",
    "    def __init__(self, d_model: int = 128, projection_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, projection_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        proj = self.projection(x)\n",
    "        proj = F.normalize(proj, p=2, dim=-1)\n",
    "        return proj\n",
    "        \n",
    "class AttentionPooling(nn.Module):\n",
    "    \"\"\"Self-attention pooling for better speaker embeddings\"\"\"\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, frames, d_model)\n",
    "            mask: (batch, frames) - optional, 1 for valid frames\n",
    "        Returns:\n",
    "            pooled: (batch, d_model)\n",
    "        \"\"\"\n",
    "        # Compute attention weights\n",
    "        attn_weights = self.attention(x).squeeze(-1)  # (batch, frames)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(~mask.bool(), -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(attn_weights, dim=1)  # (batch, frames)\n",
    "        \n",
    "        # Weighted sum\n",
    "        pooled = (x * attn_weights.unsqueeze(-1)).sum(dim=1)  # (batch, d_model)\n",
    "        return pooled\n",
    "\n",
    "class ImprovedAdaptiveThresholdPredictor:\n",
    "    \"\"\"\n",
    "    Better threshold calibration that considers:\n",
    "    - Per-speaker thresholds\n",
    "    - Silence detection\n",
    "    - F1 optimization\n",
    "    \"\"\"\n",
    "    def __init__(self, num_speakers: int = 6):\n",
    "        self.num_speakers = num_speakers\n",
    "        self.thresholds = None\n",
    "        self.silence_threshold = 0.15  # If all speakers < this, predict silence\n",
    "    \n",
    "    def calibrate(self, model, val_loader, device):\n",
    "        \"\"\"Calibrate thresholds to maximize F1\"\"\"\n",
    "        print(\"\\nðŸŽ¯ Calibrating adaptive thresholds...\")\n",
    "        \n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for mel, labels, _ in val_loader:\n",
    "                mel = mel.to(device)\n",
    "                with torch.amp.autocast('cuda', enabled=(device.type=='cuda')):\n",
    "                    logits = model(mel, return_embeddings=False)\n",
    "                    probs = torch.sigmoid(logits)\n",
    "                \n",
    "                all_probs.append(probs.cpu())\n",
    "                all_labels.append(labels.cpu())\n",
    "        \n",
    "        all_probs = torch.cat(all_probs, dim=0)\n",
    "        all_labels = torch.cat(all_labels, dim=0)\n",
    "        \n",
    "        # Find optimal threshold per speaker using F1\n",
    "        self.thresholds = []\n",
    "        for spk in range(self.num_speakers):\n",
    "            best_thresh = 0.5\n",
    "            best_f1 = 0\n",
    "            \n",
    "            # Test thresholds from 0.2 to 0.8\n",
    "            for thresh in np.linspace(0.2, 0.8, 31):\n",
    "                pred = (all_probs[:, :, spk] > thresh).float()\n",
    "                \n",
    "                tp = ((pred == 1) & (all_labels[:, :, spk] == 1)).sum().item()\n",
    "                fp = ((pred == 1) & (all_labels[:, :, spk] == 0)).sum().item()\n",
    "                fn = ((pred == 0) & (all_labels[:, :, spk] == 1)).sum().item()\n",
    "                \n",
    "                prec = tp / (tp + fp + 1e-8)\n",
    "                rec = tp / (tp + fn + 1e-8)\n",
    "                f1 = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "                \n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    best_thresh = thresh\n",
    "            \n",
    "            self.thresholds.append(best_thresh)\n",
    "            print(f\"  Speaker {spk}: threshold={best_thresh:.3f}, F1={best_f1:.3f}\")\n",
    "        \n",
    "        self.thresholds = torch.tensor(self.thresholds)\n",
    "        \n",
    "        # Compute optimal silence threshold\n",
    "        # Find threshold where max(all_speakers) best separates silence from speech\n",
    "        has_speech = (all_labels.sum(dim=2) > 0).float()\n",
    "        max_probs = all_probs.max(dim=2)[0]\n",
    "        \n",
    "        best_silence_thresh = 0.15\n",
    "        best_silence_f1 = 0\n",
    "        \n",
    "        for thresh in np.linspace(0.05, 0.5, 46):\n",
    "            pred_silence = (max_probs < thresh).float()\n",
    "            true_silence = 1 - has_speech\n",
    "            \n",
    "            tp = ((pred_silence == 1) & (true_silence == 1)).sum().item()\n",
    "            fp = ((pred_silence == 1) & (true_silence == 0)).sum().item()\n",
    "            fn = ((pred_silence == 0) & (true_silence == 1)).sum().item()\n",
    "            \n",
    "            prec = tp / (tp + fp + 1e-8)\n",
    "            rec = tp / (tp + fn + 1e-8)\n",
    "            f1 = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "            \n",
    "            if f1 > best_silence_f1:\n",
    "                best_silence_f1 = f1\n",
    "                best_silence_thresh = thresh\n",
    "        \n",
    "        self.silence_threshold = best_silence_thresh\n",
    "        print(f\"  Silence threshold: {best_silence_thresh:.3f}, F1={best_silence_f1:.3f}\")\n",
    "    \n",
    "    def predict(self, probs):\n",
    "        \"\"\"Apply calibrated thresholds with silence detection\"\"\"\n",
    "        if self.thresholds is None:\n",
    "            return (probs > 0.5).float()\n",
    "        \n",
    "        # Apply per-speaker thresholds\n",
    "        thresholds = self.thresholds.to(probs.device).view(1, 1, -1)\n",
    "        activities = (probs > thresholds).float()\n",
    "        \n",
    "        # Silence detection: if all speakers below threshold, predict silence\n",
    "        max_probs = probs.max(dim=-1)[0]\n",
    "        silence_mask = max_probs < self.silence_threshold\n",
    "        activities[silence_mask.unsqueeze(-1).expand_as(activities)] = 0.0\n",
    "        \n",
    "        return activities\n",
    "\n",
    "class BalancedOverlapGating:\n",
    "    \"\"\"\n",
    "    Apply overlap predictions only when confident\n",
    "    Prevents false alarm explosion\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def apply(speaker_activities: torch.Tensor, \n",
    "              overlap_probs: torch.Tensor,\n",
    "              overlap_threshold: float = 0.65,\n",
    "              min_second_speaker_prob: float = 0.25) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            speaker_activities: (batch, frames, num_speakers) - binary\n",
    "            overlap_probs: (batch, frames) - probabilities\n",
    "            overlap_threshold: Confidence threshold for overlap\n",
    "            min_second_speaker_prob: Min probability for second speaker\n",
    "        \n",
    "        Returns:\n",
    "            gated_activities: Overlap only where confident\n",
    "        \"\"\"\n",
    "        # Don't enforce overlap where it's already detected\n",
    "        num_active = speaker_activities.sum(dim=-1)\n",
    "        \n",
    "        # Identify frames where overlap is predicted but confidence is low\n",
    "        low_confidence_overlap = (num_active >= 2) & (overlap_probs < overlap_threshold)\n",
    "        \n",
    "        # For low-confidence overlap, remove weaker speaker\n",
    "        if low_confidence_overlap.any():\n",
    "            for b in range(speaker_activities.shape[0]):\n",
    "                for f in range(speaker_activities.shape[1]):\n",
    "                    if low_confidence_overlap[b, f]:\n",
    "                        # Get active speakers\n",
    "                        active_speakers = speaker_activities[b, f].nonzero(as_tuple=True)[0]\n",
    "                        \n",
    "                        if len(active_speakers) >= 2:\n",
    "                            # Keep only the strongest speaker\n",
    "                            speaker_activities[b, f, :] = 0\n",
    "                            speaker_activities[b, f, active_speakers[0]] = 1\n",
    "        \n",
    "        return speaker_activities\n",
    "\n",
    "\n",
    "class OverlapDetectionHead(nn.Module):\n",
    "    \"\"\"Dedicated head for detecting overlapping speech\"\"\"\n",
    "    def __init__(self, d_model: int = 128, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.detector = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.detector(x)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 3: MEMORY COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "class SpeakerMemoryBank(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory bank that stores speaker representations across chunks.\n",
    "    Key innovation: Maintains speaker identity over time!\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int = 128, num_speakers: int = 4, \n",
    "                 memory_size: int = 100, temperature: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_speakers = num_speakers\n",
    "        self.memory_size = memory_size\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Memory storage: (num_speakers, memory_size, d_model)\n",
    "        self.register_buffer('memory', torch.zeros(num_speakers, memory_size, d_model))\n",
    "        self.register_buffer('memory_ptr', torch.zeros(num_speakers, dtype=torch.long))\n",
    "        self.register_buffer('memory_filled', torch.zeros(num_speakers, dtype=torch.bool))\n",
    "        \n",
    "        # Speaker statistics\n",
    "        self.register_buffer('speaker_counts', torch.zeros(num_speakers))\n",
    "        \n",
    "    def update(self, embeddings: torch.Tensor, speaker_labels: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Update memory with new speaker embeddings\n",
    "        \n",
    "        Args:\n",
    "            embeddings: (batch, frames, d_model)\n",
    "            speaker_labels: (batch, frames, num_speakers) - binary\n",
    "        \"\"\"\n",
    "        # Detach to prevent backprop through memory updates\n",
    "        embeddings = embeddings.detach()\n",
    "        speaker_labels = speaker_labels.detach()\n",
    "\n",
    "        batch_size, frames, _ = embeddings.shape\n",
    "        \n",
    "        for spk_idx in range(self.num_speakers):\n",
    "            # Get frames for this speaker\n",
    "            spk_mask = speaker_labels[:, :, spk_idx] > 0.5  # (batch, frames)\n",
    "            \n",
    "            if spk_mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            # Extract speaker embeddings\n",
    "            spk_embeddings = embeddings[spk_mask]  # (N, d_model)\n",
    "            \n",
    "            # Aggregate to single representation (mean pooling)\n",
    "            spk_repr = spk_embeddings.mean(dim=0)  # (d_model,)\n",
    "            \n",
    "            # Store in memory (FIFO)\n",
    "            ptr = self.memory_ptr[spk_idx].item()\n",
    "            self.memory[spk_idx, ptr] = spk_repr\n",
    "            \n",
    "            # Update pointer\n",
    "            self.memory_ptr[spk_idx] = (ptr + 1) % self.memory_size\n",
    "            \n",
    "            # Mark as filled after first full cycle\n",
    "            if ptr == self.memory_size - 1:\n",
    "                self.memory_filled[spk_idx] = True\n",
    "            \n",
    "            # Update statistics\n",
    "            self.speaker_counts[spk_idx] += spk_mask.sum().item()\n",
    "    \n",
    "    def query(self, embeddings: torch.Tensor, top_k: int = 5) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Query memory to get speaker context\n",
    "        \n",
    "        Args:\n",
    "            embeddings: (batch, frames, d_model)\n",
    "            top_k: Number of closest memory entries to retrieve\n",
    "        \n",
    "        Returns:\n",
    "            memory_context: (batch, frames, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, frames, d_model = embeddings.shape\n",
    "        \n",
    "        # Flatten embeddings for efficient computation\n",
    "        emb_flat = embeddings.reshape(-1, d_model)  # (B*F, D)\n",
    "        \n",
    "        # Compute similarity with all memory entries\n",
    "        memory_flat = self.memory.reshape(-1, d_model)  # (S*M, D)\n",
    "        \n",
    "        # Cosine similarity\n",
    "        sim = torch.mm(emb_flat, memory_flat.T)  # (B*F, S*M)\n",
    "        \n",
    "        # Get top-k most similar memories\n",
    "        top_k_sim, top_k_idx = torch.topk(sim, k=min(top_k, memory_flat.shape[0]), dim=1)\n",
    "        \n",
    "        # Retrieve corresponding memory entries\n",
    "        top_k_memories = memory_flat[top_k_idx]  # (B*F, top_k, D)\n",
    "        \n",
    "        # Weighted aggregation using similarity as attention\n",
    "        weights = F.softmax(top_k_sim / self.temperature, dim=1).unsqueeze(-1)  # (B*F, top_k, 1)\n",
    "        context = (top_k_memories * weights).sum(dim=1)  # (B*F, D)\n",
    "        \n",
    "        # Reshape back\n",
    "        context = context.reshape(batch_size, frames, d_model)\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def get_speaker_prototypes(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Get current speaker prototypes (mean of memory)\n",
    "        \n",
    "        Returns:\n",
    "            prototypes: (num_speakers, d_model)\n",
    "        \"\"\"\n",
    "        prototypes = []\n",
    "        for spk_idx in range(self.num_speakers):\n",
    "            if self.memory_filled[spk_idx]:\n",
    "                # Use all memory\n",
    "                proto = self.memory[spk_idx].mean(dim=0)\n",
    "            else:\n",
    "                # Use filled portion\n",
    "                ptr = self.memory_ptr[spk_idx].item()\n",
    "                if ptr > 0:\n",
    "                    proto = self.memory[spk_idx, :ptr].mean(dim=0)\n",
    "                else:\n",
    "                    proto = torch.zeros(self.d_model, device=self.memory.device)\n",
    "            prototypes.append(proto)\n",
    "        \n",
    "        return torch.stack(prototypes)  # (num_speakers, d_model)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset memory bank (for new conversation)\"\"\"\n",
    "        self.memory.zero_()\n",
    "        self.memory_ptr.zero_()\n",
    "        self.memory_filled.zero_()\n",
    "        self.speaker_counts.zero_()\n",
    "\n",
    "\n",
    "class CrossChunkAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-chunk attention to maintain temporal consistency.\n",
    "    Attends to memory context from previous chunks.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int = 128, n_heads: int = 4, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        # Multi-head cross attention\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            d_model, n_heads, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Layer norm\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, query: torch.Tensor, memory_context: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: Current chunk embeddings (batch, frames, d_model)\n",
    "            memory_context: Memory context (batch, frames, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            attended: Attended embeddings (batch, frames, d_model)\n",
    "        \"\"\"\n",
    "        # Cross attention: query attends to memory\n",
    "        attn_out, _ = self.cross_attn(\n",
    "            self.norm1(query),\n",
    "            self.norm1(memory_context),\n",
    "            self.norm1(memory_context)\n",
    "        )\n",
    "        \n",
    "        # Residual connection\n",
    "        x = query + attn_out\n",
    "        \n",
    "        # Feed-forward with residual\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeakerTracker(nn.Module):\n",
    "    \"\"\"\n",
    "    Speaker tracking module that resolves permutation problem.\n",
    "    Matches current chunk speakers to memory bank speakers.\n",
    "    Uses Hungarian algorithm for optimal assignment.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int = 128, similarity_threshold: float = 0.7):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        \n",
    "        # Projection for speaker embeddings\n",
    "        self.speaker_proj = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "    \n",
    "    def compute_speaker_embeddings(self, encoded: torch.Tensor, \n",
    "                                   activities: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Extract speaker embeddings from encoded features and activities\n",
    "        \n",
    "        Args:\n",
    "            encoded: (batch, frames, d_model)\n",
    "            activities: (batch, frames, num_speakers) - probabilities\n",
    "        \n",
    "        Returns:\n",
    "            speaker_embeddings: (batch, num_speakers, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, frames, d_model = encoded.shape\n",
    "        num_speakers = activities.shape[2]\n",
    "        \n",
    "        speaker_embeddings = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            batch_embeddings = []\n",
    "            for spk in range(num_speakers):\n",
    "                # Weighted pooling by speaker activity\n",
    "                weights = activities[b, :, spk].unsqueeze(-1)  # (frames, 1)\n",
    "                \n",
    "                # Normalize weights\n",
    "                weight_sum = weights.sum()\n",
    "                if weight_sum > 0:\n",
    "                    weights = weights / weight_sum\n",
    "                else:\n",
    "                    weights = torch.ones_like(weights) / frames\n",
    "                \n",
    "                # Weighted sum\n",
    "                spk_emb = (encoded[b] * weights).sum(dim=0)  # (d_model,)\n",
    "                batch_embeddings.append(spk_emb)\n",
    "            \n",
    "            speaker_embeddings.append(torch.stack(batch_embeddings))\n",
    "        \n",
    "        speaker_embeddings = torch.stack(speaker_embeddings)  # (batch, num_speakers, d_model)\n",
    "        \n",
    "        # Project\n",
    "        speaker_embeddings = self.speaker_proj(speaker_embeddings)\n",
    "        \n",
    "        return speaker_embeddings\n",
    "    \n",
    "    def match_speakers(self, current_embeddings: torch.Tensor,\n",
    "                      memory_prototypes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Match current speakers to memory bank speakers using Hungarian algorithm\n",
    "        \n",
    "        Args:\n",
    "            current_embeddings: (batch, num_speakers, d_model)\n",
    "            memory_prototypes: (num_speakers, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            permutation: (batch, num_speakers) - indices for reordering\n",
    "        \"\"\"\n",
    "        batch_size, num_speakers, d_model = current_embeddings.shape\n",
    "        \n",
    "        permutations = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            # Compute similarity matrix\n",
    "            sim_matrix = torch.mm(\n",
    "                current_embeddings[b],  # (num_speakers, d_model)\n",
    "                memory_prototypes.T     # (d_model, num_speakers)\n",
    "            )  # (num_speakers, num_speakers)\n",
    "            \n",
    "            # Convert to cost (maximize similarity = minimize negative similarity)\n",
    "            cost_matrix = -sim_matrix.detach().cpu().numpy()\n",
    "            \n",
    "            # Hungarian algorithm\n",
    "            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "            \n",
    "            # Create permutation tensor\n",
    "            perm = torch.tensor(col_ind, dtype=torch.long, device=current_embeddings.device)\n",
    "            permutations.append(perm)\n",
    "        \n",
    "        return torch.stack(permutations)  # (batch, num_speakers)\n",
    "    \n",
    "    def apply_permutation(self, activities: torch.Tensor, \n",
    "                         permutation: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply permutation to reorder speaker activities\n",
    "        \n",
    "        Args:\n",
    "            activities: (batch, frames, num_speakers)\n",
    "            permutation: (batch, num_speakers)\n",
    "        \n",
    "        Returns:\n",
    "            reordered_activities: (batch, frames, num_speakers)\n",
    "        \"\"\"\n",
    "        batch_size, frames, num_speakers = activities.shape\n",
    "        \n",
    "        # Create index tensor for gathering\n",
    "        perm_expanded = permutation.unsqueeze(1).expand(batch_size, frames, num_speakers)\n",
    "        \n",
    "        # Reorder\n",
    "        reordered = torch.gather(activities, dim=2, index=perm_expanded)\n",
    "        \n",
    "        return reordered\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 3: ADAPTIVE CHUNK-WISE EEND MODEL\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "class SimplifiedPhase3EEND(nn.Module):\n",
    "    \"\"\"Enhanced Phase 3 with overlap detection\"\"\"\n",
    "    def __init__(self, input_dim: int = 83, d_model: int = 128,\n",
    "                 encoder_layers: int = 6, decoder_layers: int = 2,\n",
    "                 n_heads: int = 4, num_speakers: int = 6,\n",
    "                 projection_dim: int = 64, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_speakers = num_speakers\n",
    "        \n",
    "        self.encoder = ConformerEncoder(\n",
    "            input_dim=input_dim, d_model=d_model, n_layers=encoder_layers,\n",
    "            n_heads=n_heads, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.decoder = EENDDecoder(\n",
    "            d_model=d_model, num_speakers=num_speakers,\n",
    "            n_layers=decoder_layers, n_heads=n_heads, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # NEW: Overlap detection head\n",
    "        self.overlap_detector = OverlapDetectionHead(d_model, dropout)\n",
    "        \n",
    "        self.contrastive_head = ContrastiveHead(d_model, projection_dim)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, return_embeddings: bool = False):\n",
    "        encoded = self.encoder(x)\n",
    "        logits = self.decoder(encoded)\n",
    "        \n",
    "        if return_embeddings:\n",
    "            overlap_logits = self.overlap_detector(encoded)\n",
    "            embeddings = self.contrastive_head(encoded)\n",
    "            return logits, overlap_logits, embeddings\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def predict(self, x: torch.Tensor, threshold_predictor=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Balanced prediction with:\n",
    "        1. Calibrated thresholds\n",
    "        2. Silence detection\n",
    "        3. Conservative overlap enforcement\n",
    "        \"\"\"\n",
    "        encoded = self.encoder(x)\n",
    "        logits = self.decoder(encoded)\n",
    "        overlap_logits = self.overlap_detector(encoded)\n",
    "        \n",
    "        speaker_probs = torch.sigmoid(logits)\n",
    "        overlap_probs = torch.sigmoid(overlap_logits).squeeze(-1)\n",
    "        \n",
    "        # Step 1: Base speaker activities with calibrated thresholds\n",
    "        if threshold_predictor is not None and threshold_predictor.thresholds is not None:\n",
    "            activities = threshold_predictor.predict(speaker_probs)\n",
    "        else:\n",
    "            # Fallback: higher default threshold to reduce FA\n",
    "            activities = (speaker_probs > 0.6).float()\n",
    "            \n",
    "            # Simple silence detection\n",
    "            max_probs = speaker_probs.max(dim=-1)[0]\n",
    "            silence_mask = max_probs < 0.2\n",
    "            activities[silence_mask.unsqueeze(-1).expand_as(activities)] = 0.0\n",
    "        \n",
    "        # Step 2: Overlap enforcement (conservative)\n",
    "        num_active = activities.sum(dim=-1)\n",
    "        \n",
    "        # Only add overlap if:\n",
    "        # 1. Overlap probability is high (>0.6)\n",
    "        # 2. Currently 0-1 speakers active\n",
    "        # 3. Second speaker has decent probability (>0.3)\n",
    "        overlap_detected = overlap_probs > 0.6\n",
    "        needs_overlap = overlap_detected & (num_active < 2)\n",
    "        \n",
    "        if needs_overlap.any():\n",
    "            for b in range(activities.shape[0]):\n",
    "                for f in range(activities.shape[1]):\n",
    "                    if needs_overlap[b, f]:\n",
    "                        probs = speaker_probs[b, f]\n",
    "                        top2_probs, top2_idx = torch.topk(probs, k=2)\n",
    "                        \n",
    "                        # Require minimum confidence for second speaker\n",
    "                        if top2_probs[1] > 0.3:\n",
    "                            activities[b, f, top2_idx] = 1.0\n",
    "        \n",
    "        # Step 3: Remove low-confidence overlaps\n",
    "        # If overlap detected but overlap_prob is low, keep only strongest speaker\n",
    "        low_confidence_overlap = (num_active >= 2) & (overlap_probs < 0.4)\n",
    "        \n",
    "        if low_confidence_overlap.any():\n",
    "            for b in range(activities.shape[0]):\n",
    "                for f in range(activities.shape[1]):\n",
    "                    if low_confidence_overlap[b, f]:\n",
    "                        # Find strongest speaker\n",
    "                        probs = speaker_probs[b, f]\n",
    "                        active_mask = activities[b, f] > 0\n",
    "                        \n",
    "                        if active_mask.sum() >= 2:\n",
    "                            # Keep only strongest\n",
    "                            strongest_idx = (probs * active_mask).argmax()\n",
    "                            activities[b, f, :] = 0\n",
    "                            activities[b, f, strongest_idx] = 1\n",
    "        \n",
    "        return activities\n",
    "    \n",
    "    def load_phase2_weights(self, checkpoint_path: str):\n",
    "        \"\"\"Load Phase 2 weights\"\"\"\n",
    "        print(f\"\\nðŸ“¦ Loading Phase 2 checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        \n",
    "        phase2_state = checkpoint['model_state_dict']\n",
    "        model_state = {}\n",
    "        \n",
    "        for key, value in phase2_state.items():\n",
    "            if 'decoder.output_proj.3' in key:\n",
    "                if value.shape[0] != self.num_speakers:\n",
    "                    print(f\"âš ï¸  Skipping {key}\")\n",
    "                    continue\n",
    "            \n",
    "            if key.startswith(('encoder.', 'decoder.', 'contrastive_head.')):\n",
    "                model_state[key] = value\n",
    "        \n",
    "        missing, unexpected = self.load_state_dict(model_state, strict=False)\n",
    "        print(\"âœ“ Phase 2 weights loaded (overlap_detector randomly initialized)\")\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 3: CHUNK-WISE DATASET\n",
    "# ============================================================================\n",
    "\n",
    "def build_audio_rttm_mapping(audio_dir: str, rttm_dir: str) -> List[Tuple[Path, Path]]:\n",
    "    \"\"\"Build mapping - works for both CallHome and VoxConverse\"\"\"\n",
    "    audio_dir = Path(audio_dir)\n",
    "    rttm_dir = Path(rttm_dir)\n",
    "    \n",
    "    # Try different extensions\n",
    "    audio_files = sorted(audio_dir.glob('*.wav'))\n",
    "    if len(audio_files) == 0:\n",
    "        audio_files = sorted(audio_dir.glob('*.flac'))\n",
    "    if len(audio_files) == 0:\n",
    "        audio_files = sorted(audio_dir.glob('*.mp3'))\n",
    "    \n",
    "    rttm_files = sorted(rttm_dir.glob('*.rttm'))\n",
    "    \n",
    "    print(f\"\\nFound {len(audio_files)} audio files\")\n",
    "    print(f\"Found {len(rttm_files)} RTTM files\")\n",
    "    \n",
    "    # Build mapping by filename (not index!)\n",
    "    audio_map = {}\n",
    "    for audio_file in audio_files:\n",
    "        # Use stem (filename without extension) as key\n",
    "        key = audio_file.stem\n",
    "        audio_map[key] = audio_file\n",
    "    \n",
    "    rttm_map = {}\n",
    "    for rttm_file in rttm_files:\n",
    "        key = rttm_file.stem\n",
    "        rttm_map[key] = rttm_file\n",
    "    \n",
    "    # Match by filename\n",
    "    pairs = []\n",
    "    for key in sorted(audio_map.keys()):\n",
    "        if key in rttm_map:\n",
    "            pairs.append((audio_map[key], rttm_map[key]))\n",
    "        else:\n",
    "            print(f\"âš ï¸  No RTTM for audio: {key}\")\n",
    "    \n",
    "    print(f\"Matched {len(pairs)} audio-RTTM pairs\\n\")\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# --- Audio-style data augmentation for EEND ---\n",
    "class ImprovedFeatureAugmentor:\n",
    "    \"\"\"\n",
    "    Diarization-friendly augmentation:\n",
    "    - No pitch/time changes (preserves speaker identity)\n",
    "    - Focus on noise robustness\n",
    "    - Preserves temporal alignment with labels\n",
    "    \"\"\"\n",
    "    def __init__(self, noise_std=0.002, spec_augment_prob=0.3):\n",
    "        self.noise_std = noise_std\n",
    "        self.spec_augment_prob = spec_augment_prob\n",
    "    \n",
    "    def add_gaussian_noise(self, mel_spec):\n",
    "        \"\"\"Add light Gaussian noise to mel spectrogram\"\"\"\n",
    "        noise = torch.randn_like(mel_spec) * self.noise_std\n",
    "        return mel_spec + noise\n",
    "    \n",
    "    def spec_augment(self, mel_spec):\n",
    "        \"\"\"\n",
    "        SpecAugment: mask time/frequency bands\n",
    "        Keeps temporal structure intact\n",
    "        \"\"\"\n",
    "        n_mels, time_steps = mel_spec.shape\n",
    "        \n",
    "        # Frequency masking (mask mel bins)\n",
    "        if random.random() < 0.5:\n",
    "            f_mask_width = random.randint(1, max(1, n_mels // 10))\n",
    "            f_mask_start = random.randint(0, n_mels - f_mask_width)\n",
    "            mel_spec[f_mask_start:f_mask_start + f_mask_width, :] = 0\n",
    "        \n",
    "        # Time masking (mask short segments)\n",
    "        if random.random() < 0.5:\n",
    "            t_mask_width = random.randint(1, max(1, time_steps // 20))\n",
    "            t_mask_start = random.randint(0, time_steps - t_mask_width)\n",
    "            mel_spec[:, t_mask_start:t_mask_start + t_mask_width] = 0\n",
    "        \n",
    "        return mel_spec\n",
    "    \n",
    "    def __call__(self, mel_spec, apply=True):\n",
    "        \"\"\"\n",
    "        Apply augmentation to mel spectrogram\n",
    "        \n",
    "        Args:\n",
    "            mel_spec: torch.Tensor (n_mels, time)\n",
    "            apply: bool - whether to apply augmentation\n",
    "        \"\"\"\n",
    "        if not apply:\n",
    "            return mel_spec\n",
    "        \n",
    "        # Light Gaussian noise (always apply for robustness)\n",
    "        mel_spec = self.add_gaussian_noise(mel_spec)\n",
    "        \n",
    "        # SpecAugment (probabilistic)\n",
    "        if random.random() < self.spec_augment_prob:\n",
    "            mel_spec = self.spec_augment(mel_spec)\n",
    "        \n",
    "        return mel_spec\n",
    "        \n",
    "\n",
    "class ChunkWiseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for chunk-wise processing (Phase 3)\n",
    "    Processes long conversations in overlapping chunks\n",
    "    \"\"\"\n",
    "    def __init__(self, audio_dir: str, rttm_dir: str, audio_processor: AudioProcessor,\n",
    "                 chunk_size: float = 30.0, overlap: float = 5.0,\n",
    "                 sample_rate: int = 16000, max_speakers: int = 20, augment: bool = True):\n",
    "        \n",
    "        self.audio_dir = Path(audio_dir)\n",
    "        self.rttm_dir = Path(rttm_dir)\n",
    "        self.audio_processor = audio_processor\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_speakers = max_speakers\n",
    "        self.augment = augment\n",
    "\n",
    "        #Augmentation\n",
    "        if augment:\n",
    "            self.augmentor = ImprovedFeatureAugmentor(\n",
    "                noise_std=0.002,\n",
    "                spec_augment_prob=0.3\n",
    "            )\n",
    "        \n",
    "        # Chunk stride (non-overlapping portion)\n",
    "        self.stride = chunk_size - overlap\n",
    "        \n",
    "        # Build mapping\n",
    "        self.audio_rttm_pairs = build_audio_rttm_mapping(audio_dir, rttm_dir)\n",
    "        \n",
    "        # Parse into chunks\n",
    "        self.chunks = self._create_chunks()\n",
    "        \n",
    "        print(f\"âœ“ Created {len(self.chunks)} chunks from {len(self.audio_rttm_pairs)} conversations\")\n",
    "        print(f\"  Chunk size: {chunk_size}s, Overlap: {overlap}s, Stride: {self.stride}s\")\n",
    "    \n",
    "    def _create_chunks(self) -> List[Dict]:\n",
    "        \"\"\"Create overlapping chunks from full conversations\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for audio_path, rttm_path in tqdm(self.audio_rttm_pairs, desc=\"Creating chunks\"):\n",
    "            # Get audio duration\n",
    "            info = torchaudio.info(str(audio_path))\n",
    "            duration = info.num_frames / info.sample_rate\n",
    "            \n",
    "            # Parse RTTM\n",
    "            speaker_segments = {}\n",
    "            with open(rttm_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) < 8 or parts[0] != 'SPEAKER':\n",
    "                        continue\n",
    "                    \n",
    "                    start = float(parts[3])\n",
    "                    dur = float(parts[4])\n",
    "                    speaker = parts[7]\n",
    "                    \n",
    "                    if speaker not in speaker_segments:\n",
    "                        speaker_segments[speaker] = []\n",
    "                    \n",
    "                    speaker_segments[speaker].append({\n",
    "                        'start': start,\n",
    "                        'end': start + dur\n",
    "                    })\n",
    "            \n",
    "            if len(speaker_segments) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Create overlapping chunks\n",
    "            chunk_start = 0.0\n",
    "            chunk_idx = 0\n",
    "            \n",
    "            while chunk_start < duration:\n",
    "                chunk_end = min(chunk_start + self.chunk_size, duration)\n",
    "                \n",
    "                # Only add chunk if it's long enough\n",
    "                if chunk_end - chunk_start >= self.chunk_size * 0.5:\n",
    "                    chunks.append({\n",
    "                        'audio_path': audio_path,\n",
    "                        'rttm_path': rttm_path,\n",
    "                        'chunk_start': chunk_start,\n",
    "                        'chunk_end': chunk_end,\n",
    "                        'speaker_segments': speaker_segments,\n",
    "                        'conversation_id': audio_path.stem,\n",
    "                        'chunk_idx': chunk_idx,\n",
    "                        'total_duration': duration\n",
    "                    })\n",
    "                    chunk_idx += 1\n",
    "                \n",
    "                # Move to next chunk\n",
    "                chunk_start += self.stride\n",
    "                \n",
    "                # Stop if we've covered the whole audio\n",
    "                if chunk_end >= duration:\n",
    "                    break\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _create_label_tensor(self, speaker_segments: Dict, chunk_start: float,\n",
    "                            chunk_end: float, num_frames: int) -> torch.Tensor:\n",
    "        \"\"\"Create frame-level labels for chunk\"\"\"\n",
    "        labels = torch.zeros(num_frames, self.max_speakers)\n",
    "        \n",
    "        speaker_list = sorted(speaker_segments.keys())[:self.max_speakers]\n",
    "        frame_duration = (chunk_end - chunk_start) / num_frames\n",
    "        \n",
    "        for spk_idx, speaker in enumerate(speaker_list):\n",
    "            segments = speaker_segments[speaker]\n",
    "            \n",
    "            for segment in segments:\n",
    "                seg_start = segment['start']\n",
    "                seg_end = segment['end']\n",
    "                \n",
    "                # Check overlap with chunk\n",
    "                overlap_start = max(seg_start, chunk_start)\n",
    "                overlap_end = min(seg_end, chunk_end)\n",
    "                \n",
    "                if overlap_start < overlap_end:\n",
    "                    # Convert to frame indices\n",
    "                    frame_start = int((overlap_start - chunk_start) / frame_duration)\n",
    "                    frame_end = int((overlap_end - chunk_start) / frame_duration)\n",
    "                    \n",
    "                    frame_start = max(0, min(frame_start, num_frames - 1))\n",
    "                    frame_end = max(0, min(frame_end, num_frames))\n",
    "                    \n",
    "                    labels[frame_start:frame_end, spk_idx] = 1.0\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.chunks)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            mel: (n_mels, frames)\n",
    "            labels: (encoder_frames, max_speakers)\n",
    "            metadata: Dict with chunk info\n",
    "        \"\"\"\n",
    "        chunk = self.chunks[idx]\n",
    "        \n",
    "        # Load audio chunk\n",
    "        start_frame = int(chunk['chunk_start'] * self.sample_rate)\n",
    "        num_frames = int((chunk['chunk_end'] - chunk['chunk_start']) * self.sample_rate)\n",
    "        \n",
    "        waveform, sr = torchaudio.load(\n",
    "            str(chunk['audio_path']),\n",
    "            frame_offset=start_frame,\n",
    "            num_frames=num_frames\n",
    "        )\n",
    "        \n",
    "        # Resample if needed\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Ensure correct length\n",
    "        if waveform.shape[1] < num_frames:\n",
    "            waveform = F.pad(waveform, (0, num_frames - waveform.shape[1]))\n",
    "        elif waveform.shape[1] > num_frames:\n",
    "            waveform = waveform[:, :num_frames]\n",
    "        \n",
    "        # Convert to mel\n",
    "        mel = self.audio_processor(waveform.squeeze(0))\n",
    "        \n",
    "        # Apply augmentation if in training mode\n",
    "        if self.augment:\n",
    "            mel = self.augmentor(mel, apply=True)\n",
    "        \n",
    "        mel_frames = mel.shape[1]\n",
    "        encoder_frames = ConformerEncoder.compute_output_frames_static(mel_frames)\n",
    "        \n",
    "        labels = self._create_label_tensor(\n",
    "            chunk['speaker_segments'],\n",
    "            chunk['chunk_start'],\n",
    "            chunk['chunk_end'],\n",
    "            encoder_frames\n",
    "        )\n",
    "        \n",
    "        metadata = {\n",
    "            'conversation_id': chunk['conversation_id'],\n",
    "            'chunk_idx': chunk['chunk_idx'],\n",
    "            'chunk_start': chunk['chunk_start'],\n",
    "            'chunk_end': chunk['chunk_end']\n",
    "        }\n",
    "        \n",
    "        return mel, labels, metadata\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 3: LOSS FUNCTIONS WITH HARD NEGATIVE MINING\n",
    "# ============================================================================\n",
    "\n",
    "class ImprovedOverlapWeightedPITLoss(nn.Module):\n",
    "    \"\"\"Enhanced PIT loss with focal loss and higher overlap weighting\"\"\"\n",
    "    def __init__(self, overlap_weight: float = 10.0, focal_gamma: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.overlap_weight = overlap_weight\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self._cached_perms = None\n",
    "    \n",
    "    def focal_bce_loss(self, pred_logits, target):\n",
    "        \"\"\"Focal loss to focus on hard examples\"\"\"\n",
    "        bce = F.binary_cross_entropy_with_logits(pred_logits, target, reduction='none')\n",
    "        \n",
    "        pred_probs = torch.sigmoid(pred_logits)\n",
    "        pt = target * pred_probs + (1 - target) * (1 - pred_probs)\n",
    "        focal_weight = (1 - pt) ** self.focal_gamma\n",
    "        \n",
    "        return focal_weight * bce\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, frames, num_speakers = pred.shape\n",
    "        \n",
    "        overlap_mask = (target.sum(dim=2) > 1).float()\n",
    "        \n",
    "        weights = torch.ones(batch_size, frames, 1, device=pred.device)\n",
    "        weights[overlap_mask > 0] = self.overlap_weight\n",
    "        \n",
    "        if self._cached_perms is None or len(self._cached_perms[0]) != num_speakers:\n",
    "            self._cached_perms = list(permutations(range(num_speakers)))\n",
    "        \n",
    "        min_loss = float('inf')\n",
    "        \n",
    "        for perm in self._cached_perms:\n",
    "            pred_perm = pred[:, :, list(perm)]\n",
    "            loss_per_element = self.focal_bce_loss(pred_perm, target)\n",
    "            loss = (loss_per_element * weights).mean()\n",
    "            \n",
    "            if loss < min_loss:\n",
    "                min_loss = loss\n",
    "                if min_loss < 0.01:\n",
    "                    break\n",
    "        \n",
    "        return min_loss\n",
    "\n",
    "\n",
    "\n",
    "class BalancedOverlapAwareLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Properly balanced loss that handles:\n",
    "    1. Base speaker detection\n",
    "    2. Overlap detection\n",
    "    3. Silence frames\n",
    "    \"\"\"\n",
    "    def __init__(self, overlap_weight: float = 6.0, \n",
    "                 overlap_detection_weight: float = 1.5,\n",
    "                 focal_gamma: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.overlap_weight = overlap_weight\n",
    "        self.overlap_detection_weight = overlap_detection_weight\n",
    "        self.focal_gamma = focal_gamma\n",
    "        self._cached_perms = None\n",
    "    \n",
    "    def focal_bce_loss(self, pred_logits, target):\n",
    "        \"\"\"Focal loss - focuses on hard examples\"\"\"\n",
    "        bce = F.binary_cross_entropy_with_logits(pred_logits, target, reduction='none')\n",
    "        \n",
    "        pred_probs = torch.sigmoid(pred_logits)\n",
    "        pt = target * pred_probs + (1 - target) * (1 - pred_probs)\n",
    "        focal_weight = (1 - pt) ** self.focal_gamma\n",
    "        \n",
    "        return focal_weight * bce\n",
    "    \n",
    "    def forward(self, speaker_logits, overlap_logits, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            speaker_logits: (batch, frames, num_speakers)\n",
    "            overlap_logits: (batch, frames, 1)\n",
    "            labels: (batch, frames, num_speakers)\n",
    "        \"\"\"\n",
    "        batch_size, frames, num_speakers = speaker_logits.shape\n",
    "        \n",
    "        # === 1. Speaker Diarization Loss (PIT with overlap weighting) ===\n",
    "        \n",
    "        # Detect overlap and silence frames\n",
    "        num_speakers_active = labels.sum(dim=2)\n",
    "        overlap_mask = (num_speakers_active > 1).float()  # >1 speaker\n",
    "        silence_mask = (num_speakers_active == 0).float()  # 0 speakers\n",
    "        \n",
    "        # Create frame weights\n",
    "        weights = torch.ones(batch_size, frames, 1, device=speaker_logits.device)\n",
    "        weights[overlap_mask > 0] = self.overlap_weight      # Emphasize overlap\n",
    "        weights[silence_mask > 0] = 1.5                       # Slightly emphasize silence (reduce FA)\n",
    "        \n",
    "        # PIT loss\n",
    "        if self._cached_perms is None or len(self._cached_perms[0]) != num_speakers:\n",
    "            self._cached_perms = list(permutations(range(num_speakers)))\n",
    "        \n",
    "        min_pit_loss = float('inf')\n",
    "        \n",
    "        for perm in self._cached_perms:\n",
    "            speaker_logits_perm = speaker_logits[:, :, list(perm)]\n",
    "            \n",
    "            # Focal BCE for hard examples\n",
    "            loss_per_element = self.focal_bce_loss(speaker_logits_perm, labels)\n",
    "            loss = (loss_per_element * weights).mean()\n",
    "            \n",
    "            if loss < min_pit_loss:\n",
    "                min_pit_loss = loss\n",
    "                if min_pit_loss < 0.01:\n",
    "                    break\n",
    "        \n",
    "        # === 2. Overlap Detection Loss ===\n",
    "        \n",
    "        overlap_labels = (num_speakers_active > 1).float().unsqueeze(-1)\n",
    "        \n",
    "        # Balanced pos_weight (not too high)\n",
    "        pos_weight = torch.tensor([3.0], device=overlap_logits.device)\n",
    "        \n",
    "        overlap_loss = F.binary_cross_entropy_with_logits(\n",
    "            overlap_logits, \n",
    "            overlap_labels,\n",
    "            pos_weight=pos_weight\n",
    "        )\n",
    "        \n",
    "        # === 3. Combined Loss ===\n",
    "        \n",
    "        total_loss = min_pit_loss + self.overlap_detection_weight * overlap_loss\n",
    "        \n",
    "        return total_loss, min_pit_loss, overlap_loss\n",
    "\n",
    "\n",
    "class HardNegativeContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced contrastive loss with hard negative mining (Phase 3)\n",
    "    Mines hard negatives from memory bank\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature: float = 0.1, max_samples: int = 500,\n",
    "                 hard_negative_ratio: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.max_samples = max_samples\n",
    "        self.hard_negative_ratio = hard_negative_ratio\n",
    "    \n",
    "    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor,\n",
    "            memory_prototypes: Optional[torch.Tensor] = None,\n",
    "            contrastive_head: Optional[nn.Module] = None) -> torch.Tensor:  #  Add constrastive head parameter\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings: (batch, frames, projection_dim) - L2 normalized\n",
    "            labels: (batch, frames, num_speakers) - binary\n",
    "            memory_prototypes: (num_speakers, projection_dim) - speaker prototypes from memory\n",
    "        \"\"\"\n",
    "        batch_size, frames, proj_dim = embeddings.shape\n",
    "        num_speakers = labels.shape[2]\n",
    "        \n",
    "        total_loss = 0\n",
    "        valid_speakers = 0\n",
    "        \n",
    "        for spk in range(num_speakers):\n",
    "            spk_labels = labels[:, :, spk]\n",
    "            active_mask = spk_labels > 0.5\n",
    "            \n",
    "            if active_mask.sum() < 2:\n",
    "                continue\n",
    "            \n",
    "            active_embeddings = embeddings[active_mask]\n",
    "            \n",
    "            # Sample if too many\n",
    "            if len(active_embeddings) > self.max_samples:\n",
    "                indices = torch.randperm(len(active_embeddings), device=embeddings.device)[:self.max_samples]\n",
    "                active_embeddings = active_embeddings[indices]\n",
    "            \n",
    "            if len(active_embeddings) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Compute similarity matrix\n",
    "            sim_matrix = torch.mm(active_embeddings, active_embeddings.T) / self.temperature\n",
    "            \n",
    "            # Add hard negatives from memory if available\n",
    "            if memory_prototypes is not None and spk < len(memory_prototypes):\n",
    "                # Get memory prototype for OTHER speakers (hard negatives)\n",
    "                other_speaker_protos = []\n",
    "                for other_spk in range(num_speakers):\n",
    "                    if other_spk != spk:\n",
    "                        other_speaker_protos.append(memory_prototypes[other_spk])\n",
    "                \n",
    "                if len(other_speaker_protos) > 0:\n",
    "                    other_protos = torch.stack(other_speaker_protos)  # (n_others, d_model=128)\n",
    "                    \n",
    "                    if contrastive_head is not None:\n",
    "                        other_protos = contrastive_head.projection(other_protos)  # (n_others, 64)\n",
    "                        other_protos = F.normalize(other_protos, p=2, dim=-1)\n",
    "                                \n",
    "                    # Similarity to hard negatives\n",
    "                    hard_neg_sim = torch.mm(active_embeddings, other_protos.T) / self.temperature\n",
    "                    \n",
    "                    # Select hardest negatives (highest similarity to wrong speakers)\n",
    "                    n_hard = int(len(active_embeddings) * self.hard_negative_ratio)\n",
    "                    if n_hard > 0:\n",
    "                        hardest_sim, _ = hard_neg_sim.max(dim=1)  # Max similarity to wrong speaker\n",
    "                        hard_indices = torch.topk(hardest_sim, k=min(n_hard, len(hardest_sim)))[1]\n",
    "                        \n",
    "                        # Weight hard negatives more\n",
    "                        weights = torch.ones(len(active_embeddings), device=embeddings.device)\n",
    "                        weights[hard_indices] *= 2.0  # Double weight for hard negatives\n",
    "                    else:\n",
    "                        weights = torch.ones(len(active_embeddings), device=embeddings.device)\n",
    "                else:\n",
    "                    weights = torch.ones(len(active_embeddings), device=embeddings.device)\n",
    "            else:\n",
    "                weights = torch.ones(len(active_embeddings), device=embeddings.device)\n",
    "            \n",
    "            # Mask diagonal\n",
    "            mask = torch.ones_like(sim_matrix)\n",
    "            mask.fill_diagonal_(0)\n",
    "            \n",
    "            # InfoNCE loss with weights\n",
    "            exp_sim = torch.exp(sim_matrix) * mask\n",
    "            log_prob = sim_matrix - torch.log(exp_sim.sum(1, keepdim=True) + 1e-8)\n",
    "            loss = -(mask * log_prob).sum(1) / mask.sum(1)\n",
    "            \n",
    "            # Apply weights\n",
    "            weighted_loss = (loss * weights).sum() / weights.sum()\n",
    "            \n",
    "            total_loss += weighted_loss\n",
    "            valid_speakers += 1\n",
    "        \n",
    "        if valid_speakers == 0:\n",
    "            return torch.tensor(0.0, device=embeddings.device)\n",
    "        \n",
    "        return total_loss / valid_speakers\n",
    "\n",
    "\n",
    "class MemoryConsistencyLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    NEW: Memory consistency loss\n",
    "    Encourages consistent speaker representations across chunks\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, speaker_embeddings: torch.Tensor,\n",
    "                memory_prototypes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            speaker_embeddings: (batch, num_speakers, d_model)\n",
    "            memory_prototypes: (num_speakers, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            loss: scalar - encourages current embeddings to match memory\n",
    "        \"\"\"\n",
    "        # Normalize\n",
    "        speaker_embeddings = F.normalize(speaker_embeddings, p=2, dim=-1)\n",
    "        memory_prototypes = F.normalize(memory_prototypes, p=2, dim=-1)\n",
    "        \n",
    "        # Cosine similarity\n",
    "        similarity = torch.mm(\n",
    "            speaker_embeddings.reshape(-1, speaker_embeddings.shape[-1]),\n",
    "            memory_prototypes.T\n",
    "        )\n",
    "        \n",
    "        # We want diagonal to be high (each speaker matches its prototype)\n",
    "        batch_size = speaker_embeddings.shape[0]\n",
    "        num_speakers = speaker_embeddings.shape[1]\n",
    "        \n",
    "        # Create target: identity matrix repeated for batch\n",
    "        target = torch.eye(num_speakers, device=speaker_embeddings.device)\n",
    "        target = target.unsqueeze(0).repeat(batch_size, 1, 1).reshape(-1, num_speakers)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        log_sim = F.log_softmax(similarity, dim=1)\n",
    "        loss = -(target * log_sim).sum(1).mean()\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "# ============================================================================\n",
    "# PHASE 3: TRAINER\n",
    "# ============================================================================\n",
    "\n",
    "class ImprovedPhase3Trainer:\n",
    "    \"\"\"Enhanced trainer with CSV logging and better monitoring\"\"\"\n",
    "    def __init__(self, model: nn.Module, train_loader: DataLoader,\n",
    "                 val_loader: DataLoader, device: str = 'cuda',\n",
    "                 learning_rate: float = 5e-5, weight_decay: float = 1e-4,\n",
    "                 contrastive_weight: float = 0.02,  # Reduced from 0.05\n",
    "                 log_dir: str = './logs'):\n",
    "        \n",
    "        # Normalize device to torch.device\n",
    "        self.device = torch.device(device) if isinstance(device, str) else device\n",
    "        self.model = model.to(self.device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.contrastive_weight = contrastive_weight\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create CSV log file\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.csv_path = self.log_dir / f'training_log_{timestamp}.csv'\n",
    "        self._init_csv()\n",
    "        \n",
    "        self.overlap_aware_loss = BalancedOverlapAwareLoss(\n",
    "            overlap_weight=6.0,\n",
    "            overlap_detection_weight=1.5,\n",
    "            focal_gamma=2.0\n",
    "        )\n",
    "        self.contrastive_loss = HardNegativeContrastiveLoss(temperature=0.1)\n",
    "\n",
    "        self.threshold_predictor = ImprovedAdaptiveThresholdPredictor(\n",
    "            num_speakers=model.num_speakers\n",
    "        )\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "            betas=(0.9, 0.98)\n",
    "        )\n",
    "        \n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer,\n",
    "            T_max=30,\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        self.use_amp = (self.device.type == 'cuda')\n",
    "        self.scaler = torch.amp.GradScaler('cuda') if self.use_amp else None\n",
    "        \n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_der = float('inf')  # Track best DER\n",
    "        self.patience = 10\n",
    "        self.patience_counter = 0\n",
    "\n",
    "    def compute_jer(self, predictions: torch.Tensor, targets: torch.Tensor, \n",
    "                   mask: Optional[torch.Tensor] = None) -> float:\n",
    "        \"\"\"\n",
    "        Compute Jaccard Error Rate (micro-averaged)\n",
    "        \n",
    "        Args:\n",
    "            predictions: (batch, frames, num_speakers) - binary or probabilities\n",
    "            targets: (batch, frames, num_speakers) - binary ground truth\n",
    "            mask: (batch, frames) - valid frame mask\n",
    "        \n",
    "        Returns:\n",
    "            jer: Jaccard Error Rate (0-1, lower is better)\n",
    "        \"\"\"\n",
    "        # Binarize predictions\n",
    "        pred_binary = (predictions > 0.5).float()\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            mask_expanded = mask.unsqueeze(-1).expand_as(pred_binary)\n",
    "            pred_binary = pred_binary * mask_expanded.float()\n",
    "            targets = targets * mask_expanded.float()\n",
    "        \n",
    "        # Compute intersection and union per frame\n",
    "        intersection = (pred_binary * targets).sum()\n",
    "        union = ((pred_binary + targets) > 0).float().sum()\n",
    "        \n",
    "        # Jaccard index\n",
    "        jaccard = intersection / (union + 1e-8)\n",
    "        jer = 1.0 - jaccard.item()\n",
    "        \n",
    "        return jer\n",
    "    \n",
    "    def compute_overlap_metrics(self, predictions: torch.Tensor, targets: torch.Tensor,\n",
    "                                mask: Optional[torch.Tensor] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute overlap detection metrics (precision, recall, F1)\n",
    "        \n",
    "        Args:\n",
    "            predictions: (batch, frames, num_speakers) - probabilities\n",
    "            targets: (batch, frames, num_speakers) - binary\n",
    "            mask: (batch, frames) - valid frames\n",
    "        \n",
    "        Returns:\n",
    "            metrics: dict with overlap_precision, overlap_recall, overlap_f1\n",
    "        \"\"\"\n",
    "        pred_binary = (predictions > 0.5).float()\n",
    "        \n",
    "        # Detect overlapping frames (>1 speaker active)\n",
    "        overlap_pred = (pred_binary.sum(dim=-1) > 1).float()  # (batch, frames)\n",
    "        overlap_true = (targets.sum(dim=-1) > 1).float()\n",
    "        \n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            overlap_pred = overlap_pred * mask.float()\n",
    "            overlap_true = overlap_true * mask.float()\n",
    "        \n",
    "        # Compute TP, FP, FN\n",
    "        TP = ((overlap_pred == 1) & (overlap_true == 1)).sum().item()\n",
    "        FP = ((overlap_pred == 1) & (overlap_true == 0)).sum().item()\n",
    "        FN = ((overlap_pred == 0) & (overlap_true == 1)).sum().item()\n",
    "        \n",
    "        # Precision, Recall, F1\n",
    "        precision = TP / (TP + FP + 1e-8)\n",
    "        recall = TP / (TP + FN + 1e-8)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "        \n",
    "        return {\n",
    "            'overlap_precision': precision,\n",
    "            'overlap_recall': recall,\n",
    "            'overlap_f1': f1\n",
    "        }\n",
    "    \n",
    "    def compute_fa_miss_rates(self, predictions: torch.Tensor, targets: torch.Tensor,\n",
    "                              mask: Optional[torch.Tensor] = None) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute False Alarm and Miss rates\n",
    "        \n",
    "        Args:\n",
    "            predictions: (batch, frames, num_speakers)\n",
    "            targets: (batch, frames, num_speakers)\n",
    "            mask: (batch, frames)\n",
    "        \n",
    "        Returns:\n",
    "            metrics: dict with fa_rate, miss_rate\n",
    "        \"\"\"\n",
    "        pred_binary = (predictions > 0.5).float()\n",
    "        \n",
    "        # Speech activity (any speaker active)\n",
    "        speech_pred = (pred_binary.sum(dim=-1) > 0).float()  # (batch, frames)\n",
    "        speech_true = (targets.sum(dim=-1) > 0).float()\n",
    "        \n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            speech_pred = speech_pred * mask.float()\n",
    "            speech_true = speech_true * mask.float()\n",
    "        \n",
    "        # False Alarm: predicted speech when there's silence\n",
    "        FA = ((speech_pred == 1) & (speech_true == 0)).sum().item()\n",
    "        \n",
    "        # Miss: silence predicted when there's speech\n",
    "        Miss = ((speech_pred == 0) & (speech_true == 1)).sum().item()\n",
    "        \n",
    "        # Total frames\n",
    "        total_silence = (speech_true == 0).sum().item()\n",
    "        total_speech = (speech_true == 1).sum().item()\n",
    "        \n",
    "        fa_rate = FA / (total_silence + 1e-8)\n",
    "        miss_rate = Miss / (total_speech + 1e-8)\n",
    "        \n",
    "        return {\n",
    "            'fa_rate': fa_rate,\n",
    "            'miss_rate': miss_rate\n",
    "        }\n",
    "    \n",
    "    def compute_model_size(self) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute model size breakdown\n",
    "        \n",
    "        Returns:\n",
    "            metrics: dict with parameter counts and checkpoint size\n",
    "        \"\"\"\n",
    "        # Total parameters\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        \n",
    "        # Encoder parameters\n",
    "        encoder_params = sum(p.numel() for p in self.model.encoder.parameters())\n",
    "        \n",
    "        # Decoder parameters\n",
    "        decoder_params = sum(p.numel() for p in self.model.decoder.parameters())\n",
    "        \n",
    "        # Checkpoint size (if exists)\n",
    "        checkpoint_size_mb = 0.0\n",
    "        checkpoint_dir = Path('./checkpoints_phase3_simple')\n",
    "        if checkpoint_dir.exists():\n",
    "            checkpoints = list(checkpoint_dir.glob('*.pth'))\n",
    "            if checkpoints:\n",
    "                # Get size of latest checkpoint\n",
    "                latest_checkpoint = max(checkpoints, key=lambda p: p.stat().st_mtime)\n",
    "                checkpoint_size_mb = latest_checkpoint.stat().st_size / (1024 ** 2)\n",
    "        \n",
    "        return {\n",
    "            'model_params_millions': total_params / 1e6,\n",
    "            'encoder_params_millions': encoder_params / 1e6,\n",
    "            'decoder_params_millions': decoder_params / 1e6,\n",
    "            'checkpoint_size_mb': checkpoint_size_mb\n",
    "        }\n",
    "    \n",
    "    def measure_inference_time(self, num_samples: int = 5) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Measure inference time on random validation samples\n",
    "        \n",
    "        Args:\n",
    "            num_samples: Number of samples to average over\n",
    "        \n",
    "        Returns:\n",
    "            metrics: dict with timing info and RTF\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        mel_times = []\n",
    "        model_times = []\n",
    "        audio_durations = []\n",
    "        \n",
    "        # Sample random batches\n",
    "        sample_indices = random.sample(range(len(self.val_loader.dataset)), \n",
    "                                      min(num_samples, len(self.val_loader.dataset)))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for idx in sample_indices:\n",
    "                # Get sample\n",
    "                mel, labels, metadata = self.val_loader.dataset[idx]\n",
    "                \n",
    "                # Simulate mel extraction time (already done, so estimate)\n",
    "                # Approximate: 160 hop_length, 16000 sample_rate\n",
    "                audio_duration = mel.shape[-1] * 160 / 16000  # seconds\n",
    "                audio_durations.append(audio_duration)\n",
    "                \n",
    "                # Mel extraction time (estimated, ~1% of audio duration for efficient implementation)\n",
    "                mel_time = audio_duration * 0.01\n",
    "                mel_times.append(mel_time)\n",
    "                \n",
    "                # Move to device and add batch dim\n",
    "                mel = mel.unsqueeze(0).to(self.device)\n",
    "                \n",
    "                # Measure model inference time\n",
    "                torch.cuda.synchronize() if self.device.type == 'cuda' else None\n",
    "                start = time.time()\n",
    "                \n",
    "                with torch.amp.autocast('cuda', enabled=self.use_amp):\n",
    "                    _ = self.model.predict(mel)\n",
    "                \n",
    "                torch.cuda.synchronize() if self.device.type == 'cuda' else None\n",
    "                model_time = time.time() - start\n",
    "                model_times.append(model_time)\n",
    "        \n",
    "        # Compute averages\n",
    "        avg_mel_time = np.mean(mel_times)\n",
    "        avg_model_time = np.mean(model_times)\n",
    "        avg_total_time = avg_mel_time + avg_model_time\n",
    "        avg_audio_duration = np.mean(audio_durations)\n",
    "        \n",
    "        # Real-Time Factor\n",
    "        rtf = avg_total_time / avg_audio_duration\n",
    "        \n",
    "        return {\n",
    "            'inference_time_mel_sec': avg_mel_time,\n",
    "            'inference_time_model_sec': avg_model_time,\n",
    "            'inference_time_total_sec': avg_total_time,\n",
    "            'rtf': rtf\n",
    "        }\n",
    "    \n",
    "    def _init_csv(self):\n",
    "        \"\"\"Initialize CSV file with headers\"\"\"\n",
    "        with open(self.csv_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                'epoch', 'phase',\n",
    "                'pit_loss', 'overlap_loss','contrast_loss', 'total_loss',\n",
    "                'der', 'jer', 'fa_rate', 'miss_rate',\n",
    "                'overlap_precision', 'overlap_recall', 'overlap_f1',\n",
    "                'inference_time_mel_sec', 'inference_time_model_sec', \n",
    "                'inference_time_total_sec', 'rtf',\n",
    "                'model_params_millions', 'encoder_params_millions', \n",
    "                'decoder_params_millions', 'checkpoint_size_mb',\n",
    "                'learning_rate', 'timestamp'\n",
    "            ])\n",
    "        print(f\"âœ“ CSV log created: {self.csv_path}\")\n",
    "    \n",
    "    def _log_to_csv(self, epoch: int, phase: str, metrics: Dict):\n",
    "        \"\"\"Log metrics to CSV\"\"\"\n",
    "        with open(self.csv_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                epoch,\n",
    "                phase,\n",
    "                f\"{metrics.get('pit_loss', 0):.6f}\",\n",
    "                f\"{metrics.get('contrast_loss', 0):.6f}\",\n",
    "                f\"{metrics.get('total_loss', 0):.6f}\",\n",
    "                f\"{metrics.get('der', 0):.4f}\",\n",
    "                f\"{metrics.get('jer', 0):.4f}\",\n",
    "                f\"{metrics.get('fa_rate', 0):.4f}\",\n",
    "                f\"{metrics.get('miss_rate', 0):.4f}\",\n",
    "                f\"{metrics.get('overlap_precision', 0):.4f}\",\n",
    "                f\"{metrics.get('overlap_recall', 0):.4f}\",\n",
    "                f\"{metrics.get('overlap_f1', 0):.4f}\",\n",
    "                f\"{metrics.get('inference_time_mel_sec', 0):.4f}\",\n",
    "                f\"{metrics.get('inference_time_model_sec', 0):.4f}\",\n",
    "                f\"{metrics.get('inference_time_total_sec', 0):.4f}\",\n",
    "                f\"{metrics.get('rtf', 0):.4f}\",\n",
    "                f\"{metrics.get('model_params_millions', 0):.2f}\",\n",
    "                f\"{metrics.get('encoder_params_millions', 0):.2f}\",\n",
    "                f\"{metrics.get('decoder_params_millions', 0):.2f}\",\n",
    "                f\"{metrics.get('checkpoint_size_mb', 0):.2f}\",\n",
    "                f\"{metrics.get('learning_rate', 0):.8f}\",\n",
    "                datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            ])\n",
    "    \n",
    "    def train_epoch(self, epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"Train one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_pit_loss = 0\n",
    "        total_overlap_loss = 0\n",
    "        total_contrast_loss = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch} [Train]')\n",
    "        \n",
    "        for batch_idx, (mel, labels, metadata) in enumerate(pbar):\n",
    "            mel = mel.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            with torch.amp.autocast('cuda', enabled=self.use_amp):\n",
    "                logits, overlap_logits, embeddings = self.model(mel, return_embeddings=True)\n",
    "                \n",
    "                loss, pit_loss, overlap_loss = self.overlap_aware_loss(\n",
    "                    logits, overlap_logits, labels\n",
    "                )\n",
    "                \n",
    "                contrast_loss = self.contrastive_loss(\n",
    "                    embeddings, labels, \n",
    "                    memory_prototypes=None,\n",
    "                    contrastive_head=self.model.contrastive_head\n",
    "                )\n",
    "                \n",
    "                total = loss + self.contrastive_weight * contrast_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            if self.use_amp:\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5.0)\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            total_pit_loss += pit_loss.item()\n",
    "            total_overlap_loss += overlap_loss.item()\n",
    "            total_contrast_loss += contrast_loss.item()\n",
    "            total_loss += total.item()  # Changed from 'loss' to 'total'\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'pit': f'{pit_loss.item():.4f}',\n",
    "                'ovlp': f'{overlap_loss.item():.4f}',\n",
    "                'cont': f'{contrast_loss.item():.4f}',\n",
    "                'total': f'{loss.item():.4f}'\n",
    "            })\n",
    "        \n",
    "        n = len(self.train_loader)\n",
    "        metrics = {\n",
    "            'pit_loss': total_pit_loss / n,\n",
    "            'overlap_loss': total_overlap_loss / n,\n",
    "            'contrast_loss': total_contrast_loss / n,\n",
    "            'total_loss': total_loss / n,\n",
    "            'learning_rate': self.optimizer.param_groups[0]['lr']\n",
    "        }\n",
    "        \n",
    "        # Log to CSV\n",
    "        self._log_to_csv(epoch, 'train', metrics)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def validate(self, epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"Validate with comprehensive metrics\"\"\"\n",
    "        if epoch == 1 or epoch % 3 == 0:\n",
    "            self.threshold_predictor.calibrate(\n",
    "                self.model, self.val_loader, self.device\n",
    "            )\n",
    "        \n",
    "        self.model.eval()\n",
    "        total_pit_loss = 0\n",
    "        total_overlap_loss = 0\n",
    "        total_contrast_loss = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Accumulators for metrics\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "        all_masks = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(self.val_loader, desc=f'Epoch {epoch} [Val]')\n",
    "            \n",
    "            for mel, labels, metadata in pbar:\n",
    "                mel = mel.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                with torch.amp.autocast('cuda', enabled=self.use_amp):\n",
    "                    logits, overlap_logits, embeddings = self.model(mel, return_embeddings=True)\n",
    "                    \n",
    "                    loss, pit_loss, overlap_loss = self.overlap_aware_loss(\n",
    "                        logits, overlap_logits, labels\n",
    "                    )\n",
    "                    contrast_loss = self.contrastive_loss(\n",
    "                        embeddings, labels,\n",
    "                        memory_prototypes=None,\n",
    "                        contrastive_head=self.model.contrastive_head\n",
    "                    )\n",
    "                    \n",
    "                    loss = pit_loss + self.contrastive_weight * contrast_loss\n",
    "                    \n",
    "                    # Get predictions\n",
    "                    predictions = self.model.predict(mel, self.threshold_predictor)\n",
    "                \n",
    "                total_pit_loss += pit_loss.item()\n",
    "                total_contrast_loss += contrast_loss.item()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Store for metric calculation\n",
    "                all_predictions.append(predictions.cpu())\n",
    "                all_targets.append(labels.cpu())\n",
    "                # Create mask (all valid for now)\n",
    "                mask = torch.ones(labels.shape[0], labels.shape[1], dtype=torch.bool)\n",
    "                all_masks.append(mask)\n",
    "                \n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        n = len(self.val_loader)\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        all_predictions = torch.cat(all_predictions, dim=0)\n",
    "        all_targets = torch.cat(all_targets, dim=0)\n",
    "        all_masks = torch.cat(all_masks, dim=0)\n",
    "        \n",
    "        # Compute additional metrics\n",
    "        jer = self.compute_jer(all_predictions, all_targets, all_masks)\n",
    "        overlap_metrics = self.compute_overlap_metrics(all_predictions, all_targets, all_masks)\n",
    "        fa_miss_metrics = self.compute_fa_miss_rates(all_predictions, all_targets, all_masks)\n",
    "        \n",
    "        metrics = {\n",
    "            'pit_loss': total_pit_loss / n,\n",
    "            'contrast_loss': total_contrast_loss / n,\n",
    "            'total_loss': total_loss / n,\n",
    "            'learning_rate': self.optimizer.param_groups[0]['lr'],\n",
    "            'jer': jer,\n",
    "            **overlap_metrics,\n",
    "            **fa_miss_metrics\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def compute_der(self, loader: DataLoader) -> float:\n",
    "        \"\"\"Compute DER\"\"\"\n",
    "        self.model.eval()\n",
    "        total_frames = 0\n",
    "        error_frames = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for mel, labels, metadata in loader:\n",
    "                mel = mel.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                with torch.amp.autocast('cuda', enabled=self.use_amp):\n",
    "                    activities = self.model.predict(mel)\n",
    "                \n",
    "                pred_binary = (activities > 0.5).float()\n",
    "                errors = torch.abs(pred_binary - labels).sum()\n",
    "                frames = labels.numel()\n",
    "                \n",
    "                error_frames += errors.item()\n",
    "                total_frames += frames\n",
    "        \n",
    "        der = (error_frames / total_frames) * 100 if total_frames > 0 else 0\n",
    "        return der\n",
    "    \n",
    "    def save_checkpoint(self, epoch: int, metrics: Dict, filepath: str):\n",
    "        \"\"\"Save checkpoint\"\"\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'metrics': metrics,\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'best_der': self.best_der,\n",
    "            'patience_counter': self.patience_counter\n",
    "        }, filepath)\n",
    "        print(f\"Checkpoint saved: {filepath}\")\n",
    "    \n",
    "    def train(self, num_epochs: int, checkpoint_dir: str = './checkpoints_phase3_fixed'):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training Configuration:\")\n",
    "        print(f\"  Contrastive Weight: {self.contrastive_weight}\")\n",
    "        print(f\"  CSV Log: {self.csv_path}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Train\n",
    "            train_metrics = self.train_epoch(epoch)\n",
    "            print(f\"Train - PIT: {train_metrics['pit_loss']:.4f}, \"\n",
    "                  f\"Contrast: {train_metrics['contrast_loss']:.4f}, \"\n",
    "                  f\"Total: {train_metrics['total_loss']:.4f}\")\n",
    "\n",
    "            # Validate\n",
    "            val_metrics = self.validate(epoch)\n",
    "            print(f\"Val - PIT: {val_metrics['pit_loss']:.4f}, \"\n",
    "                  f\"Contrast: {val_metrics['contrast_loss']:.4f}, \"\n",
    "                  f\"Total: {val_metrics['total_loss']:.4f}\")\n",
    "            \n",
    "            # Compute DER\n",
    "            der = self.compute_der(self.val_loader)\n",
    "            val_metrics['der'] = der\n",
    "            print(f\"DER: {der:.2f}%\")\n",
    "            \n",
    "            # Compute JER and other metrics (already in val_metrics from validate())\n",
    "            print(f\"JER: {val_metrics['jer']:.4f}\")\n",
    "            print(f\"Overlap - P: {val_metrics['overlap_precision']:.3f}, \"\n",
    "                  f\"R: {val_metrics['overlap_recall']:.3f}, \"\n",
    "                  f\"F1: {val_metrics['overlap_f1']:.3f}\")\n",
    "            print(f\"FA Rate: {val_metrics['fa_rate']:.3f}, Miss Rate: {val_metrics['miss_rate']:.3f}\")\n",
    "            \n",
    "            # Measure inference time (sample 5 files)\n",
    "            print(\"Measuring inference time...\")\n",
    "            timing_metrics = self.measure_inference_time(num_samples=5)\n",
    "            val_metrics.update(timing_metrics)\n",
    "            print(f\"Inference - Mel: {timing_metrics['inference_time_mel_sec']:.3f}s, \"\n",
    "                  f\"Model: {timing_metrics['inference_time_model_sec']:.3f}s, \"\n",
    "                  f\"RTF: {timing_metrics['rtf']:.3f}\")\n",
    "            \n",
    "            # Compute model size (once per epoch for consistency)\n",
    "            if epoch == 1:\n",
    "                size_metrics = self.compute_model_size()\n",
    "                self.model_size_cache = size_metrics  # Cache for reuse\n",
    "                print(f\"Model Size - Total: {size_metrics['model_params_millions']:.2f}M params, \"\n",
    "                      f\"Checkpoint: {size_metrics['checkpoint_size_mb']:.1f} MB\")\n",
    "            \n",
    "            # Add cached model size to metrics\n",
    "            val_metrics.update(self.model_size_cache)\n",
    "\n",
    "\n",
    "            \n",
    "            # Log validation to CSV\n",
    "            self._log_to_csv(epoch, 'val', val_metrics)\n",
    "            \n",
    "            # Update scheduler\n",
    "            self.scheduler.step()\n",
    "            print(f\"Learning Rate: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint_path = Path(checkpoint_dir) / f'phase3_epoch_{epoch}.pth'\n",
    "            self.save_checkpoint(epoch, val_metrics, str(checkpoint_path))\n",
    "            \n",
    "            # Track best model by DER (more important than loss!)\n",
    "            if der < self.best_der:\n",
    "                self.best_der = der\n",
    "                self.patience_counter = 0\n",
    "                best_path = Path(checkpoint_dir) / 'phase3_best_der.pth'\n",
    "                self.save_checkpoint(epoch, val_metrics, str(best_path))\n",
    "                print(f\"âœ“ New best DER! {der:.2f}%\")\n",
    "            \n",
    "            # Early stopping based on val loss\n",
    "            if val_metrics['total_loss'] < self.best_val_loss - 1e-4:\n",
    "                self.best_val_loss = val_metrics['total_loss']\n",
    "                self.patience_counter = 0\n",
    "                best_path = Path(checkpoint_dir) / 'phase3_best_loss.pth'\n",
    "                self.save_checkpoint(epoch, val_metrics, str(best_path))\n",
    "                print(f\"âœ“ New best loss! {val_metrics['total_loss']:.4f}\")\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                print(f\"Patience: {self.patience_counter}/{self.patience}\")\n",
    "                \n",
    "                if self.patience_counter >= self.patience:\n",
    "                    print(f\"\\nâš ï¸ Early stopping triggered after {epoch} epochs\")\n",
    "                    break\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Training Complete!\")\n",
    "        print(f\"Best Val Loss: {self.best_val_loss:.4f}\")\n",
    "        print(f\"Best DER: {self.best_der:.2f}%\")\n",
    "        print(f\"CSV Log: {self.csv_path}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "\n",
    "class ConversationBatchSampler:\n",
    "    \"\"\"Sample batches from same conversation\"\"\"\n",
    "    def __init__(self, dataset, batch_size=4):\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Handle if dataset is wrapped in Subset (from train_test_split)\n",
    "        if hasattr(dataset, 'dataset'):\n",
    "            # It's a Subset\n",
    "            base_dataset = dataset.dataset\n",
    "            subset_indices = dataset.indices\n",
    "            chunks = [base_dataset.chunks[i] for i in subset_indices]\n",
    "        else:\n",
    "            # It's the raw dataset\n",
    "            chunks = dataset.chunks\n",
    "            subset_indices = list(range(len(chunks)))\n",
    "        \n",
    "        # Group indices by conversation\n",
    "        self.conv_to_indices = {}\n",
    "        for enum_idx, chunk_idx in enumerate(subset_indices):\n",
    "            if hasattr(dataset, 'dataset'):\n",
    "                chunk = base_dataset.chunks[chunk_idx]\n",
    "            else:\n",
    "                chunk = chunks[enum_idx]\n",
    "            \n",
    "            conv_id = chunk['conversation_id']\n",
    "            if conv_id not in self.conv_to_indices:\n",
    "                self.conv_to_indices[conv_id] = []\n",
    "            self.conv_to_indices[conv_id].append(enum_idx)\n",
    "        \n",
    "        # Sort by chunk index within conversation\n",
    "        for conv_id in self.conv_to_indices:\n",
    "            indices = self.conv_to_indices[conv_id]\n",
    "            if hasattr(dataset, 'dataset'):\n",
    "                self.conv_to_indices[conv_id] = sorted(\n",
    "                    indices,\n",
    "                    key=lambda i: base_dataset.chunks[subset_indices[i]]['chunk_idx']\n",
    "                )\n",
    "            else:\n",
    "                self.conv_to_indices[conv_id] = sorted(\n",
    "                    indices,\n",
    "                    key=lambda i: chunks[i]['chunk_idx']\n",
    "                )\n",
    "    \n",
    "    def __iter__(self):\n",
    "        conv_ids = list(self.conv_to_indices.keys())\n",
    "        random.shuffle(conv_ids)\n",
    "        \n",
    "        for conv_id in conv_ids:\n",
    "            indices = self.conv_to_indices[conv_id]\n",
    "            \n",
    "            for i in range(0, len(indices), self.batch_size):\n",
    "                batch = indices[i:i + self.batch_size]\n",
    "                if len(batch) == self.batch_size:\n",
    "                    yield batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        total = 0\n",
    "        for indices in self.conv_to_indices.values():\n",
    "            total += len(indices) // self.batch_size\n",
    "        return total\n",
    "\n",
    "def smooth_speaker_boundaries(activities: np.ndarray, kernel_size: int = 11) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply median filter to smooth speaker activity boundaries\n",
    "    \n",
    "    Args:\n",
    "        activities: (frames, num_speakers) - numpy array\n",
    "        kernel_size: Size of median filter (use odd number, default 11 = 0.11s)\n",
    "    \n",
    "    Returns:\n",
    "        smoothed: (frames, num_speakers)\n",
    "    \"\"\"\n",
    "    from scipy.ndimage import median_filter\n",
    "    \n",
    "    smoothed = np.zeros_like(activities)\n",
    "    for spk in range(activities.shape[1]):\n",
    "        smoothed[:, spk] = median_filter(\n",
    "            activities[:, spk], \n",
    "            size=kernel_size, \n",
    "            mode='nearest'\n",
    "        )\n",
    "    return smoothed\n",
    "\n",
    "def collate_fn_pad_chunks(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function that pads variable-length chunks to same size\n",
    "    \n",
    "    Args:\n",
    "        batch: List of (mel, labels, metadata) tuples\n",
    "    \n",
    "    Returns:\n",
    "        Batched tensors with padding\n",
    "    \"\"\"\n",
    "    mels = []\n",
    "    labels = []\n",
    "    metadata = {\n",
    "        'conversation_id': [],\n",
    "        'chunk_idx': [],\n",
    "        'chunk_start': [],\n",
    "        'chunk_end': []\n",
    "    }\n",
    "    \n",
    "    # Find max sizes in this batch\n",
    "    max_mel_frames = max(mel.shape[1] for mel, _, _ in batch)\n",
    "    max_label_frames = max(label.shape[0] for _, label, _ in batch)\n",
    "    \n",
    "    for mel, label, meta in batch:\n",
    "        # Pad mel spectrogram to max length\n",
    "        if mel.shape[1] < max_mel_frames:\n",
    "            pad_size = max_mel_frames - mel.shape[1]\n",
    "            mel = F.pad(mel, (0, pad_size))  # Pad time dimension\n",
    "        mels.append(mel)\n",
    "        \n",
    "        # Pad labels to max length\n",
    "        if label.shape[0] < max_label_frames:\n",
    "            pad_size = max_label_frames - label.shape[0]\n",
    "            label = F.pad(label, (0, 0, 0, pad_size))  # Pad frame dimension\n",
    "        labels.append(label)\n",
    "        \n",
    "        # Collect metadata\n",
    "        for key in metadata:\n",
    "            metadata[key].append(meta[key])\n",
    "    \n",
    "    # Stack into batches\n",
    "    mels = torch.stack(mels)\n",
    "    labels = torch.stack(labels)\n",
    "    \n",
    "    return mels, labels, metadata\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function for Simplified Phase 3\"\"\"\n",
    "    \n",
    "    set_seed(42)\n",
    "    \n",
    "    # Simplified configuration\n",
    "    config = {\n",
    "        'audio_dir': '/kaggle/input/voxconverse-dataset/voxconverse_dev_wav/audio',\n",
    "        'rttm_dir': '/kaggle/input/voxconverse-dataset/labels/dev',\n",
    "        'phase2_checkpoint': '/kaggle/input/2/pytorch/default/1/contraeend_phase2_best.pth',\n",
    "        \n",
    "        # Optimized hyperparameters\n",
    "        'batch_size': 8,\n",
    "        'num_epochs': 30,\n",
    "        'learning_rate': 2e-5,\n",
    "        'weight_decay': 1e-4,\n",
    "        'contrastive_weight': 0.05,\n",
    "        'overlap_weight': 5.0,              # Moderate (was 10.0)\n",
    "        'overlap_detection_weight': 1.0,\n",
    "        \n",
    "        # Architecture (simple, proven)\n",
    "        'chunk_size': 20.0,\n",
    "        'overlap': 3.0,\n",
    "        'd_model': 128,\n",
    "        'encoder_layers': 6,\n",
    "        'decoder_layers': 2,\n",
    "        'n_heads': 4,\n",
    "        'num_speakers': 6,\n",
    "        'projection_dim': 64,\n",
    "        \n",
    "        # System\n",
    "        'num_workers': 4,\n",
    "        'persistent_workers': True,\n",
    "        'prefetch_factor': 2,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"Simplified Phase 3: Proven Techniques Only\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Improvements over Phase 2:\")\n",
    "    print(\"  âœ“ Attention pooling for speaker embeddings\")\n",
    "    print(\"  âœ“ Overlap-weighted PIT loss\")\n",
    "    print(\"  âœ“ Boundary smoothing (inference)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Device: {config['device']}\")\n",
    "    print(f\"Batch Size: {config['batch_size']}\")\n",
    "    print(f\"Chunk Size: {config['chunk_size']}s\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Audio processor\n",
    "    audio_processor = AudioProcessor()\n",
    "    \n",
    "    # Dataset (reuse from Phase 3)\n",
    "    print(\"\\nLoading dataset...\")\n",
    "    full_dataset = ChunkWiseDataset(\n",
    "        audio_dir=config['audio_dir'],\n",
    "        rttm_dir=config['rttm_dir'],\n",
    "        audio_processor=audio_processor,\n",
    "        chunk_size=config['chunk_size'],\n",
    "        overlap=config['overlap'],\n",
    "        max_speakers=config['num_speakers'],\n",
    "        augment=True,\n",
    "    )\n",
    "    \n",
    "    # Split\n",
    "    train_size = int(0.9 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    # Disable augmentation for validation\n",
    "    val_dataset.dataset.augment = False\n",
    "    \n",
    "    print(f\"Train chunks: {len(train_dataset)}\")\n",
    "    print(f\"Val chunks: {len(val_dataset)}\")\n",
    "    \n",
    "    # DataLoaders (reuse collate function)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True,\n",
    "        persistent_workers=config['persistent_workers'],\n",
    "        prefetch_factor=config['prefetch_factor'],\n",
    "        collate_fn=collate_fn_pad_chunks,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=True,\n",
    "        persistent_workers=config['persistent_workers'],\n",
    "        prefetch_factor=config['prefetch_factor'],\n",
    "        collate_fn=collate_fn_pad_chunks,\n",
    "    )\n",
    "    \n",
    "    # Simplified model\n",
    "    print(\"\\nInitializing Simplified Phase 3 model...\")\n",
    "    model = SimplifiedPhase3EEND(\n",
    "        input_dim=83,\n",
    "        d_model=config['d_model'],\n",
    "        encoder_layers=config['encoder_layers'],\n",
    "        decoder_layers=config['decoder_layers'],\n",
    "        n_heads=config['n_heads'],\n",
    "        num_speakers=config['num_speakers'],\n",
    "        projection_dim=config['projection_dim']\n",
    "    )\n",
    "    \n",
    "    # Load Phase 2 weights\n",
    "    if os.path.exists(config['phase2_checkpoint']):\n",
    "        model.load_phase2_weights(config['phase2_checkpoint'])\n",
    "    else:\n",
    "        print(f\"âš ï¸ Warning: Phase 2 checkpoint not found\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    # Simplified trainer\n",
    "    print(\"\\nInitializing trainer...\")\n",
    "    trainer = ImprovedPhase3Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=config['device'],\n",
    "        learning_rate=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay'],\n",
    "        contrastive_weight=config['contrastive_weight'],\n",
    "        log_dir='./logs',\n",
    "    )\n",
    "        \n",
    "    # Train\n",
    "    print(\"\\nStarting training...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    trainer.train(\n",
    "        num_epochs=config['num_epochs'],\n",
    "        checkpoint_dir='./checkpoints_phase3_simple'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Simplified Phase 3 Complete!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1877225,
     "sourceId": 3085190,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 477082,
     "modelInstanceId": 461327,
     "sourceId": 613958,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 477083,
     "modelInstanceId": 461328,
     "sourceId": 613959,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
